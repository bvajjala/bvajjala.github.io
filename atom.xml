<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Balaji Vajjala's Blog]]></title>
  <link href="http://bvajjala.github.io/atom.xml" rel="self"/>
  <link href="http://bvajjala.github.io/"/>
  <updated>2014-04-24T15:29:43-04:00</updated>
  <id>http://bvajjala.github.io/</id>
  <author>
    <name><![CDATA[Balaji Vajjala]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Best Practices of Continuous Integration]]></title>
    <link href="http://bvajjala.github.io/blog/2014/04/22/best-practices-of-continuous-integration/"/>
    <updated>2014-04-22T10:41:40-04:00</updated>
    <id>http://bvajjala.github.io/blog/2014/04/22/best-practices-of-continuous-integration</id>
    <content type="html"><![CDATA[<h1>What is Continuous Integration</h1>

<p><em>Martin Fowler has the best description of CI</em></p>

<p>  Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily &ndash; leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly.</p>

<!-- more -->


<h1>Best Practices of Continuous Integration</h1>

<h2>1. Maintain a Single Source Repositry</h2>

<ul>
<li> Use SCM tools like Git, Subversion, Perforce for all Software Projects that need to be orchestarted together to build a product</li>
<li> Put everything that is required for a build in the SCM system and this should include: test scripts,properties files, database schema, install scripts, and third party libraries.</li>
<li> Keep your use of  branches to a minimum. Have a <em>mainline</em>: a single branch of the project under developemnt.</li>
<li><p> Don&rsquo;t add build artifacts/binaries to the SCM system. It only indicates the inability to reliabily recreate builds and absence of any Depndency management Solution.</p>

<p><strong>Tools Used : Git, SVN, Perforce, hg</strong></p></li>
</ul>


<h2>2. Automate the Build</h2>

<ul>
<li> Automate all phases of Build including Compilation, moving files aound, loading schema into the databases.</li>
<li><p> Only Build what has changed. Looks for dates of the source and object files and only compile if the source date is later</p>

<p><strong>Tools Used : Ant, Maven</strong></p></li>
</ul>


<h2>3. Make your Build Self-Testing</h2>

<ul>
<li><p>Use Test-Driven-Development (TDD) approaches to catch bugs in the code-base. These tests needs to be self-testing. For a build to be self-testing the failure of a test should cause the build to fail.</p>

<p><strong>Tools Used : XUnit family, FIT, Selenium,Sahi,Watir, FITnesse</strong></p></li>
</ul>


<h2>4. Everyone Commits to the mainline Every Day</h2>

<ul>
<li>Integration is primarily about Communication. Diing integration regularly allows the developers to tell others about the changes they have made and allow others to quickly react to the changes. The only prerequiste for a developer committing to the mailibe is that they can correctly build their code. This includes passing the build tests</li>
<li>Every Developer should commit to the repository every day. The more frequest they commit, the less places they have to look for conflict errors, and more rapidly they can then fix the conflicts</li>
</ul>


<h2>5. Every Commit Should Build the Mainline on an Integration Machine</h2>

<ul>
<li>Using Daily commits, a team gets frequent tested builds and this means that the <em>mainline</em> is always in <em>Ready-to-Release</em> state.</li>
<li>Use CI Server. A CI server acts as a monitor to the repository. Every time a commit against the repository finishes the CI server automatically checks out the sources onto the integration machine, initiates a build, and notifies the committer of the result of the build. The committer isn&rsquo;t done until she gets the notification &ndash; usually an email.</li>
</ul>


<p><strong>Tools Used : Jenkins/Hudson, Bamboo, TeamCity</strong></p>

<h2>6. Keep the Build Fast</h2>

<ul>
<li>Rather than having a monolithic single build which covers all phases of the build: Commit builds, Unit Tests builds, Code Coverage and Analysis, split the build pipeline into two stages. the first stage is the commit build and used as as the main CI cycle. The second-stage build runs when it can, picking up the executable from the latest good commit build for further testing. If this secondary build fails, then this may not have the same &lsquo;stop everything&rsquo; quality, but the team does aim to fix such bugs as rapidly as possible, while keeping the commit build running.</li>
<li>Keep the <em>commit build</em> times to be less than 10 mins. The <em>commit build</em> is the build that&rsquo;s needed when someone commits to the mainline</li>
<li>Parallize test stage by running the tests on multiple machines that run half the tests each.</li>
</ul>


<h2>7. Test in a Clone of the Production Environment</h2>

<ul>
<li>Use virtualization to put together test environments that mimics the Production Environment. Virtualized machines can be saved with all the necessary elements baked into the virtualization. It&rsquo;s then relatively straightforward to install the latest build and run tests. Furthermore this can allow you to run multiple tests on one machine, or simulate multiple machines in a network on a single machine.</li>
</ul>


<h2>8. Make it easy for Anyone to get the latest Executables</h2>

<ul>
<li> Store all build Executables/Installers in a centralized location, easily accessible to anyone involved with the software project. All Stakeholders should be easily be able to get the latest executable and be able to run it.</li>
</ul>


<h2>9. Everyone can see what&rsquo;s happening</h2>

<ul>
<li>CI is all about communication. Create a Dashboard or Wiki page where everone can easily see the state of the system and the changes that have been made to it.</li>
<li>Important thing to communicate is the stae of the mainline build. The Dashboard/Web Site should show if there&rsquo;s a build in progress and what was the last state of the mainline build.</li>
</ul>


<h2>10. Automate Deployment</h2>

<ul>
<li>To do effective Continuous Integration one needs multiple environments, one to run commit tests, one or more to run secondary tests. Since this involves moving executables between these environments multiple times a day, hence the need for Automation. So it&rsquo;s important to have scripts that will allow you to deploy the application into any environment easily.</li>
</ul>


<h1>Summary</h1>

<ul>
<li>Look for frequent builds to be triggered by code commits and to not take more than 15 minutes to run.</li>
<li>Keep builds simple and chain them together if more complexity is required. Simpler and quicker builds encourage more frequent use and are easier to debug when they break.</li>
<li>Prioritise fixing broken builds over starting new development work.</li>
<li>Identify failures sooner</li>
<li>Identify culprit change precisely</li>
<li>Avoids divide-and-conquer and tribal knowledge</li>
<li>Lowers compute costs using fine grained dependencies</li>
<li>Keeps the build green by reducing time to fix breaks</li>
<li>Accepted enthusiastically by product teams</li>
<li>Enables teams to ship with fast iteration times</li>
<li>Chain simple Jenkins jobs rather than trying to do everything in one big job.</li>
<li>Configure Jenkins as master + slaves.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Implementing Practical Continuous Deployment]]></title>
    <link href="http://bvajjala.github.io/blog/2014/04/21/implementing-practical-continuous-deployment/"/>
    <updated>2014-04-21T12:04:18-04:00</updated>
    <id>http://bvajjala.github.io/blog/2014/04/21/implementing-practical-continuous-deployment</id>
    <content type="html"><![CDATA[<p>
  <span>In Early part of this year, I had the pleasure of speaking at one of the Local User Group about some of our experiences with continuous delivery and deployment here. The slides for this </span><a target="_blank" href="http://bvajjala.github.io/blog/2014/04/21/practical-continuous-deployment/slides/#/">are available online</a><span>, but the talk generated a lot of discussion at the time and I&#8217;d like to recap some of it here.</span>
</p>


<!-- more -->


<p>
  To give a bit of context, I work in the business platform team; we are primarily responsible for developing the tools that allow the business to interact with the customer. In particular, My group develops the order management tools such as HAMS (hosted account management system) and MAC (my.actioncenter.com); if you have ever bought or evaluated our products, these are the systems that have been doing the work in the background. As we grow, these systems need to undergo a lot of changes, so we need to speed up our delivery of improvements and features. To do this, we are moving to a continuous delivery and deployment model for all of our business tools, which will allow us to get improvements out to our customers (both internal and external) faster.
</p>


<h2>Integration vs. deployment vs. delivery</h2>


<p>
  Before we start, I should probably clarify what I mean by <em>continuous integration, delivery, </em>and<em> deployment</em>. The terms are often used interchangeably, but they have different meanings for our purposes:
</p>


<h3>Continuous integration</h3>


<p>
  The process of automatically building and testing your software on a regular basis. How regularly this occurs varies; in the early days of agile this meant daily builds, however, with the rise of CI tools like Bamboo/Jenkins, this can be as often as every commit. In the business systems team we build and run full unit and integration tests of every commit to every branch using Bamboo&#8217;s/Jenkins branch-build feature.
</p>


<h3>Continuous delivery</h3>


<p>
  A logical step forward from continuous integration. If your tests are run constantly, and you trust your tests to provide a guarantee of quality, then it becomes possible to release your software at any point in time. Note that continuous delivery does not always entail actually delivering as your customers may not need or want constant updates. Instead, it represents a philosophy and a commitment to ensuring that your code is always in a release-ready state.
</p>


<h3>Continuous deployment</h3>


<p>
  The ultimate culmination of this process; it&#8217;s the actual delivery of features and fixes to the customer as soon as they are ready. It usually refers to cloud and SaaS, as these are the most amenable to being silently updated in the background. But some desktop software offers this in the form of optional beta and nightly updates such as Mozilla&#8217;s <a target="_blank" href="http://www.mozilla.org/en-US/firefox/channel/#beta">beta and aurora</a> Firefox channels, for example.
</p>


<p>
  In practice there&#8217;s a continuous spectrum of options between these techniques, ranging from just running tests regularly, to a completely automated deployment pipeline from commit to the customer. The constant theme through all of them, however, is a commitment to constant QA and testing, and a level of test coverage that imparts confidence in the readiness of your software for delivery.
</p>


<h2>So why would you bother?</h2>


<p>
  When bringing up the subject of continuous deployment adoption, there will inevitably (and rightly) be questions about what the benefits of the of the model are. For the business platforms team the main drivers were:
</p>


<ul>
  <li>
    We want to move to feature-based releases rather than a weekly &ldquo;whatever happens to be ready&rdquo; release. This is allows faster and finer-grained upgrades, and assists debugging and regression detection by only changing one thing at a time.
  </li>
  <li>
    The weekly release process was only semi-automated; While we have build tools such as Maven to perform the steps, the actual process of cutting a new release was driven by developers, following instructions on a Confluence/Wiki page. By automating every step of the process we make it self documenting and repeatable.
  </li>
  <li>
    Similarly, the actual process of getting the software onto our servers was semi automated; we had detailed scripts to upgrade servers, but running these required coordination with the sysadmin team, who already had quite enough work to be going on with. By making the deployment to the servers fully automated and driven by Bamboo/Jenkins rather than by humans, we created a repeatable deployment process and freed the sysadmins of busy work.
  </li>
  <li>
    By automating the release and deployment process, we can constantly release ongoing work to staging and QA servers, giving visibility of the state of development.
  </li>
</ul>


<p>
  But these benefits are driven by our internal processes; When advocating for adopting continuous deployment, you all often receive requests for justification from other stakeholders in your company. Continuous deployment bring benefits to them, too:
</p>


<ul>
  <li> 
    <em>To customers</em>: By releasing features when they are ready rather than waiting for a fixed upgrade window, customers will get them faster. By releasing constantly to a staging server while developing them, customers have visibility of the changes and can be part of the development process.
  </li>
  <li>
    <em>To management</em>: When we release more often, managers will see the result of work sooner and progress will be visible.
  </li>
  <li>
    <em>To developers</em>: This removes the weekly/monthly/whatever mad dash to get changes into the release window. If a developer needs a few more hours to make sure a feature is fully working, then the feature will go out a few hours later, not a when the next release window opens.
  </li>
  <li>
    <em>To sysadmins</em>: Not only will sysadmins not have to perform the releases themselves, but the change to small, discrete feature releases will allow easier detection of what changes affected the system adversely.
  </li>
</ul>


<p>
  The last point should probably be expanded upon, as it is such a powerful concept. If your release process bundles multiple features together, it becomes much harder to nail down exactly what caused regressions in the system. Consider the following scenario: On a Monday, a release is performed with the changes that were made the previous week. Shortly afterwards, however, the sysadmin team notices a large increase in load on the database server. While triaging the issue they notice the following changes were included:
</p>


<ul>
  <li>
    Developer A added a new AJAX endpoint for the order process.
  </li>
  <li>
    Developer B added a column to a table in the database.
  </li>
  <li>
    Developer C upgraded the version of the database driver in the application.
  </li>
</ul>


<p>
  Identifying which one is the cause would require investigation, possibly to the point of reverting changes or running <a target="_blank" href="http://git-scm.com/book/en/Git-Tools-Debugging-with-Git">git bisect</a>. However, if each change is released separately, the start of the performance regression can be easily correlated with the release of each feature, especially if your release process <a target="_blank" href="http://codeascraft.com/2010/12/08/track-every-release/">tags each release in your monitoring system</a>.
</p>


<h2>So how do you actually, you know, <em>do it</em>?</h2>


<p>
  Continuous deployment guides frequently focus on the culture and adoption aspects of it (as indeed this one has so far). What is less common to see are practical nuts-and-bolts issues being addressed. For the rest of this post I will attempt to address some of the practical hurdles we had to jump while transitioning to continuous deployment.
</p>


<p>
  It&#8217;s worth noting that everything here should be treated as a starting point rather than a set of rules; Much of what I cover is still actively being changed as we discover new ways of working and new requirements. In particular, the development process is still being actively tweaked as we find out what works best for us. Processes should serve the goals, not the other way around.
</p>




<h2>Development workflow</h2>


<p>
  Continuous deployment implies a clearer development process, with your main release branch always being in a releasable state. There are any number of methodologies that allow this and I won&#8217;t cover them all here, but if you want a deep dive into them I would recommend the book <a target="_blank" href="http://continuousdelivery.com/"><span>Continuous Delivery</span> by Jez Humble and David Farley</a>. As mentioned above, the model we follow is **release by feature** every distinct change we wish to make results in a separate release, containing only that change. What I will outline briefly here is our current workflow and the tools we are using to enable it.
</p>


<h3>#1: Use an issue tracker for everything</h3>


<p>
  For every bug, feature request, or other change, we create a unique ticket. If this is part of an ongoing project, we will use a parent epic, and for smaller chunks of work, we will use a sub-task. Obviously all this is best practice anyway, but in our case we use the issue IDs generated to track the change from concept to deployment. This is important as having a single reference point makes it easier to track the state of work, and enables some of the tool integrations I will describe below. As you may have guessed, we use JIRA for this purpose.
</p>


<h3>
  #2: Create a separate branch for this work, tagged with the issue number
</h3>


<p>
  In your version control system, create a branch which contains the issue number and a short description of the change; e.g. &ldquo;BIZPLAT-64951-remove-smtp-logging&rdquo;. It should hopefully go without saying at this point that you should be using a <a target="_blank" href="http://en.wikipedia.org/wiki/Distributed_revision_control#Systems">DVCS</a> for this work. Older version control systems such as Subversion make branching and merging difficult, and without the ability to separate work into streams, keeping the main branch pristine rapidly becomes unwieldy.
</p>


<p>
  We use Git for version control, and GitHub to manage our repositories. JIRA has a useful integration point here; from within the tracking issue we created in #1 we can just press a button and GitHub will create a branch for us. <a target="_blank" href="https://www.atlassian.com/software/sourcetree/overview">SourceTree</a> also has the ability to pick-up and checkout these new branches. JIRA also has the ability to query GitHub and list all branches associated with a given issue in the issue itself, along with number of commits to the branch. This turns your JIRA ticket into a convenient dashboard for the state of your development.
</p>


<h3>#3: Develop on this branch, and continuously test and integrate it</h3>


<p>
  Git allows you to easily make many commits on a branch, and then only merge when ready &ndash; but while working on this branch you should be constantly running your test system. We run our full integration test suite against every commit on all active branches.  We use Bamboo/Jenkins to do this; in particular we use its <a target="_blank" href="http://blogs.atlassian.com/2012/04/bamboofeature-branch-continuous-integration-hg-git/">plan branch</a> feature to automatically create the necessary build configuration.
</p>


<p>
  Again, there are useful integration points here. One is that, as with the branches and commits, JIRA can display the state of any branch plans associated with tickets, creating an at-a-glance view of the feature development. But a more powerful one (in my opinion) is that Bamboo can also inform GitHub of the state of builds for a branch. Why that is so important becomes evident when we come to pull requests in step #4.
</p>


<h3>#3.1: Optional: Push your changes to a rush box</h3>


<p>
  <a target="_blank" href="http://en.wikipedia.org/wiki/Dailies">Rush boxes</a> are used as staging servers for ongoing work that is not yet ready for QA or staging. This step is particularly useful when you have a customer involved in the development of the feature. By pushing out work in progress to a viewable stage environment you give them visibility of changes and allow them to review.
</p>


<p>
  A version of this is to do this automatically using the same infrastructure tooling you use to perform your deployments below to also push to the staging server. In extreme cases, you can actually create dedicated staging infrastructure (e.g at AWS) at the same time that you create the feature branch. We haven&#8217;t got to this stage yet, but it&#8217;s definitely on my &ldquo;cool stuff I&#8217;d like to do&rdquo; list.
</p>


<h3>#4: When ready, create a pull request for the branch</h3>


<p>
  Pull requests are the DVCS world&#8217;s version of <a target="_blank" href="http://en.wikipedia.org/wiki/Code_review">code review</a>. Inexperienced developers tend to dislike the idea, but many experienced developers love them, as they provide a safety net when working on critical code and infrastructure.
</p>


<p>
  On our team we have a &ldquo;no direct merge/commit to master&rdquo; rule  that we enforce through GitHub access controls. This means that all merges must come through a pull request. On top of this we define a couple of extra quality rules:
</p>


<ul>
  <li>
    All pull requests must have approval from at least one reviewer
  </li>
  <li>
    The Bamboo tests for this branch must pass.
  </li>
</ul>


<p>
  Both of these are enforced by GitHub; in particular, the second one is enforced by the Bamboo to GitHub pass/fail notification mentioned in #3.
</p>


<h3>#5: Merge and release</h3>


<p>
  Once the pull request has passed, the merge to the release branch can be performed. At this point we perform full release of the software; we use a separate dedicated Bamboo build plan for this, which first runs the full test suite before bumping the version and pushing to our build repository.
</p>


<h3>#6: Deploy to staging</h3>


<p>
  For us, this stage is also fully automated. The build of a release version of the software triggers its automatic deployment to our pre-production staging servers. This allows additional QA to be performed on it, and possible review by customers or other interested parties.
</p>


<p>
  The actual nuts and bolts of how we perform these deployments will be covered below.
</p>


<h3>#7: Promote to production</h3>


<p>
  One of our rules is that we never push builds directly out to production. The binaries that go to production are the exact same ones that have been through QA on our staging servers. Thus we don&#8217;t so much release to production as <em>promote</em> once we&#8217;re happy with the quality. Again, the mechanics of this will be covered in more detail below, but suffice to say that we use Bamboo to manage which builds are deployed to where, and Bamboo communicates this information back to JIRA where it can be displayed against the original feature request.
</p>


<h2>Some full disclosure about integration</h2>


<p>
  While I&#8217;ve pointed out a number of places where Atlassian products integrate together, the spirit of <em><a target="_blank" href="https://www.atlassian.com/company/about/values">open company, no bullshit</a></em> compels me to point out that these integration points are not just available to Atlassian tools. This interoperability is enabled by REST APIs that are <a target="_blank" href="https://developer.atlassian.com/display/DOCS/REST+API+Development">documented online</a>, so it&#8217;s perfectly possible enable these features with a little work, possibly via <a target="_blank" href="http://curl.haxx.se/">curl</a>. For example, if you&#8217;re using <a target="_blank" href="https://www.atlassian.com/software/GitHub">GitHub</a> for Git management but are still using Jenkins for CI, you can still enable the build-status integration mentioned in step #3 but calling out to the GitHub <a target="_blank" href="https://developer.atlassian.com/stash/docs/latest/how-tos/updating-build-status-for-commits.html">build status API</a>.
</p>


<p>
  However, the spirit of keeping marketing off my back and wanting to get paid compels me to point out that this is all a lot easier using our products together. 
</p>


<h2>Segue: Continuous downtime?</h2>


<p>
  It&#8217;s worth noting at this point that with continuous deployment also comes the possibility of continuous downtime. If you&#8217;re releasing infrequently, you can often get away with the occasional outage for upgrades, but if you get to the point where you&#8217;re releasing features several times a day, this quickly becomes unacceptable. Thus continuous deployment goes hand-in-hand with a need for clustering, failover, and other high-availability infrastructure.
</p>


<p>
  We went through this process, too. Earlier versions of our order systems were running a single instance for a largely historical reason, and prior to moving to the new development model we had been following a weekly release cycle, with the deployment happening on a Monday morning. This infrequency, along with the fact that the deployment happened in Australian business hours, meant that this didn&#8217;t affect customers unduly. Not that we were <em>proud</em> of this downtime, but it was never a sufficient pain point for us to invest the work necessary to turn this into a truly clustered system. But the move to continuous deployment meant we had to address this deficiency, splitting out the stateful parts of the order system (such as batch-processing) and clustering the critical parts.
</p>


<p>
  One additional point to note when clustering systems is that it&#8217;s important to select components of your network stack that will work with your deployment automation tools. For example, our original HA (high-availability) stack used Apache with <a target="_blank" href="http://httpd.apache.org/docs/2.2/mod/mod_proxy_balancer.html">mod_proxy_balancer</a>. However, while theoretically this can be automated via HTTP calls to the management front-end, in practice it was never built with this in mind and proved unreliable. In the end we moved our critical services behind a <a target="_blank" href="http://haproxy.1wt.eu/">HAProxy</a> cluster, which provides a reliable (albeit Unix-socket based) API for managing cluster members.
</p>


<h2>The last mile problem</h2>


<p>
  Another issue I seldom see addressed is how to actually get the software onto your servers; it&#8217;s assumed that if you&#8217;re doing continuous deployment you already have the &ldquo;deployment&rdquo; part taken care of. But even if this is the case, your existing tools may not necessarily fit with a fully automated deployment pipeline rather than a fixed-window schedule. I refer to this as the <a target="_blank" href="http://en.wikipedia.org/wiki/Last_mile">&lsquo;last mile&#8217;</a> problem. On the business platforms team we had this issue: We had a set of Python tools that had been in use for several years that performed upgrades on our hosts. However they were not end-to-end automated, and had no concept of cross-host coordination; rather than perform a rewrite of these scripts, we decided to look around for newer options.
</p>


<p>
  One option that may be possible in simple single-server cases is to use your Bamboo agents themselves as the last mile system. By placing an agent on the target servers and binding that to that server&#8217;s deployment environment, you can script the deployment steps as agent tasks.
</p>


<p>
  When consulting with your sysadmin team, the immediate temptation will probably be to use Puppet or Chef to perform the upgrades, as software configuration management. These didn&#8217;t work for us, however, as they work on the model of <em>eventual convergence</em>; i.e. that they&#8217;ll eventually get to the state that you desire, but not immediately. In a clustered system, however, you usually need events to happen in the correct order and be coordinated across hosts, e.g:
</p>


<ol>
  <li>Take server A out of the balancer pool</li>
  <li>Upgrade A</li>
  <li>Check A is operational</li>
  <li>Put A back into balancer pool</li>
  <li>Take B out of the balancer pool</li>
  <li>And so on&hellip;</li>
</ol>


<p>
  This is difficult to do with configuration management tools as you need something more directed; luckily a lot of work has been done in this area by the DevOps community over the last few years. Going into a full review of the options would be make this already too-long post longer, but suffice to say that we trialled a few options and eventually settled on <a target="_blank" href="http://www.ansible.com/home">Ansible</a> for automation. The main deciding factor was that it has explicit support for a number of tools we already use including HipChat and Nagios (and look for <a target="_blank" href="http://docs.ansible.com/jira_module.html">JIRA support coming in 1.6</a>), and that it was explicitly designed to perform rolling upgrades across clusters of servers.
</p>


<p>
  Ansible uses YAML <a target="_blank" href="http://docs.ansible.com/playbooks.html">playbooks</a> to describe sequences of operations to perform across hosts. We maintain these in their own Git repository that we pull as a task in the deployment environment. In total, our deployment environment task list looks like:
</p>


<ul>
  <li>Download the build artifacts</li>
  <li>Fetch Ansible from git</li>
  <li>Fetch Ansible playbooks from git</li>
  <li>Run the necessary playbook</li>
</ul>


<p>
  However we use Puppet to the base-line configuration of the hosts, including the filesystem permissions and SSH keys that Ansible uses. Personally I&#8217;ve never understood the Puppet/Chef vs. Ansible/Salt/Rundeck dichotomy; I see them as entirely complementary. For example, I use Ansible to create clusters of AWS servers, place them in the DNS and behind load-balancers, and then bootstrap Puppet on the host to manage the OS-level configuration. Once that&#8217;s complete, Ansible is then used to manage the deployment and upgrade of the application-level software.</p>


<h2>Managing deployments</h2>


<p>
  So now you have your release being built and deployed to development, staging, QA, and production environments &ndash; but how do you know which versions of the software are deployed where? How do you arrange the promotion of QA/staging builds up to production? And how do you ensure only certain users can perform these promotions?
</p>


<p>
  Earlier spikes of the business-platform automated deployments relied on Bamboo&#8217;s child-plans to separate the build and deployment stages of the pipeline and allow setting of permissions. However, this was unwieldy, and it quickly became hard to track what was deployed where. But with the release of Bamboo 5 we rapidly replaced these with the new <a target="_blank" href="https://confluence.atlassian.com/display/BAMBOO/Deployment+projects">deployment environments</a>. These look a lot like standard build plans (and so can use the same task plugins available to the build jobs), but they support concepts necessary for effective deployments. In particular, they support the idea that a single build may be deployed to multiple locations (e.g. QA and staging), and then migrated onto other locations (e.g. production). They also have explicit integration with build plans but have their own permissions separate for the builds.
</p>


<p>
  There&#8217;s a lot to deployment environments, and going into their features would take too long. However I will note that, like the other stages of the build pipeline, deployment environments have their own integration and feedback points; most notably the original JIRA ticket can display when, for example, a feature has been deployed to the staging environment.
</p>


<h2>Procedural issues</h2>


<p>
  On the subject of management, there are some additional questions you may need to take into account. What these are will depend a lot on your organization, but I&#8217;ll cover a couple of the issues we had to address along the way:
</p>


<h3>Deployment security</h3>


<p>
  Most of our systems share a common Bamboo server with a massive array of build agents that is managed by a dedicated team. Our <em>open company, no bullshit</em> philosophy means that we extend a lot of trust internally, and assume all employees are acting in the best interests of the company. This means that we grant developers quite high privileges to create and administer their own build plans. However the company is growing, and the regulatory bodies ends to take a less stoic view of such things. In particular, the software we produce modifies the company financials, and so must have strict access controls associated with it.
</p>


<p>
  Although we investigated methods of remaining open but secure, in the end we decided that we should err on the side of strict compliance. To do this we identified all the systems that could be considered as falling under <a target="_blank" href="http://en.wikipedia.org/wiki/Sarbanes%E2%80%93Oxley_Act">SOX</a> scope and placed them into a separate build environment separated from the more liberal policies of the master Bamboo server.
</p>


<h3>Separation of duties</h3>


<p>
  Historically, the business platforms team has done a lot of our own operations. In some ways, we were doing DevOps before the idea came to wider recognition. However, SOX contains some strict rules about separation of duties; in particular the implementers of a software change must have oversight and cannot themselves sign-off on the deployment to production.
</p>


<p>
  Our solution to this was to hand off the deployment to production to the business analysts. The analysts are almost always involved in the triage and specification of software changes, and are therefore in a good position to judge the readiness of features etc. to be deployed. And the use of automated deployment, in particular Bamboo&#8217;s deployment environments, makes this role available to them rather than just sysadmins.
</p>


<p>
  In practice, we use the the deployment environment permission system to restrict production deployments to specific group (&ldquo;bizplat-releasers&rdquo;); we can then add and remove members as required.
</p>


<h2>Conclusion</h2>


<p>
  I&#8217;ve covered a lot here, and have only really scratched the surface of the practice of continuous deployment. But I hope this has helped to clarify some of the issues and solutions involved in adopting this powerful model into your organization.
</p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Practical Continuous Deployment]]></title>
    <link href="http://bvajjala.github.io/blog/2014/04/21/practical-continuous-deployment/"/>
    <updated>2014-04-21T11:18:15-04:00</updated>
    <id>http://bvajjala.github.io/blog/2014/04/21/practical-continuous-deployment</id>
    <content type="html"><![CDATA[<p><a href="http://bvajjala.github.io/blog/2014/04/21/practical-continuous-deployment/slides"><h2>View the Presentation</h2></a></p>

<h5>Or view the slide sheet below</h5>


<hr />


<section class='slide'><div class='content'>

<h2>Practical continuous deployment</h2>

<h3>Balaji Vajjala</h3>

<h5>Principal DevOps Consultant/Solution Architect</h5>

<h5>Feb 2014</h5>

</div>

</section>

<section class='slide'><div class='content'>

<h3>￼Who Am I?</h3>

<h3>Balaji Vajjala!</h3>

<ul>
<li>A Full Stack DevOps Engineer/Solution Architect for 10+ years!</li>
<li>Original company sysadmin!</li>
<li>Developer for last 4 years!</li>
<li>Now working out of East Coast</li>
<li>Not a professional speaker</li>
</ul>


</div>

<aside class='notes'>

<p>Hi, my name is Balaji Vajjala and I&rsquo;m the Chief Architect and DevOps Solution Architect.</p>

<p>I have been developing software in the financial and Telecom industry since Mid 90&rsquo;s.</p>

<p>I am always passionate about technology, and have delivered diverse project portfolios initially as a software developer and subsequently as a leader at various development organisations.</p>

</aside>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼What I’ve been up to&hellip;</h3>

<ul>
<li>Last 6 months converting our order systems to high-availability and continuous deployment.</li>
<li>Why 6 months?

<ul>
<li>Because the concept is straightforward, but it’s implications affect a lot of your organisation.</li>
</ul>
</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<h3>What is DevOps?</h3>

<ul>
<li>Continuous Deployment</li>
<li>Continuous Delivery</li>
<li>Continuous Integration</li>
</ul>


</div>

<aside class='notes'>

<p>I don&rsquo;t have to tell you guys about DevOps, but for our purposes the main point is that we can do more stuff using the DevOps principles and Deliver software in a Agile Fashion.</p>

</aside>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼“Deployment”? “Delivery”?</h3>

<ul>
<li>Continuous integration is continuous, automated build and test.!</li>
<li>Continuous delivery is the next obvious step; be continuously release-ready !</li>
<li>Continuous deployment is the final step, the continuous delivery of software to production.</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼“Deployment”? “Delivery”?</h3>

<ul>
<li>Constant QA is the common theme.!</li>
<li>In practice there’s a continuous spectrum of options, each organisation has different needs and constraints.!</li>
<li>But if you trust your testing and process you can adopt the level appropriate for you.</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼Why Continuous deployment?</h3>

<ul>
<li>We want to release features, not “what ever happens to be done”!</li>
<li>Automation: Releasing is hard, automation makes it repeatable!</li>
<li>Remove organisational bottlenecks to releases</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼Why Continuous deployment? Stakeholder benefits</h3>

<ul>
<li>To customers: You’ll get your requested feature faster!!</li>
<li>To management: You’ll get results faster and clearer progress.!</li>
<li>To devs: No more death-marches, mad- dashes, clean-up after releases.!</li>
<li>To admins: You know which change broke the system!</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼Continuous deployment: So how do you actually do it?</h3>

<ul>
<li>Continuous deployment guides tend to focus on the high-level philosophy!</li>
<li>But how do you actually get a feature from a customer request to your servers?</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼Development workflow</h3>

<ul>
<li>Continuous deployment implies a clearer development process.!</li>
<li>You need to know what is going out when you release, not a dump of the current state.!</li>
<li>Hence release by feature</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼Development</h3>

<ul>
<li>Track your feature requests in a bug tracker!</li>
<li>Branch on each feature, automatically test!</li>
<li>Pull requests for code-review/merge!</li>
<li>Automatic release to staging on each merge!</li>
<li>Promote from staging to production</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼Step 1: Track your requests</h3>

<ul>
<li>Each feature/update request should have a unique ID.!</li>
<li>This allows tracking the state of a feature from request to deployment.!</li>
<li>Bug-trackers are a good choice for this.
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>Step 2: Work on this feature in a branch</h3>

<ul>
<li>Create a branch for just this feature!</li>
<li>Name it after the feature request!</li>
<li>Jira/Stash integration will do this!</li>
<li>The branch will be merged when complete!</li>
<li>You need a sane version control system!</li>
<li>We use git, Mercurial is good too
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Step 3: Automatically test the branch</h3>

<ul>
<li>Run a continuous integration tool that will automatically run tests against the branch.!</li>
<li>Features may not be merged until all tests are passing.!</li>
<li>Stash has some features to support this.
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Step 4: Code review</h3>

<ul>
<li>No code may be merged to the release branch until reviewed by other members of the team.!</li>
<li>Team members have a responsibility to ensure quality.
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Step 4.1: Stash testing integration</h3>

<p><img src="http://bvajjala.github.io/images/presentation-2.0.png" alt="" />￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Step 5: Merge and release</h3>

<ul>
<li>Once all reviews and tests are passed them merge to release branch!</li>
<li>At this point we have a separate Bamboo plan that performs a full release.
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Step 6: Deploy to staging</h3>

<ul>
<li>Allows testing of more advanced interactions and against production samples.!</li>
<li>More testing can occur at this point, including testing by humans.
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Step 7: Release to production</h3>

<ul>
<li>Valid staging builds may be promoted up to production.
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Segue: “Continuous downtime”?</h3>

<ul>
<li>So if you’re doing all these releases, what about uptime?!</li>
<li>For public-facing service clustering/HA is important.!</li>
<li>Ideally you should be able to automate cluster configuration as part of the deployment
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Last mile</h3>

<p><img src="http://bvajjala.github.io/images/presentation-2.1.png" alt="" />
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Practical issue</h3>

<ul>
<li>How do you actually get releases onto your staging and production servers?!</li>
<li>AKA “the last-mile problem”
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Last mile</h3>

<ul>
<li>Puppet/Chef are not appropriate!</li>
<li>For simple/single-node applications you can use a Bamboo agent directly!</li>
<li>For more complex setups use an automation tool
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Last mile</h3>

<ul>
<li>Puppet/Chef are not appropriate!</li>
<li>.. if timing is critical!</li>
<li>.. if cross-host coordination required
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Last mile</h3>

<ul>
<li>Roll your own!</li>
<li>Bamboo SSH plugin + bash scripting!</li>
<li>Number of existing automation solutions!</li>
<li>func, capistrano, SaltStack, Ansible, mcollective, Fabric&hellip;
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Last mile</h3>

<ul>
<li>Bamboo agent per-node!</li>
<li>SSH not required!</li>
<li>Works for simple (single node) apps! *  Coordination is tricky
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Last mile</h3>

<ul>
<li>Agent-based frameworks!</li>
<li>Powerful and flexible!</li>
<li>Can parallelise deployments!</li>
<li>Requires setup on all nodes!</li>
<li>If you already have it setup then use it
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Last mile</h3>

<ul>
<li>SSH scripting!</li>
<li>Requires management of SSH keys on agent!</li>
<li>Bamboo SSH plugin!</li>
<li>Scripting (Bash, Python, Ruby, etc.)!</li>
<li>Automation frameworks (Ansible, SaltStack, Func, Fabric)
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Last mile</h3>

<ul>
<li>Our solution!</li>
<li>Ansible for automation (explicit support for load-balancer integration)!</li>
<li>Minimal requirements, SSH+Python!</li>
<li>Bamboo pulls Ansible directly from their source repository!</li>
<li>Ansible playbooks checked into git
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Practical issue</h3>

<ul>
<li>How do you manage what has been released, and to where?!</li>
<li>How do you control who performs deployments?
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Bamboo deployment environments</h3>

<ul>
<li>The release build plan can be associated with certain environments!</li>
<li>Normal ones are dev, staging (QA) and production
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Bamboo deployment environments</h3>

<p><img src="http://bvajjala.github.io/images/presentation-2.2.png" alt="" />
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Bamboo deployment environments</h3>

<ul>
<li>Environment has tasks, like a build plan!</li>
<li>Tasks perform the actual deployment!</li>
<li>Environments have permissions, limiting who may perform deployments!</li>
<li>Generates releases, which are deployed!</li>
<li>Has some nice integrations&hellip;
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Bamboo deployment release</h3>

<p><img src="http://bvajjala.github.io/images/presentation-2.3.png" alt="" />
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Bamboo deployment JIRA integration</h3>

<p><img src="http://bvajjala.github.io/images/presentation-2.4.png" alt="" />
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Procedural issues</h3>

<ul>
<li>Where’s the oversight in all this?!</li>
<li>What about SoX, PCI, SEC requirements?!</li>
<li>Who is allowed to do releases?!</li>
<li>Who signs off?
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Procedural issues</h3>

<ul>
<li>Our solution &ndash; separate the infrastructure!</li>
<li>Dedicated Bamboo server for business software!</li>
<li>Dedicated agents for building!</li>
<li>Separate, dedicated agents for deployment
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼￼Procedural issues</h3>

<ul>
<li>Access controls!</li>
<li>Build team/admins control the server!</li>
<li>Business analysts define features!</li>
<li>Devs code, review, merge and release!</li>
<li>Features pushed to staging for BA review!</li>
<li>BAs can promote releases to production
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</li>
</ul>


</div>

</section>

<section class='slide'><div class='content'>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼￼Questions?</h3>

<p>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼
Balaji Vajjala
@bvajjala <a href="&#109;&#x61;&#x69;&#x6c;&#x74;&#x6f;&#58;&#x62;&#x76;&#97;&#106;&#x6a;&#97;&#108;&#x61;&#64;&#x67;&#109;&#97;&#105;&#x6c;&#x2e;&#x63;&#x6f;&#109;">&#x62;&#x76;&#97;&#106;&#106;&#97;&#108;&#97;&#x40;&#x67;&#x6d;&#97;&#105;&#108;&#46;&#99;&#111;&#x6d;</a>
<a href="http://bvajjala.github.io/">http://bvajjala.github.io/</a>
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼</p>

<h3>￼￼Thanks!</h3>

<p><img src="http://bvajjala.github.io/images/presentation-2.1.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Jenkins Job Builder and How to Extned it]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/22/jenkins-job-builder-and-how-to-extned-it/"/>
    <updated>2014-02-22T08:57:36-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/22/jenkins-job-builder-and-how-to-extned-it</id>
    <content type="html"><![CDATA[<h1>What is jenkins job builder</h1>

<p>Jenkins job builder is extreme good tool to manage your jenkins CI jobs, it takes simple description from YAML files, and use them to configure jenkins.</p>

<pre><code>#set free style job
#job-template.yml
- job:
    name: testjob
    project-type: freestyle
    defaults: global
    disabled: false
    display-name: 'Fancy job name'
    concurrent: true
    quiet-period: 5
    workspace: /srv/build-area/job-name
    block-downstream: false
    block-upstream: false
</code></pre>

<p>Then put your jenkins access into jenkins.ini file</p>

<pre><code>[jenkins]
user=USERNAME
password=USER_TOKEN
url=JENKINS_URL
ignore_cache=IGNORE_CACHE_FLAG
</code></pre>

<p>Based on the job configuration above, you just need to type command</p>

<pre><code>$ jenkins-jobs --conf jenkins.ini update job-template.yaml 
</code></pre>

<p>Then your job <em>testjob</em> is created in your jenkins server.</p>

<p>The project is created by <a href="https://wiki.openstack.org/wiki/InfraTeam">openstack-infrastructure team</a>, it is used to manage the openstack environment, fairly good.</p>

<h1>How it works</h1>

<p>There is no magic behind it, <em>jenkins-jobs</em> just convert the <em>job-template.yaml</em> to jenkins XML request file, and use jenkins remote API to send create request.</p>

<p>Try to do below to understand this.</p>

<pre><code>$ jenkins-jobs test job-template.yaml -o .
</code></pre>

<p>Then xml file <em>testjob</em> is created, see</p>

<pre><code>&lt;?xml version="1.0" ?&gt;
&lt;project&gt;
  &lt;actions/&gt;
  &lt;description&gt;

&amp;lt;!-- Managed by Jenkins Job Builder --&amp;gt;&lt;/description&gt;
  &lt;keepDependencies&gt;false&lt;/keepDependencies&gt;
  &lt;disabled&gt;false&lt;/disabled&gt;
  &lt;displayName&gt;Fancy job name&lt;/displayName&gt;
  &lt;blockBuildWhenDownstreamBuilding&gt;false&lt;/blockBuildWhenDownstreamBuilding&gt;
  &lt;blockBuildWhenUpstreamBuilding&gt;false&lt;/blockBuildWhenUpstreamBuilding&gt;
  &lt;concurrentBuild&gt;true&lt;/concurrentBuild&gt;
  &lt;customWorkspace&gt;/srv/build-area/job-name&lt;/customWorkspace&gt;
  &lt;quietPeriod&gt;5&lt;/quietPeriod&gt;
  &lt;canRoam&gt;true&lt;/canRoam&gt;
  &lt;properties/&gt;
  &lt;scm class="hudson.scm.NullSCM"/&gt;
  &lt;builders/&gt;
  &lt;publishers/&gt;
  &lt;buildWrappers/&gt;
&lt;/project&gt;
</code></pre>

<p>Now you can use curl command to send the request (testjob) directly !!</p>

<pre><code>$ curl --user USER:PASS -H "Content-Type: text/xml" -s --data "@testjob" "http://jenkins-server/createItem?name=testjob"
</code></pre>

<h2>How to recreate your jenkins job</h2>

<p>Looks great, finally you need think about how to re-create your jenkins job, it is also simple, just download the config.xml</p>

<pre><code>$ curl --user USER:PASS http://jenkins-server/testjob/config.xml
</code></pre>

<p>Or open the configuration page in broswer *<a href="http://jenkins-server/testjob/configure*">http://jenkins-server/testjob/configure*</a> and map from YAML file.</p>

<p>You need to read <a href="http://ci.openstack.org/jenkins-job-builder/configuration.html">jenkins job builder&rsquo;s guideline</a> to know the map, generate it had level Macro like <a href="https://wiki.openstack.org/wiki/InfraTeam">builders</a>, which is connected to the <a href="https://github.com/openstack-infra/jenkins-job-builder/blob/master/jenkins_jobs/modules/builders.py">real python builders module</a> to do transformation from YAML to XML.</p>

<p>What you stated in YAML file like</p>

<pre><code>-job:
  name: test_job
  builders:
- shell: "make test"
</code></pre>

<p>it will be converted to</p>

<pre><code>&lt;builders&gt;
&lt;hudson.tasks.Shell&gt;
  &lt;command&gt;make test&lt;/command&gt;&lt;/hudson.tasks.Shell&gt;
&lt;/builders&gt;
</code></pre>

<h2>How to extend</h2>

<p>Greatly to see jenkins job builder already had lots of default modules to support your normal jenkins jobs, but there is exceptions like some none popular jenkins plugins or your own plugins.</p>

<p>Then it is time to extend the module, the existing document: Extending is not clear enough, I will use example to show how it works, code is in <a href="https://github.com/bv2012/jenkins-buddy">github jenkins-buddy</a> project</p>

<p><a href="https://wiki.jenkins-ci.org/display/JENKINS/ArtifactDeployer+Plugin">ArtifactDeployer</a> Plugin is used as example, this plugin is the popular plugin to deploy the artifacts to other folder.</p>

<p>Artifact Deploy Plugin</p>

<p><img src="../downloads/code/artifactdeploy.png" alt="" /></p>

<p>And I want to have .YAML like below</p>

<pre><code>*#artifactdeploy.yaml*
- job:
name: test-job
publishers:
  - artifactdeployer: 
  includes: 'buddy-*.tar.gz'
  remote: '/project/buddy'
</code></pre>

<h2>write codes to transform</h2>

<p>Now I need to download the existing jobs to see how XML looks like, using curl above, I got it like</p>

<pre><code>&lt;publishers&gt;
   ...  
  &lt;org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerPublisher plugin="artifactdeployer@0.27"&gt;
&lt;entries&gt;
  &lt;org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerEntry&gt;
&lt;includes&gt;buddy-*.tar.gz&lt;/includes&gt;
&lt;basedir&gt;&lt;/basedir&gt;
&lt;excludes&gt;&lt;/excludes&gt;
&lt;remote&gt;/project/buddy&lt;/remote&gt;
&lt;flatten&gt;false&lt;/flatten&gt;
&lt;deleteRemote&gt;false&lt;/deleteRemote&gt;
&lt;deleteRemoteArtifacts&gt;false&lt;/deleteRemoteArtifacts&gt;
&lt;deleteRemoteArtifactsByScript&gt;false&lt;/deleteRemoteArtifactsByScript&gt;
&lt;failNoFilesDeploy&gt;false&lt;/failNoFilesDeploy&gt;
  &lt;/org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerEntry&gt;
&lt;/entries&gt;
&lt;deployEvenBuildFail&gt;false&lt;/deployEvenBuildFail&gt;
  &lt;/org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerPublisher&gt;
..
&lt;/publishers&gt; 
</code></pre>

<p>It belongs the section publishers So I write the jenkins_buddy/modules/publishers.py module to add one function artifactdeployer:</p>

<pre><code>def artifactdeployer(parser, xml_parent, data):
    logger = logging.getLogger("%s:artifactdeployer" % __name__)
    artifactdeployer = XML.SubElement(xml_parent, 'org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerPublisher')
    entries = XML.SubElement(artifactdeployer, 'entries')
    entry = XML.SubElement(entries, 'org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerEntry')
    print data
    XML.SubElement(entry, 'includes').text = data['includes']
    XML.SubElement(entry, 'remote').text = data['remote']
</code></pre>

<p>It is the core part handling convert.</p>

<h3>Hook into jenkins-job builder</h3>

<p>Now you need hook this script into jenkins-jobs builder, thank for the entry_points in python, it can be used for this.</p>

<p>Create the plugin related script and structure, add new entry_point in setup.py</p>

<pre><code>#setup.py in jenkins-buddy
entry_points={
    'jenkins_jobs.publishers': [
    'artifactdeployer=jenkins_buddy.modules.publishers:artifactdeployer',
    ],
}
</code></pre>

<p>it tells jenkins-jobs if you meet new keyword artifactdeployer in publishers, please let me jenkins_buddy.modules.publishers:artifactdeployer to handle.</p>

<h3>Verify it</h3>

<p>Build the pip package local and install it</p>

<pre><code>$ python setup.py sdist
$ pip install dist/jenkins-buddy-0.0.5.zip
</code></pre>

<p>And verify the new job, Bingo, it works.</p>

<pre><code>$ jenkins-jobs test artifactdeploy.yaml -o . 
</code></pre>

<h3>###Make it more complete by checking jenkins plugin java code</h3>

<p>Maybe you noticed, it is hack solution, since I skipped some parameter converting and guess what the XML will look like, if you want to make it more complete, we need to check the java codes directly.</p>

<p>src/main/java/org/jenkinsci/plugins/artifactdeployer/ArtifactDeployerPublisher.java is the class we need to take care.</p>

<pre><code>@DataBoundConstructor
public ArtifactDeployerPublisher(List&lt;ArtifactDeployerEntry&gt; deployedArtifact, boolean deployEvenBuildFail) {
    this.entries = deployedArtifact;
    this.deployEvenBuildFail = deployEvenBuildFail;
    if (this.entries == null)
    this.entries = Collections.emptyList();
}
</code></pre>

<p>It is directly mapping from XML into internal data, if you need know more, learn how to develop jenkins plugin.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nodejs Deployment: Building and Configuring on Amazon Linux AMI]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/04/nodejs-deployment-building-and-configuring-on-amazon-linux-ami/"/>
    <updated>2014-02-04T13:47:50-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/04/nodejs-deployment-building-and-configuring-on-amazon-linux-ami</id>
    <content type="html"><![CDATA[<h2>Logging in and updating system to latest</h2>

<p>SSH your shiny new VM,</p>

<p>Now lets update the system to the latest:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo yum update</span></code></pre></td></tr></table></div></figure>


<h2>Install OS dependencies</h2>

<p>We’r going to build Node.js from sources, some dependencies (such as gcc) are required:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo yum install gcc-c++ make openssl-devel git</span></code></pre></td></tr></table></div></figure>




<!--more-->


<h2>Cloning n Building Node.js</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd
</span><span class='line'>git clone git://github.com/joyent/node.git
</span><span class='line'>cd node
</span><span class='line'>git checkout v0.10.13 #check for other stable tags by executing 'git tag'
</span><span class='line'>./configure --prefix=/usr/local/node
</span><span class='line'>make
</span><span class='line'>sudo make install</span></code></pre></td></tr></table></div></figure>


<h2>Configuration</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo useradd _yourappuser_
</span><span class='line'>passwd _yourappuser_
</span><span class='line'>sudo su - _yourappuser_</span></code></pre></td></tr></table></div></figure>


<h2>Put your app</h2>

<p>Now put your app in ~, for instance:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd
</span><span class='line'>pwd
</span><span class='line'>#/home/_yourappuser_
</span><span class='line'>git clone _https://myhost.com/myapp myapp_</span></code></pre></td></tr></table></div></figure>


<h2>Init.d</h2>

<p>We would like to have nodejs to start automatically as a service, to do so, lets create an init.d file Note: you have to change the properties in the file such as yourappuser, myapp to your app folder and server.js to your node app file.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cat &lt;&lt; 'EOF' > /etc/init.d/nodejs
</span><span class='line'>
</span><span class='line'>#!/bin/sh
</span><span class='line'>
</span><span class='line'>#
</span><span class='line'># chkconfig: 35 99 99
</span><span class='line'># description: Node.js /home/yourappuser/myapp/app.js
</span><span class='line'>#
</span><span class='line'>
</span><span class='line'>. /etc/rc.d/init.d/functions
</span><span class='line'>
</span><span class='line'>USER="_yourappuser_"
</span><span class='line'>
</span><span class='line'>NODE_ENV="production"
</span><span class='line'>DAEMON="/usr/local/node/bin/node"
</span><span class='line'>ROOT_DIR="/home/yourappuser/myapp"
</span><span class='line'>
</span><span class='line'>SERVER="$ROOT_DIR/server.js"
</span><span class='line'>LOG_FILE="$ROOT_DIR/app.js.log"
</span><span class='line'>
</span><span class='line'>LOCK_FILE="/var/lock/subsys/node-server"
</span><span class='line'>
</span><span class='line'>do_start()
</span><span class='line'>{
</span><span class='line'>        if [ ! -f "$LOCK_FILE" ] ; then
</span><span class='line'>                echo -n $"Starting $SERVER: "
</span><span class='line'>                runuser -l "$USER" -c "NODE_ENV=$NODE_ENV $DAEMON $SERVER >> $LOG_FILE &" && echo_success || echo_failure
</span><span class='line'>                RETVAL=$?
</span><span class='line'>                echo
</span><span class='line'>                [ $RETVAL -eq 0 ] && touch $LOCK_FILE
</span><span class='line'>        else
</span><span class='line'>                echo "$SERVER is locked."
</span><span class='line'>                RETVAL=1
</span><span class='line'>        fi
</span><span class='line'>}
</span><span class='line'>do_stop()
</span><span class='line'>{
</span><span class='line'>        echo -n $"Stopping $SERVER: "
</span><span class='line'>        pid=`ps -aefw | grep "$DAEMON $SERVER" | grep -v " grep " | awk '{print $2}'`
</span><span class='line'>        kill -9 $pid > /dev/null 2>&1 && echo_success || echo_failure
</span><span class='line'>        RETVAL=$?
</span><span class='line'>        echo
</span><span class='line'>        [ $RETVAL -eq 0 ] && rm -f $LOCK_FILE
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>case "$1" in
</span><span class='line'>        start)
</span><span class='line'>                do_start
</span><span class='line'>                ;;
</span><span class='line'>        stop)
</span><span class='line'>                do_stop
</span><span class='line'>                ;;
</span><span class='line'>        restart)
</span><span class='line'>                do_stop
</span><span class='line'>                do_start
</span><span class='line'>                ;;
</span><span class='line'>        *)
</span><span class='line'>                echo "Usage: $0 {start|stop|restart}"
</span><span class='line'>                RETVAL=1
</span><span class='line'>esac
</span><span class='line'>
</span><span class='line'>exit $RETVAL
</span><span class='line'>
</span><span class='line'>EOF</span></code></pre></td></tr></table></div></figure>


<p>Add execution permission to the nodejs init script</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo chmod +x /etc/init.d/nodejs</span></code></pre></td></tr></table></div></figure>


<h2>Pre Routing to port 80</h2>

<p>Linux does not allow non super users to listen to ports &lt; 1024, assuming your application listen to port 8080, You would probably like to pre route any traffic arriving from port 80 to your node app that listens to port 8080</p>

<p>You can do this by the pre routing nat capability of Iptables</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>chkconfig iptables on
</span><span class='line'>service iptables start
</span><span class='line'>iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 8080
</span><span class='line'>iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT --to-port 8443 #if you want SSL too
</span><span class='line'>service iptables save</span></code></pre></td></tr></table></div></figure>


<h2></h2>

<p>Configuring node-http-proxy ##</p>

<p>It is common to install http proxies such as nginx on front of nodejs, This architecture has many advantages such as raising security level, listening natively to port 80, load balancing, multiple node apps support via url rewrite, etc…</p>

<p>I personally think that the best approach, which is very native to node apps is to use <a href="https://github.com/nodejitsu/node-http-proxy,">https://github.com/nodejitsu/node-http-proxy,</a></p>

<p>Which have several advantages:</p>

<p>Reverse proxies incoming http.ServerRequest streams, WebSockets, HTTPS
Minimal request overhead and latency
Battled-hardened through production usage
Very native for nodejs apps
TODO: Will post more details in the future but you can simply visit “<a href="https://github.com/nodejitsu/node-http-proxy%E2%80%9D">https://github.com/nodejitsu/node-http-proxy%E2%80%9D</a> site.</p>

<p><a href="https://github.com/pkrumins/nodejs-proxy">https://github.com/pkrumins/nodejs-proxy</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploy/Release Workflow from GitHub]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/04/deploy-slash-release-workflow-from-github/"/>
    <updated>2014-02-04T09:50:50-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/04/deploy-slash-release-workflow-from-github</id>
    <content type="html"><![CDATA[<h1>## Workflow : Deploying/Release Apps from Development to Production  ##</h1>

<p>Deploying is a big part of the lives of most of our Engineering employees. We don&rsquo;t have a release manager and there are no set weekly deploys. Developers and designers are responsible for shipping new stuff themselves as soon as it&rsquo;s ready. This means that deploying needs to be as smooth and safe a process as possible.</p>

<p>The best system we&rsquo;ve found so far to provide this flexibility is to have people deploy branches. Changes never get merged to master until they have been verified to work in production from a branch. This means that master is always stable; a safe point that we can roll back to if there&rsquo;s a problem.</p>

<p>The basic workflow goes like this:</p>

<ul>
<li>Push changes to a branch in GitHub</li>
<li>Wait for the build to pass on our CI server (Jenkins)</li>
<li>Tell Hubot to deploy it</li>
<li>Verify that the changes work and fix any problems that come up</li>
<li>Merge the branch into master
Not too long ago, however, this system wasn&rsquo;t very smart. A branch could accidentally be deployed before the build finished, or even if the build failed. Employees could mistakenly deploy over each other. As the company has grown, we&rsquo;ve needed to add some checks and balances to help us prevent these kinds of mistakes.</li>
</ul>


<h2>Safety First</h2>

<p>The first thing we do now, when someone tries to deploy, is make a call to <a href="https://github.com/github/janky">Janky</a> to determine whether the current CI build is green. If it hasn&rsquo;t finished yet or has failed, we&rsquo;ll tell the deployer to fix the situation and try again.</p>

<p>Next we check whether the application is currently &ldquo;locked&rdquo;. The lock indicates that a particular branch is being deployed in production and that no other deploys of the application should proceed for the moment. Successful builds on the master branch would otherwise get deployed automatically, so we don&rsquo;t want those going out while a branch is being tested. We also don&rsquo;t want another developer to accidentally deploy something while the branch is out.</p>

<p>The last step is to make sure that the branch we&rsquo;re deploying contains the latest commit on master that has made it into production. Once a commit on master has been deployed to production, it should never be “removed” from production by deploying a branch that doesn’t have that commit in it yet.</p>

<p>We use the GitHub API to verify this requirement. An endpoint on the github.com application exposes the SHA1 that is currently running in production. We submit this to the GitHub compare API to obtain the &ldquo;merge base&rdquo;, or the common ancestor, of master and the production SHA1. We can then compare this to the branch that we&rsquo;re attempting to deploy to check that the branch is caught up. By using the common ancestor of master and production, code that only exists on a branch can be removed from production, and changes that have landed on master but haven&rsquo;t been deployed yet won&rsquo;t require branches to merge them in before deploying.</p>

<p>If it turns out the branch is behind, master gets merged into it automatically. We do this using the new :sparkles:Merging API:sparkles: that we&rsquo;re making available today. This merge starts a new CI build like any other push-style event, which starts a deploy when it passes.</p>

<p>At this point the code actually gets deployed to our servers. We usually deploy to all servers for consistency, but a subset of servers can be specified if necessary. This subset can be by functional role — front-end, file server, worker, search, etc. — or we can specify an individual machine by name, e.g, &lsquo;fe7&rsquo;.</p>

<h2>Watch it in action</h2>

<p>What now? It depends on the situation, but as a rule of thumb, small to moderate changes should be observed running correctly in production for at least 15 minutes before they can be considered reasonably stable. During this time we monitor exceptions, performance, tweets, and do any extra verification that might be required. If non-critical tweaks need to be made, changes can be pushed to the branch and will be deployed automatically. In the event that something bad happens, rolling back to master only takes 30 seconds.</p>

<h2>All done!</h2>

<p>If everything goes well, it&rsquo;s time to merge the changes. At GitHub, we use Pull Requests for almost all of our development, so merging typically happens through the pull request page. We detect when the branch gets merged into master and unlock the application. The next deployer can now step up and ship something awesome.</p>

<h1>How do we do it?</h1>

<p>Most of the magic is handled by an internal deployment service called Heaven. At its core, Heaven is a catalog of Capistrano recipes wrapped up in a Sinatra application with a JSON API. Many of our applications are deployed using generic recipes, but more complicated apps can define their own to specify additional deployment steps. Wiring it up to Janky, along with clever use of post-receive hooks and the GitHub API, lets us hack on the niceties over time. Hubot is the central interface to both Janky and Heaven, giving everyone in Campfire great visibility into what’s happening all of the time. As of this writing, 75 individual applications are deployed by Heaven.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Designing A RESTful API That Doesn't Suck]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/03/designing-a-restful-api-that-doesnt-suck/"/>
    <updated>2014-02-03T14:23:30-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/03/designing-a-restful-api-that-doesnt-suck</id>
    <content type="html"><![CDATA[<h2><a href="http://bvajjala.github.io/blog/2013/03/22/designing-a-restful-api-that-doesn-t-suck.html">Designing A RESTful API That Doesn&rsquo;t Suck</a></h2>

<p>As we&rsquo;re getting closer to shipping the first version of <a href="http://devo.ps">devo.ps</a> and we are joined by a few new team members, the team took the time to review the few principles we followed when designing our RESTful JSON API. A lot of these can be found on <a href="https://blog.apigee.com/taglist/rest_api_design">apigee&rsquo;s blog</a> (a recommended read). Let me give you the gist of it:</p>

<ul>
<li><p><strong>Design your API for developers first</strong>, they are the main users. In that respect, simplicity and intuitivity matter.</p></li>
<li><p><strong>Use <a href="http://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods">HTTP verbs</a></strong> instead of relying on parameters (e.g. <code>?action=create</code>). HTTP verbs map nicely with <a href="http://en.wikipedia.org/wiki/Create,_read,_update_and_delete">CRUD</a>:</p>

<ul>
<li><code>POST</code> for <em>create</em>,</li>
<li><code>GET</code> for <em>read</em>,</li>
<li><code>DELETE</code> for <em>remove</em>,</li>
<li><code>PUT</code> for <em>update</em> (and <code>PATCH</code> too).</li>
</ul>
</li>
<li><p><strong>Use <a href="http://en.wikipedia.org/wiki/List_of_HTTP_status_codes">HTTP status codes</a></strong>, especially for errors (authentication required, error on the server side, incorrect parameters)… There are plenty to choose from, here are a few:</p>

<ul>
<li><code>200</code>: <em>OK</em></li>
<li><code>201</code>: <em>Created</em></li>
<li><code>304</code>: <em>Not Modified</em></li>
<li><code>400</code>: <em>Bad Request</em></li>
<li><code>401</code>: <em>Unauthorized</em></li>
<li><code>403</code>: <em>Forbidden</em></li>
<li><code>404</code>: <em>Not Found</em></li>
<li><code>500</code>: <em>Internal Server Error</em></li>
</ul>
</li>
<li><p><strong>Simple URLs for resources: first a noun for the collection, then the item</strong>. For example <code>/emails</code> and <code>/emails/1234</code>; the former gives you the collection of emails, the second one a specific one identified by its internal id.</p></li>
<li><p><strong>Use verbs for special actions</strong>. For example, <code>/search?q=my+keywords</code>.</p></li>
<li><p><strong>Keep errors simple but verbose (and use HTTP codes)</strong>. We only send something like <code>{ message: "Something terribly wrong happened" }</code> with the proper status code (e.g. <code>401</code> if the call requires authentication) and log more verbose information (origin, error code…) in the backend for debugging and monitoring.</p></li>
</ul>


<p>Relying on HTTP status codes and verbs should already help you keep your API calls and responses lean enough. Less crucial, but still useful:</p>

<ul>
<li><strong>JSON first</strong>, then extend to other formats if needed and if time permits.</li>
<li><strong><a href="http://en.wikipedia.org/wiki/Unix_time">Unix time</a></strong>, or you&rsquo;ll have a bad time.</li>
<li><strong>Prepend your URLs with the API version</strong>, like <code>/v1/emails/1234</code>.</li>
<li><strong>Lowercase everywhere in URLs</strong>.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[I Can Haz Init Script]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/03/i-can-haz-init-script/"/>
    <updated>2014-02-03T14:21:03-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/03/i-can-haz-init-script</id>
    <content type="html"><![CDATA[<h2><a href="http://bvajjala.github.io/blog/2013/06/19/I-can-haz-init-script.html">I Can Haz Init Script</a></h2>

<p>Something went awfully wrong, and a rogue process is eating up all of the resources on one of your servers. You have no other choice but to restart it. No big deal, really; this is the age of disposable infrastructure after all. Except when it comes back up, everything starts going awry. Half the stuff supposed to be running is down and it&rsquo;s screwing with the rest of your setup.</p>

<p><img src="http://bvajjala.github.io/images/posts/y-u-no-guy.png" alt="INIT SCRIPTS, Y U NO LIKE?" /></p>

<p>You don&rsquo;t get to think about them very often, but init scripts are a key piece of a sound, scalable strategy for your infrastructure. It&rsquo;s a <a href="">mandatory best practice</a>. Period. And there are quite a few things in the way of getting them to work properly at scale in production environments. It&rsquo;s a tough world out there.</p>

<h3>What we&rsquo;re dealing with…</h3>

<h4>Packages</h4>

<p>Often enough, you&rsquo;re gonna end up installing a service using the package manager of your distro: <code>yum</code>, <code>apt-get</code>, you name it. These packages usually come with an init script that should get you started.</p>

<p>Sadly, as your architecture grows in complexity, you&rsquo;ll probably run into some walls. Wanna have multiple memcache buckets, or several instances of redis running on the same box? You&rsquo;re out of luck buddy. Time to hack your way
through:</p>

<ul>
<li>Redefine your start logic,</li>
<li>Load one or multiple config files from <code>/etc/defaults</code> or <code>/etc/sysconfig</code>,</li>
<li>Deal with the PIDs, log and lock files,</li>
<li>Implement conditional logic to start/stop/restart one or more of the services,</li>
<li>Realize you&rsquo;ve messed something up,</li>
<li>Same player shoot again.</li>
</ul>


<p>Honestly: PITA.</p>

<h4>Built from source</h4>

<p>First things first: <strong>you shouldn&rsquo;t be building from source</strong> (unless you really, really need to).</p>

<p>Now if you do, you&rsquo;ll have to be thorough: there may be samples of init scripts in there, but you&rsquo;ll have to dig them out. <code>/contrib</code>, <code>/addons</code>, …it&rsquo;s never in the same place.</p>

<p>And that makes things &ldquo;fun&rdquo; when you&rsquo;re <a href="http://devo.ps/blog/2013/03/06/troubleshooting-5minutes-on-a-yet-unknown-box.html">trying to unscrew things on a box</a>:</p>

<ul>
<li>You figured out that MySQL is running from <code>/home/user/src/mysql</code>,</li>
<li>You check if there&rsquo;s an init script: no luck this time…</li>
<li>You try to understand what exactly launched <code>mysqld_safe</code>,</li>
<li>You spend a while digging into the bash history smiling at typos,</li>
<li>You stumble on a <code>run.sh</code> script (uncommented, of course) in the home directory. Funny enough, it seems to be starting everything from MySQL, NGINX and php-fpm to the coffee maker.</li>
<li>You make a mental note to try and track down the &ldquo;genius&rdquo; who did that mess of a job, and get busy with converting everything to a proper init script.</li>
</ul>


<p>Great.</p>

<h3>Why existing solutions suck</h3>

<p>Well, based on what we&rsquo;ve just seen, you really only have two options:</p>

<ol>
<li> <strong>DIY</strong>; but if you&rsquo;re good at what you do, you&rsquo;re probably also lazy. You may do it the first couple times, but that&rsquo;s not gonna scale, especially when dealing with the various flavors of init daemons (upstart, systemd…),</li>
<li> <strong>Use that thing called &ldquo;the Internet&rdquo;</strong>; you read through forum pages, issue queues, gists and if you&rsquo;re lucky you&rsquo;ll find a perfect one (or more likely 10 sucky ones). Kudos to all those of whom shared their work, but you&rsquo;ll probably be back to option 1.</li>
</ol>


<h3>We can do better than this</h3>

<p>You&rsquo;ll find a gazillion websites for pictures of kittens, but as far as I know, there is no authoritative source for init scripts. That&rsquo;s just not right: we have to fix it. A few things I&rsquo;m aiming for:</p>

<ul>
<li><strong>Scalable</strong>; allow for multiple instances of a service to be started at once from different config files (see the memcache/redis example),</li>
<li><strong>Secure</strong>; ensure <code>configtest</code> is run before a restart/reload (because, you know, a faulty config file preventing the service to restart is kind of a bummer),</li>
<li><strong>Smart</strong>; ensuring for example that the cache is aggressively flushed before restarting your database (so that you don&rsquo;t end-up waiting 50 min for the DB to cleanly shutdown).</li>
</ul>


<p><a href="https://github.com/devo-ps/init-scripts">I&rsquo;ve just created a repo</a> where I&rsquo;ll be dumping various init scripts that will hopefully be helpful to others. I&rsquo;d love to get suggestions or help.</p>

<p>And by the way, things are not much better with applications, though we&rsquo;re trying our best to improve things there too with things like <a href="https://github.com/Unitech/pm2">pm2</a> (fresh and shinny, more about it in a later post).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Goodbye node-forever]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/03/goodbye-node-forever/"/>
    <updated>2014-02-03T14:15:10-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/03/goodbye-node-forever</id>
    <content type="html"><![CDATA[<h2><a href="http://bvajjala.github.io/blog/2013/06/26/goodbye-node-forever-hello-pm2.html">Goodbye node-forever, hello PM2</a></h2>

<p><img src="http://apps.hemca.com/pm2/pres/pm22.png" alt="pm2 logo" /></p>

<p>It&rsquo;s no secret that the devo.ps team has a crush on Javascript; node.js in the backend, AngularJS for our clients, there isn&rsquo;t much of our stack that isn&rsquo;t at least in part built with it. Our approach of building <a href="http://devo.ps/blog/2013/01/31/farewell-to-regular-web-development-approaches.html">static clients and RESTful JSON APIs</a> means that we run a lot of node.js and I must admit that, despite all of it awesomeness, node.js still is a bit of a
headache when it comes to running in production. Tooling and best practices (think monitoring, logging, error traces…) are still lacking when compared to some of the more established languages.</p>

<p>So far, we had been relying on the pretty nifty <a href="https://github.com/nodejitsu/forever">node-forever</a>. Great tool, but a few things were missing:</p>

<ul>
<li>Limited monitoring and logging abilities,</li>
<li>Poor support for process management configuration,</li>
<li>No support for clusterization,</li>
<li>Aging codebase (which meant frequent failures when upgrading Node).</li>
</ul>


<p>This is what led us to write <a href="https://github.com/Unitech/pm2">PM2</a> in the past couple months. We thought we&rsquo;d give you a quick look at it while we&rsquo;re nearing a production ready release.</p>

<h3>So what&rsquo;s in the box?</h3>

<p>First things first, you can install it with <code>npm</code>:</p>

<pre><code>npm install -g pm2
</code></pre>

<p>Let&rsquo;s open things up with the usual comparison table:</p>

<p>FeatureForeverPM2</p>

<p>Keep Alive</p>

<p>✔</p>

<p>✔</p>

<p>Coffeescript</p>

<p>✔</p>

<p>Log aggregation</p>

<p>✔</p>

<p>API</p>

<p>✔</p>

<p>Terminal monitoring</p>

<p>✔</p>

<p>Clustering</p>

<p>✔</p>

<p>JSON configuration</p>

<p>✔</p>

<p>And now let me geek a tad more about the main features…</p>

<h3>Native clusterization</h3>

<p>Node v0.6 introduced the cluster feature, allowing you to share a socket across multiple networked Node applications. Problem is, it doesn&rsquo;t work out of the box and requires some tweaking to handle master and children processes.</p>

<p>PM2 handles this natively, without any extra code: PM2 itself will act as the master process and wrap your code into a special clustered process, as Nodejs does, to add some global variables to your files.</p>

<p>To start a clustered app using all the CPUs you just need to type something like that:</p>

<pre><code>$ pm2 start app.js -i max
</code></pre>

<p>Then;</p>

<pre><code>$ pm2 list
</code></pre>

<p>Which should display something like (ASCII UI FTW);</p>

<p><img src="http://apps.hemca.com/pm2/pres/pm2-list.png" alt="pm2 list" /></p>

<p>As you can see, your app is now forked into multiple processes depending on the number of CPUs available.</p>

<h3>Monitoring <em>a la</em> termcaps-HTOP</h3>

<p>It&rsquo;s nice enough to have an overview of the running processes and their status with the <code>pm2 list</code> command. But what about tracking their resources consumption? Fear not:</p>

<pre><code>$ pm2 monit
</code></pre>

<p>You should get the CPU usage and memory consumption by process (and cluster).</p>

<p><img src="http://apps.hemca.com/pm2/pres/pm2-monit.png" alt="pm2 monit" /></p>

<p><strong>Disclaimer</strong>: <a href="https://github.com/arunoda/node-usage">node-usage</a> doesn&rsquo;t support MacOS for now (feel free to PR). It works just fine on Linux though.</p>

<p>Now, what about checking on our clusters and GC cleaning of the memory stack?
Let&rsquo;s consider you already have an HTTP benchmark tool (if not, you should definitely check <a href="https://github.com/wg/wrk">WRK</a>):</p>

<pre><code>$ express bufallo     // Create an express app
$ cd bufallo
$ npm install
$ pm2 start app.js -i max
$ wrk -c 100 -d 100 http://localhost:3000/
</code></pre>

<p>In another terminal, launch the monitoring option:</p>

<pre><code>$ pm2 monit
</code></pre>

<p>W00t!</p>

<h3>Realtime log aggregation</h3>

<p>Now you have to manage multiple clustered processes: one who&rsquo;s crawling data, another who is processing stuff, and so on so forth. That means logs, lots of it. You can still handle it the old fashioned way:</p>

<pre><code>$ tail -f /path/to/log1 /path/to/log2 ...
</code></pre>

<p>But we&rsquo;re nice, so we wrote the <code>logs</code> feature:</p>

<pre><code>$ pm2 logs
</code></pre>

<p><img src="http://apps.hemca.com/pm2/pres/pm2-logs.png" alt="pm2 monit" /></p>

<h3>Resurrection</h3>

<p>So things are nice and dandy, your processes are humming and you need to do a hard restart. What now? Well, first, dump things:</p>

<pre><code>$ pm2 dump
</code></pre>

<p>From there, you should be able to resurrect things from file:</p>

<pre><code>$ pm2 kill     // let's simulate a pm2 stop
$ pm2 resurect // All my processes are now up and running 
</code></pre>

<h3>API Health point</h3>

<p>Let&rsquo;s say you want to monitor all the processes managed by PM2, as well as the status of the machine they run on (and maybe even build a nice Angular app to consume this API…):</p>

<pre><code>$ pm2 web
</code></pre>

<p>Point your browser at <code>http://localhost:9615</code>, aaaaand… done!</p>

<h3>And there&rsquo;s more…</h3>

<ul>
<li>Full tests,</li>
<li>Generation of <code>update-rc.d</code> (<code>pm2 startup</code>), though still very alpha,</li>
<li>Development mode with auto restart on file change (<code>pm2 dev</code>), still very drafty too,</li>
<li>Log flushing,</li>
<li>Management of your applications fleet via JSON file,</li>
<li>Log uncaught exceptions in error logs,</li>
<li>Log of restart count and time,</li>
<li>Automated killing of processes exiting too fast.</li>
</ul>


<h3>What&rsquo;s next?</h3>

<p>Well first, you could show your love on Github (we love stars):
<a href="https://github.com/Unitech/pm2">https://github.com/Unitech/pm2</a>.</p>

<p>We developed PM2 to offer an advanced and complete solution for Node process management. We&rsquo;re looking forward to getting more people helping us getting there: pull requests are more than welcome. A few things already on the
roadmap that we&rsquo;ll get right at once we have a stable core:</p>

<ul>
<li>Remote administration/status checking,</li>
<li>Built-in inter-processes communication channel (message bus),</li>
<li>V8 GC memory leak detection,</li>
<li>Web interface,</li>
<li>Persistent storage for monitoring data,</li>
<li>Email notifications.</li>
</ul>


<p>Special thanks to <a href="https://github.com/makara">Makara Wang</a> for concepts/tools and <a href="https://github.com/rlidwka">Alex Kocharin</a> for advices and pull requests.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ansible Simply Kicks Ass]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/03/ansible-simply-kicks-ass/"/>
    <updated>2014-02-03T14:12:31-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/03/ansible-simply-kicks-ass</id>
    <content type="html"><![CDATA[<h2><a href="http://bvajjala.github.io/blog/2013/07/03/ansible-simply-kicks-ass.html">Ansible Simply Kicks Ass</a></h2>

<p>The devo.ps team has been putting quite a few tools to the test over the years when it comes to managing infrastructures. We&rsquo;ve developed some ourselves and have adopted others. While the choice to use one over another is not always as clear-cut as we&rsquo;d like (I&rsquo;d love to rant about monitoring but will leave that for a later post), we&rsquo;ve definitely developed kind of a crush for <a href="https://github.com/ansible/ansible">Ansible</a> in the past 6 months. We went through years of using Puppet, then Chef and more recently Salt Stack, before Ansible gained unanimous adoption among our team.</p>

<p>What makes it awesome? Well, on top of my head:</p>

<ul>
<li>It&rsquo;s <strong>agent-less</strong> and works by default in <strong>push mode</strong> (that last point is subjective, I know).</li>
<li>It&rsquo;s <strong>easy to pick up</strong> (honestly, try and explain Chef or Puppet to a developer and see how long that takes you compared to Ansible).</li>
<li>It&rsquo;s <strong>just Python</strong>. It makes it easier for people like me to contribute (Ruby is not necessarily that mainstream among ops) and also means minimal dependency on install (Python is shipped by default with Linux).</li>
<li>It&rsquo;s <strong>picking up steam</strong> at an impressive pace (I believe we&rsquo;re at 10 to 15 pull requests a day).</li>
<li>And it has all of the good stuff: idempotence, roles, playbooks, tasks, handlers, lookups, callback plugins…</li>
</ul>


<p>Now, Ansible is still very much in its infancy and some technologies may not yet be supported. But there are a great deal of teams pushing hard on contributions, including us. In the past few weeks, for example, we&rsquo;ve
contributed both Digital Ocean and Linode modules. And we have a lot more coming, including some experimentations with Vagrant.</p>

<p>Now, an interesting aspect of Ansible, and one that makes it so simple, is that it comes by default with a tool-belt. Understand that it is shipped with a range of modules that add support for well known technologies: <a href="http://www.ansibleworks.com/docs/modules.html">EC2,
Rackspace, MySQL, PostgreSQL, rpm, apt,…</a>. This now includes our
Linode contribution. That means that with the latest version of Ansible you can spin off a new Linode box as easily as:</p>

<pre><code>ansible all -m linode -a "name='my-linode-box' plan=1 datacenter=2 distribution=99 password='p@ssword' "
</code></pre>

<p>Doing this with Chef would probably mean chasing down a knife plugin for adding Linode support, and would simply require a full Chef stack (say hello to RabbitMQ, Solr, CouchDB and a gazillion smaller dependencies). Getting
Ansible up and running is as easy as:</p>

<pre><code>pip install ansible
</code></pre>

<p>Et voila! You gotta appreciate the simple things in life. Especially the life of a sysadmin.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Code Reuse With Node.js]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/03/code-reuse-with-node-dot-js/"/>
    <updated>2014-02-03T14:09:35-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/03/code-reuse-with-node-dot-js</id>
    <content type="html"><![CDATA[<h2><a href="http://bvajjala.github.io/blog/2013/07/11/code-reuse-with-nodejs.html">Code Reuse With Node.js</a></h2>

<p><img src="http://devo.ps/images/posts/recycle.png" alt="Code recycling" /></p>

<p>Any project that grows to a decent size will need to re-use parts of its code extensively. That often means, through the development cycle, a fair amount of rewrites and refactoring exercises. Elegant code re-use is hard to pull off.</p>

<p>With node.js, which we use quite a bit at <a href="http://devo.ps">devo.ps</a>, the most common ways to do this often rely on prototype or class inheritance. The problem is, as the inheritance chain grows, managing attributes and functions
can become quite complex.</p>

<p>The truth is, people usually just need the objects. This led us to adopt a certain form of object-based prototyping. We believe it to be leaner and more straightforward in most cases. But before we get there, let&rsquo;s have a look at how people usually approach this issue.</p>

<h2>The &ldquo;Function copy&rdquo;</h2>

<p>Usually in the form of <code>this[key] = that[key]</code>. A quick example:</p>

<pre><code>var objectA = {
    lorem: 'lorem ipsum'
};
var objectB = {};

// Direct copy of a string, but you get the idea
objectB.lorem = objectA.lorem;
console.log(objectB); // Will output: { lorem: 'lorem ipsum' }
</code></pre>

<p>Crude, but it works. Next…</p>

<h2>Object.defineProperties()</h2>

<p>The previous method may work with simple structures, but it won&rsquo;t hold when
your use cases become more complex. That&rsquo;s when I usually call my buddy
<code>Object.defineProperties()</code>:</p>

<pre><code>var descriptor = Object.getOwnPropertyDescriptor;
var defineProp = Object.defineProperty;

var objectA = {};
var objectB = {};
var objectC = {};

objectA.__defineGetter__('lorem', function() {
    return 'lorem ipsum';
});
console.log(objectA); // Will output: { lorem: [Getter] }

// Direct copy, which copies the result of the getter.
objectB.lorem = objectA.lorem;
console.log(objectB); // Will output: { lorem: 'lorem ipsum' }

// Copying with Object.defineProperty(), and it copies the getter itself.
defineProp(objectC, 'lorem', descriptor(objectA, 'lorem'));
console.log(objectC); // Will output: { lorem: [Getter] }
</code></pre>

<p>I often use a library for that. A couple examples (more or less the same stuff
with different coding styles):</p>

<ol>
<li><p> <strong><a href="https://github.com/medikoo/es5-ext">es5-ext</a></strong></p>

<p> var extend = require(&lsquo;es5-ext/lib/Object/extend-properties&rsquo;);</p>

<p> var objectA = {};
 var objectC = {};</p>

<p> objectA.<strong>defineGetter</strong>(&lsquo;lorem&rsquo;, function() {
     return &lsquo;lorem ipsum&rsquo;;
 });</p>

<p> extend(objectC, objectA);
 console.log(objectC); // Will output: { lorem: [Getter] }</p></li>
<li><p> <strong><a href="https://github.com/devo-ps/carcass">Carcass</a></strong></p>

<p> var carcass = require(&lsquo;carcass&rsquo;);</p>

<p> var objectA = {};
 var objectC = {};</p>

<p> objectA.<strong>defineGetter</strong>(&lsquo;lorem&rsquo;, function() {
     return &lsquo;lorem ipsum&rsquo;;
 });</p>

<p> carcass.mixable(objectC);
 objectC.mixin(objectA);
 console.log(objectC); // Will output: { mixin: [Function: mixin], lorem: [Getter] }</p></li>
</ol>


<p>Slightly better, but not optimal. Now, let&rsquo;s see what we end up doing more and
more often:</p>

<h2>Prototyping through objects</h2>

<p>The basic idea is that we prepare some functions, wrap them into an object which then becomes a &ldquo;feature&rdquo;. That feature can then be re-used by simply merging it with the targeted structure (object or prototype).</p>

<p>Let&rsquo;s take the example of the <a href="https://github.com/devo-ps/carcass/blob/master/lib/proto/loaderSync.js">loaderSync</a> script in <a href="https://github.com/devo-ps/carcass">Carcass</a>:</p>

<pre><code>module.exports = {
    source: source,
    parser: parser,
    reload: reload,
    get: get
};

function get() {

(...)
</code></pre>

<p>Once you copy the functions to an object, this object becomes a &ldquo;loader&rdquo; that can load a &ldquo;source&rdquo; synchronously with a &ldquo;parser&rdquo;. A &ldquo;source&rdquo; can be a file path and the &ldquo;parser&rdquo; can be simply Node.js&#8217; <code>require</code> function.</p>

<p>Let&rsquo;s now see how to use this with a couple object builders. Once again, I&rsquo;ll borrow an example from Carcass; the <a href="https://github.com/devo-ps/carcass/blob/master/benchmark/proto.loaderSync.js">loaderSync benchmark script</a>. The first builder generates a function and copies the methods from what we&rsquo;ve prepared. The second one copies the methods to the prototype of a builder class:</p>

<pre><code>(...)

function LoaderA(_source) {
    function loader() {
        return loader.get();
    }
    loader.mixin = mixin;
    loader.mixin(loaderSync);
    loader.source(_source);
    return loader;
}

(...)

function LoaderC(_source) {
    if (!(this instanceof LoaderC)) return new LoaderC(_source);
    this.source(_source);
}
LoaderC.prototype.mixin = mixin;
LoaderC.prototype.mixin(loaderSync);

(...)
</code></pre>

<p>Here we can see the two approaches. Let&rsquo;s compare them quickly:</p>

<p>FeatureLoader ALoader C</p>

<p><strong>Instantiating</strong>
<code>var a = LoaderA(...)</code></p>

<p><code>var c = LoaderC(...)</code> or <code>var c = new LoaderC(...)</code></p>

<p><strong>Appearance</strong>
Generates a function</p>

<p>Builds a typical instance which is an object.</p>

<p><strong>Invoking directly</strong>
<code>a()</code> or <code>a.get()</code></p>

<p><code>c.get()</code></p>

<p><strong>Invoking as a callback</strong>
<code>ipsum(a)</code></p>

<p><code>ipsum(c.get.bind(c))</code></p>

<h2><strong>Performance † of instantiating</strong></h2>

<p>100x faster</p>

<p><strong>Performance of invoking</strong>
<em>idem</em></p>

<p><em>idem</em></p>

<p><strong>†</strong>: (check it yourself by <a href="https://github.com/devo-ps/carcass/blob/master/Makefile">benchmarking Carcass with <code>make bm</code></a>)</p>

<h3>&ldquo;Protos&rdquo; and beyond</h3>

<p>That last approach is gaining traction among our team; we prepare functions for our object builders (which, by the way, we call &ldquo;protos&rdquo;). While we still choose to use prototypes in some occurrences, it is mainly because it is
faster to get done. For the sake of convenience, we also sometimes rely on functions rather than objects to invoke our &ldquo;protos&rdquo;, however keep in mind that this is a performance trade-off.</p>

<p>I&rsquo;ll wrap this up mentioning one more method we use, admittedly less often: &ldquo;Object alter&rdquo;. The idea is to rely on an &ldquo;alter&rdquo; function designed to change objects passed to it. This is sometimes also called a &ldquo;mixin&rdquo;. An example from <a href="https://github.com/visionmedia/configurable.js">vsionmedia&rsquo;s trove of awesomeness on Github</a>:</p>

<pre><code>(...)

module.exports = function(obj){

    obj.settings = {};

    obj.set = function(name, val){
        (...)
    };

    (...)

    return obj;
};
</code></pre>

<h3>Resources</h3>

<ul>
<li><a href="http://killdream.github.io/2011/10/09/understanding-javascript-oop.html">A case for prototypes</a></li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object">MDN Object reference</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Automation And Friction]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/03/automation-and-friction/"/>
    <updated>2014-02-03T14:06:45-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/03/automation-and-friction</id>
    <content type="html"><![CDATA[<h2><a href="http://bvajjala.github.io/blog/2013/06/20/automation-and-friction.html">Automation And Friction</a></h2>

<p>I&rsquo;ll admit that the devo.ps team is a lazy bunch; we like to forget about things, especially the hard stuff. Dealing with a complex process invariably leads one of us to vent about how &ldquo;we should automate that stuff&rdquo;. That&rsquo;s what our team does day and night:</p>

<ol>
<li> Dumb things down, lower barriers of entry, and then…</li>
<li> <strong>Automate all the things!</strong></li>
</ol>


<p>This has transpired through every layer of our company, from engineering to operations. Recently we&rsquo;ve started pushing on a third point, but first let me rant a bit…</p>

<h3>The ever increasing surface of friction</h3>

<p>The past few years have seen a healthy push on UI and UX. Even developer tools and enterprise software, historically less user-friendly, have started adopting that trend. We now have things like Github. Great.</p>

<p>This trend grew in parallel with the adoption of SaaS. SaaS are the results of teams focused on specific problems, with the user experience often being a key component (not to undervalue good engineering). It&rsquo;s pretty standard for these services to offer an API for integration&rsquo;s sake. <a href="https://getbase.com">Our CRM</a> plays nicely with Dropbox, GMail and a gazillion other services. Again, great.</p>

<p><strong>However, the success of SaaS means the surface of interfaces we&rsquo;re dealing with is constantly stretching. This is far more difficult to overcome than poor UI or UX.</strong> Many of us have witnessed teams struggling to get adoption on a great tool that happen to be one too many. There&rsquo;s not much you can do about it.</p>

<h3>A bot to rule them all…</h3>

<p><img src="http://devo.ps/images/posts/borat.png" alt="Borat is omnipotent" /></p>

<p>Our team has tried a lot of different approaches over the years. We kicked the tires on a lot of products and ended up doing as usual:</p>

<ol>
<li><p> <strong>Simplify</strong>. For example, we use Github to manage most tasks and discussions, including operations (HR, admin, …), and marketing. We used <a href="http://trello.com/">Trello</a> alongside Github for a while and we loved it. But it silo-ed the discussions. Everything from our employee handbook to tasks for buying snacks for the office are now on Github. It also had an interesting side effect on transparency, but I&rsquo;ll talk about this another time.</p></li>
<li><p> <strong>Automate</strong>. We automate pretty much everything we can. When you apply to one of our job by email for example, we push the attachments in Dropbox (likely your resume) and create a ticket with the relevant information on Github. <a href="http://zapier.com">Zapier</a> is great for this kind of stuff by the way.</p></li>
<li><p> <strong>Make it accessible</strong>. That&rsquo;s the most important point for us at this stage. Borat, our <a href="http://hubot.github.com">Hubot</a> chat bot, is hooked up with most of our infrastructure and is able to pass on requests to the services we use as well as some of our automation. If one of us is awake, chances are you can find us on the chat, making it the most ubiquitous interface for our team:</p>

<ul>
<li>Need to deploy some code on production or modify some configuration on a server? Ask Borat, he&rsquo;ll relay your demands to the <a href="http://devo.ps">devo.ps</a> API.</li>
<li>Your latest commit broke the build? A new mail came from support? Expect to hear about it from Borat.</li>
<li>Need to use our time tracker? Just drop a message to the bot when you&rsquo;re starting your task and let him know when you&rsquo;re done.</li>
<li>Need to call for a SCRUM? Just mention the Github team you want to chat with and Borat will create a separate channel and invite the right people to join.</li>
<li>Somebody is at the door? Ask the bot to open it for you (you gotta love hacking on Raspberry PI).</li>
</ul>
</li>
</ol>


<p>Anybody with access to our bot&rsquo;s repository can add a script to hook him up to a new service. Git push, kill the bot and wait for him to come back to life with new skills. The tedious stuff ends up sooner or later scripted and one sentence away.</p>

<p>Really, try it. It&rsquo;s worth the investment.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Farewell to Regular Web Development Approaches]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/03/farewell-to-regular-web-development-approaches/"/>
    <updated>2014-02-03T13:52:12-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/03/farewell-to-regular-web-development-approaches</id>
    <content type="html"><![CDATA[<h2><a href="http://bvajjala.github.io/blog/2013/01/31/farewell-to-regular-web-development-approaches.html">Farewell to Regular Web Development Approaches</a></h2>

<p>At my <a href="http://wiredcraft.com">previous company</a>, we built Web applications for medium to large organizations, often in the humanitarian and non-profit space, facing original problems revolving around data. Things like building the
<a href="http://wiredcraft.com/work/southern-sudan-referendum/index.html">voting infrastructure for the Southern Sudan Referendum</a> helped us diversify our technical chops. But until a year ago, we were still mostly building regular Web applications; the user requests a page that we build and serve back.</p>

<p><strong>Until we started falling in love with APIs and static clients.</strong></p>

<p>It&rsquo;s not that we fundamentally disliked the previous approach. We just reached a point where we felt our goals weren&rsquo;t best served by this model. With lots of dynamic data, complex visualizations and a set of &ldquo;static&rdquo; interfaces, the traditional way was hindering our development speed and our ability to experiment. And so we got busy, experimenting at first with smaller parts of our projects (blog, help section, download pages…). We realized our use of complex pieces of softwares like content management systems had seriously biased our approach to problem solving. <strong>The CMS had become a boilerplate, an unchallenged dependency.</strong></p>

<p>We&rsquo;ve gradually adopted a pattern of building front-ends as static clients (may they be Web, mobile or 3rd party integrations) combined with, usually, one RESTful JSON API in the backend. And it works marvelously, thanks in part
to some awesome tech much smarter people figured out for us:</p>

<ul>
<li><a href="http://marionettejs.com">Marionette</a> and <a href="http://backbonejs.org">Backbone.js</a>, <a href="http://github.com/component/component">Component</a> (a personal favorite) and <a href="http://github.com/mojombo/jekyll">Jekyll</a> allow us to build static HTML5 + JS + CSS clients for Web and mobile,</li>
<li><a href="http://nodejs.org">node.js</a> and <a href="http://github.com/devo-ps/carcass">carcass</a> (alpha quality at this stage) in the backend for our APIs.</li>
</ul>


<p>Most of what we build at devo.ps is stemming from this accelerated recovery and follow usually that order:</p>

<ol>
<li> We start by defining our API interface through user stories and methods,</li>
<li> Both backend and front-end teams are getting cranking on implementing and solving the challenges specific to their part,</li>
</ol>


<p>A lot of things happen in parallel and changes on one side rarely impact the other: we can make drastic changes in the UI without any change on the backend. And there were a lot of unexpected gains too, in security, speed and
overall maintainability. More importantly, we&rsquo;ve freed a lot of our resources to focus on building compelling user experiences instead of fighting a large piece of software to do what we want it to do.</p>

<p>If you haven&rsquo;t tried building things that way yet, give it a try honestly (when relevant); it may be a larger initial investment in some cases but you&rsquo;ll come out on top at almost every level.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why We Dropped Swagger And I/O Docs]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/03/why-we-dropped-swagger-and-i-slash-o-docs/"/>
    <updated>2014-02-03T13:49:24-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/03/why-we-dropped-swagger-and-i-slash-o-docs</id>
    <content type="html"><![CDATA[<h2><a href="http://bvajjala.github.io/blog/2013/02/07/why-we-dropped-iodocs-and-swagger.html">Why We Dropped Swagger And I/O Docs</a></h2>

<p>As we started investing in <a href="http://bvajjala.github.io/blog/2013/01/31/farewell-to-regular-web-development-approaches.html">our new strategy</a> at <a href="http://wiredcraft.com">my previous company</a>, we looked around for solutions to document APIs. It may not be the sexiest part of the project, but documentation is the first step to designing a good API. And I mean <strong>first</strong> as in &ldquo;before you even start writing tests&rdquo; (yes, you should be writing tests first too).</p>

<p>We originally went with a simple Wiki page on Github, which served us just fine in the past. But it quickly became clear that it wasn&rsquo;t going to cut it. We started thinking about what good documentations is. We&rsquo;re fans of the single page approach that the <a href="http://backbonejs.com">Backbone.js documentation</a> illustrates well and clearly remembered <a href="http://developer.github.com/">Github</a> and <a href="https://stripe.com/docs">Stripe</a> as easy and well organized resources. Some Googling later, we were contemplating Wordnik&rsquo;s <a href="http://developers.helloreverb.com/swagger/">Swagger</a> and Mashery&rsquo;s <a href="http://www.mashery.com/product/io-docs">I/O Docs</a>. We later settled for I/O Docs as it is built with node.js and was more straightforward to set up (for us at least).</p>

<p>Once again, we hit a wall with this approach:</p>

<ol>
<li><p> <strong>No proper support for JSON body</strong>: we don&rsquo;t do much with parameters and mostly send JSON objects in the body of our requests, <a href="http://blog.apigee.com/detail/restful_api_design_nouns_are_good_verbs_are_bad">using HTTP verbs for the different types of operations</a> we perform on our collections and models in the backend. Swagger and I/O Docs fall short of support for it, letting you simply dump your JSON in a field: not ideal.</p></li>
<li><p> <strong>You&rsquo;re querying the actual API</strong>: to be fair, this is an intentional feature. Now some of you may find it interesting that your documentation allows users to easily run calls against your API. That&rsquo;s what <a href="http://www.flickr.com/services/api/explore/flickr.activity.userComments">Flickr does with their API explorer</a>, and we used to think it was pretty neat. But once we started using it, we saw the risks of exposing so casually API calls that can impact your platform (especially with <a href="http://devo.ps">devo.ps</a> which deals with your actual infrastructure). I guess you could set up a testing API for that very purpose, but that&rsquo;s quite a bit of added complexity (and <a href="http://blogoscoped.com/archive/2005-08-24-n14.html">we&rsquo;re lazy</a>).</p></li>
</ol>


<p>And that&rsquo;s how we ended up putting together <a href="https://github.com/devo-%20ps/carte">Carte</a>, a very lightweight Jekyll-based solution: drop a new post for each API call, following some loose format and specifying a few bits of meta data in the YAML header (type of the method, path…) and you&rsquo;re good to go.</p>

<p><a href="http://devo-ps.github.com/carte"><img src="http://bvajjala.github.io/images/posts/carte-screenshot.png" alt="Screenshot of Carte" /></a></p>

<p>We&rsquo;re real suckers for Jekyll. We&rsquo;ve actually used it to build quite a few static clients for our APIs. One of the advantages of this approach is that we can bundle our documentation with our codebase by simply pushing it on the
<code>gh-pages</code> branch, and it pops up as a Github page. That&rsquo;s tremendously important for us as it make it very easy for developers to keep the documentation and the code in synch.</p>

<p>Carte is intentionally crude: <a href="https://github.com/devo-ps/carte">have a look at the README and hack at will</a>. Drop us a shout at <a href="https://twitter.com/devo_ps">@devo_ps</a> if you need help or want to suggest a feature.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Best Practices: It's Always or Never]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/03/best-practices-its-always-or-never/"/>
    <updated>2014-02-03T13:45:48-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/03/best-practices-its-always-or-never</id>
    <content type="html"><![CDATA[<h2><a href="http://bvajjala.github.io/blog/2013/02/11/best-practices-it-s-always-or-never.html">Best Practices: It&rsquo;s Always Or Never ( And Preferably Always)</a></h2>

<p><img src="http://farm3.staticflickr.com/2584/4103140420_b98ee1ac62_z.jpg" alt="Messy cables" /></p>

<p>It&rsquo;s Monday morning. The development team needs a box and you&rsquo;re already contemplating the gazillion other urgent tasks that need to be done on the existing infrastructure. _Just that one time_TM, you&rsquo;re going to forget about your own rules. You&rsquo;re just gonna spawn an instance, set up the few services needed and be done with it. You&rsquo;ll drop some of the usual time suckers: backup strategy, access rules, init scripts, documentation… You can&rsquo;t just do the whole of it AND handle the rest of your day-to-day responsibilities. After all, it&rsquo;s just a development server and you&rsquo;ll probably fold it in a couple weeks, or you&rsquo;ll clean it up once your plate is a tad less full.</p>

<p>A few weeks later, the box is still there and your backlog is far from looking less crowded. The development team just rolled out their production application on the same box. <strong>And things start crashing… badly.</strong></p>

<p>After a couple of not so courteous emails from the dev team mentioning repetitive crashes, you log in the box and the fun starts. You can&rsquo;t figure out what services have been deployed, or how exactly they were installed. You can&rsquo;t restore the database because you don&rsquo;t know where the bloody backups are. You waste time to find out that CouchDB wasn&rsquo;t started at boot. All of this while receiving emails of &ldquo;encouragement&rdquo; from your colleagues.</p>

<p>Just because of that &ldquo;one time&rdquo;. Except that it&rsquo;s never just that one time.</p>

<h3>Best practices are not freaking optional</h3>

<p>I hear you: coming up with these best practices and sticking to it <strong>systematically</strong> is hard. It&rsquo;s high investment. But based on our common experience, it&rsquo;s one you can&rsquo;t afford not making. The &ldquo;quick and dirty that
one time&rdquo; approach will ultimately fail you.</p>

<p>A few things you should never consider skipping:</p>

<ul>
<li><p><strong>Document the hell out of everything as you go</strong>. You probably won&rsquo;t have time to get it done once you shipped it, and you probably won&rsquo;t remember what you did or why you did it in a few weeks from now. Your colleagues will probably appreciate too.</p></li>
<li><p><strong>Off-site backups for everything</strong>. Don&rsquo;t even think of keeping your backups on the same physical box. Disks fail (a lot) and storage like S3/Glacier is dirt cheap. Find out a way to backup your code and data and stick to it.</p></li>
<li><p><strong>Full setup and reliable sources</strong>. Avoid random AWS AMIs or RPM repositories. And when settings things up, go through the whole shebang: init script, dedicated running user, environment variables and such are not optional. Some of us also think that you shouldn&rsquo;t use rc.local for your Web services ever again.</p></li>
</ul>


<h3>Infrastructure As Code And Automation</h3>

<p>Obviously, given what we&rsquo;re working on at <a href="http://devo.ps">devo.ps</a>, we&rsquo;re pretty strong adopters of infrastructure as code and automation. What tools to use is a much larger discussion. Go have a look at the comments on <a href="http://news.ycombinator.com/item?id=5197389">the
announcement of the new version of Chef</a> to get an idea of what&rsquo;s
out there.</p>

<p>Ultimately these are just opinions, but behind them are concepts worth investing in. Capturing the work you do on your infrastructure in repeatable and testable code, and automating as much as you can helps removing yourself
from the equation. Doing so is helping you to reduce the human factor and free yourself of the repetitive boilerplate while you focus on the challenging tasks that only a creative brain can tackle.</p>

<p>Not building upon best practices is simply not an option. By doing so, you fail at investing in the foundation for a more robust infrastructure, and more importantly it is depriving you from scaling yourself.</p>

<p><em><a href="http://www.flickr.com/photos/comedynose/4103140420/">Picture from comedy_nose</a></em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Troubleshooting 5minutes on a yet Unknown Box]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/03/troubleshooting-5minutes-on-a-yet-unknown-box/"/>
    <updated>2014-02-03T13:34:05-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/03/troubleshooting-5minutes-on-a-yet-unknown-box</id>
    <content type="html"><![CDATA[<h2><a href="http://bvajjala.github.io/blog/2013/03/06/troubleshooting-5minutes-on-a-yet-unknown-box.html">First 5 Minutes Troubleshooting A Server</a></h2>

<p>Back when our team was dealing with operations, optimization and scalability at <a href="http://wiredcraft.com">our previous company</a>, we had our fair share of troubleshooting poorly performing applications and infrastructures of various sizes, often large (think CNN or the World Bank). Tight deadlines, &ldquo;exotic&rdquo; technical stacks and lack of information usually made for memorable experiences.</p>

<p>The cause of the issues was rarely obvious: here are a few things we usually got started with.</p>

<h3>Get some context</h3>

<p>Don&rsquo;t rush on the servers just yet, you need to figure out how much is already known about the server and the specifics of the issues. You don&rsquo;t want to waste your time (trouble) shooting in the dark.</p>

<p>A few &ldquo;must have&rdquo;:</p>

<ul>
<li>What exactly are the symptoms of the issue? Unresponsiveness? Errors?</li>
<li>When did the problem start being noticed?</li>
<li>Is it reproducible?</li>
<li>Any pattern (e.g. happens every hour)?</li>
<li>What were the latest changes on the platform (code, servers, stack)?</li>
<li>Does it affect a specific user segment (logged in, logged out, geographically located…)?</li>
<li>Is there any documentation for the architecture (physical and logical)?</li>
<li><strong>Is there a monitoring platform?</strong> Munin, Zabbix, Nagios, <a href="http://newrelic.com/">New Relic</a>… Anything will do.</li>
<li><strong>Any (centralized) logs?</strong>. Loggly, Airbrake, Graylog…</li>
</ul>


<p>The last two ones are the most convenient sources of information, but don&rsquo;t expect too much: they&rsquo;re also the ones usually painfully absent. Tough luck, make a note to get this corrected and move on.</p>

<h3>Who&rsquo;s there?</h3>

<pre><code>$ w
$ last
</code></pre>

<p>Not critical, but you&rsquo;d rather not be troubleshooting a platform others are playing with. One cook in the kitchen is enough.</p>

<h3>What was previously done?</h3>

<pre><code>$ history
</code></pre>

<p>Always a good thing to look at; combined with the knowledge of who was on the box earlier on. Be responsible by all means, being admin shouldn&rsquo;t allow you to break ones privacy.</p>

<p>A quick mental note for later, you may want to update the environment variable <code>HISTTIMEFORMAT</code> to keep track of the time those commands were ran. Nothing is more frustrating than investigating an outdated list of commands…</p>

<h3>What is running?</h3>

<pre><code>$ pstree -a
$ ps aux
</code></pre>

<p>While <code>ps aux</code> tends to be pretty verbose, <code>pstree -a</code> gives you a nice condensed view of what is running and who called what.</p>

<h3>Listening services</h3>

<pre><code>$ netstat -ntlp
$ netstat -nulp
$ netstat -nxlp
</code></pre>

<p>I tend to prefer running them separately, mainly because I don&rsquo;t like looking at all the services at the same time. <code>netstat -nalp</code> will do to though. Even then, I&rsquo;d ommit the <code>numeric</code> option (IPs are more readable IMHO).</p>

<p>Identify the running services and whether they&rsquo;re expected to be running or not. Look for the various listening ports. You can always match the PID of the process with the output of <code>ps aux</code>; this can be quite useful especially when you end up with 2 or 3 Java or Erlang processes running concurrently.</p>

<p>We usual prefer to have more or less specialized boxes, with a low number of services running on each one of them. If you see 3 dozens of listening ports you probably should make a mental note of investigating this further and see
what can be cleaned up or reorganized.</p>

<h3>CPU and RAM</h3>

<pre><code>$ free -m
$ uptime
$ top
$ htop
</code></pre>

<p>This should answer a few questions:</p>

<ul>
<li>Any free RAM? Is it swapping?</li>
<li>Is there still some CPU left? How many CPU cores are available on the server? Is one of them overloaded?</li>
<li>What is causing the most load on the box? What is the load average?</li>
</ul>


<h3>Hardware</h3>

<pre><code>$ lspci
$ dmidecode
$ ethtool
</code></pre>

<p>There are still a lot of bare-metal servers out there, this should help with;</p>

<ul>
<li>Identifying the RAID card (with BBU?), the CPU, the available memory slots. This may give you some hints on potential issues and/or performance improvements.</li>
<li>Is your NIC properly set? Are you running in half-duplex? In 10MBps? Any TX/RX errors?</li>
</ul>


<h3>IO Performances</h3>

<pre><code>$ iostat -kx 2
$ vmstat 2 10
$ mpstat 2 10
$ dstat --top-io --top-bio
</code></pre>

<p>Very useful commands to analyze the overall performances of your backend;</p>

<ul>
<li>Checking the disk usage: has the box a filesystem/disk with 100% disk usage?</li>
<li>Is the swap currently in use (si/so)?</li>
<li>What is using the CPU: system? User? Stolen (VM)?</li>
<li><code>dstat</code> is my all-time favorite. What is using the IO? Is MySQL sucking up the resources? Is it your PHP processes?</li>
</ul>


<h3>Mount points and filesystems</h3>

<pre><code>$ mount
$ cat /etc/fstab
$ vgs
$ pvs
$ lvs
$ df -h
$ lsof +D / /* beware not to kill your box */
</code></pre>

<ul>
<li>How many filesystems are mounted?</li>
<li>Is there a dedicated filesystem for some of the services? (MySQL by any chance..?)</li>
<li>What are the filesystem mount options: noatime? default? Have some filesystem been re-mounted as read-only?</li>
<li>Do you have any disk space left?</li>
<li>Is there any big (deleted) files that haven&rsquo;t been flushed yet?</li>
<li>Do you have room to extend a partition if disk space is an issue?</li>
</ul>


<h3>Kernel, interrupts and network usage</h3>

<pre><code>$ sysctl -a | grep ...
$ cat /proc/interrupts
$ cat /proc/net/ip_conntrack /* may take some time on busy servers */
$ netstat
$ ss -s
</code></pre>

<ul>
<li>Are your IRQ properly balanced across the CPU? Or is one of the core overloaded because of network interrupts, raid card, …?</li>
<li>How much is swappinness set to? 60 is good enough for workstations, but when it come to servers this is generally a bad idea: you do not want your server to swap… ever. Otherwise your swapping process will be locked while data is read/written to the disk.</li>
<li>Is <code>conntrack_max</code> set to a high enough number to handle your traffic?</li>
<li>How long do you maintain TCP connections in the various states (<code>TIME_WAIT</code>, …)?</li>
<li><code>netstat</code> can be a bit slow to display all the existing connections, you may want to use <code>ss</code> instead to get a summary.</li>
</ul>


<p>Have a look at <a href="http://www.lognormal.com/blog/2012/09/27/linux-tcpip-tuning/">Linux TCP tuning</a> for some more pointer as to how to tune your network stack.</p>

<h3>System logs and kernel messages</h3>

<pre><code>$ dmesg
$ less /var/log/messages
$ less /var/log/secure
$ less /var/log/auth
</code></pre>

<ul>
<li>Look for any error or warning messages; is it spitting issues about the number of connections in your conntrack being too high?</li>
<li>Do you see any hardware error, or filesystem error?</li>
<li>Can you correlate the time from those events with the information provided beforehand?</li>
</ul>


<h3>Cronjobs</h3>

<pre><code>$ ls /etc/cron* + cat
$ for user in $(cat /etc/passwd | cut -f1 -d:); do crontab -l -u $user; done
</code></pre>

<ul>
<li>Is there any cron job that is running too often?</li>
<li>Is there some user&rsquo;s cron that is &ldquo;hidden&rdquo; to the common eyes?</li>
<li>Was there a backup of some sort running at the time of the issue?</li>
</ul>


<h3>Application logs</h3>

<p>There is a lot to analyze here, but it&rsquo;s unlikely you&rsquo;ll have time to be exhaustive at first. Focus on the obvious ones, for example in the case of a LAMP stack:</p>

<ul>
<li><strong>Apache &amp; Nginx</strong>; chase down access and error logs, look for <code>5xx</code> errors, look for possible <code>limit_zone</code> errors.</li>
<li><strong>MySQL</strong>; look for errors in the <code>mysql.log</code>, trace of corrupted tables, innodb repair process in progress. Looks for slow logs and define if there is disk/index/query issues.</li>
<li><strong>PHP-FPM</strong>; if you have php-slow logs on, dig in and try to find errors (php, mysql, memcache, …). If not, set it on.</li>
<li><strong>Varnish</strong>; in <code>varnishlog</code> and <code>varnishstat</code>, check your hit/miss ratio. Are you missing some rules in your config that let end-users hit your backend instead?</li>
<li><strong>HA-Proxy</strong>; what is your backend status? Are your health-checks successful? Do you hit your max queue size on the frontend or your backends?</li>
</ul>


<h3>Conclusion</h3>

<p>After these first 5 minutes (give or take 10 minutes) you should have a better
understanding of:</p>

<ul>
<li>What is running.</li>
<li>Whether the issue seems to be related to IO/hardware/networking or configuration (bad code, kernel tuning, …).</li>
<li>Whether there&rsquo;s a pattern you recognize: for example a bad use of the DB indexes, or too many apache workers.</li>
</ul>


<p>You may even have found the actual root cause. If not, you should be in a good place to start digging further, with the knowledge that you&rsquo;ve covered the obvious.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ZooKeeper vs. Doozer vs. Etcd]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/03/zookeeper-vs-doozer-vs-etcd/"/>
    <updated>2014-02-03T13:18:39-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/03/zookeeper-vs-doozer-vs-etcd</id>
    <content type="html"><![CDATA[<h2><a href="http://bvajjala.github.io/blog/2013/09/11/zookeeper-vs-doozer-vs-etcd.html">ZooKeeper vs. Doozer vs. Etcd</a></h2>

<p>While <a href="http://devo.ps">devo.ps</a> is fast approaching a public release, the team has been dealing with an increasingly complex infrastructure. We more recently faced an interesting issue; how do you share configuration across a cluster of servers? More importantly, how do you do so in a resilient, secure, easily deployable and speedy fashion?</p>

<p>That&rsquo;s what got us to evaluate some of the options available out there;
ZooKeeper, Doozer and etcd. These tools all solve similar sets of problems but their approach differ quite significantly. Since we spent some time evaluating them, we thought we&rsquo;d share our findings.</p>

<h3>ZooKeeper, the old dog</h3>

<p><a href="http://zookeeper.apache.org/">ZooKeeper</a> is the most well known (and oldest) project we&rsquo;ve looked into. It&rsquo;s used by a few big players (Rackspace, Yahoo, eBay, <a href="https://news.ycombinator.com/item?id=6367979">Youtube</a>) and is pretty mature.</p>

<p>It was created by Yahoo to deal with distributed systems applications. I strongly recommend you <a href="http://developer.yahoo.com/blogs/hadoop/apache-zookeeper-making-417.html">read the &ldquo;making of&rdquo;</a> if you&rsquo;re interested in understanding where Yahoo came from when they wrote it.</p>

<p>It stores variables in a structure similar to a file system, an approach that both Doozer and etcd still follow. With ZooKeeper, you maintain a cluster of servers communicating with each other that share the state of the distributed configuration data. Each cluster elects one &ldquo;leader&rdquo; and clients can connect to any of the servers within the cluster to retrieve the data. Zookeeper uses its own algorithm to handle distributed storage.</p>

<ul>
<li><p><strong>Pros</strong>:</p>

<ul>
<li><strong>Mature technology</strong>; it is used by some big players (eBay, Yahoo et al).</li>
<li><strong>Feature-rich</strong>; lots of client bindings, tools, API…</li>
</ul>
</li>
<li><p><strong>Cons</strong>:</p>

<ul>
<li><strong>Complex</strong>; ZooKeeper is not for the faint of heart. It is pretty heavy and will require you to maintain a fairly large stack.</li>
<li><strong>It&#8217;s… Java</strong>; not that we especially hate Java, but it is on the heavy side and introduce a lot of dependencies. We wanted to keep our machines as lean as possible and usually shy away from dependency heavy technologies.</li>
<li><strong>Apache…</strong>; we have mixed feelings about the Apache Foundation. <a href="http://www.infoworld.com/d/open-source-software/has-apache-lost-its-way-225267">&ldquo;Has Apache Lost Its Way?&rdquo;</a> summarizes it pretty well.</li>
</ul>
</li>
</ul>


<h3>Doozer, kinda dead</h3>

<p><a href="https://github.com/ha/doozerd">Doozer</a> was developed by Heroku a few years
ago. It&rsquo;s written in Go (yay!), which means it compiles into a single binary
that runs without dependencies. On a side-note, if you&rsquo;re writing code to
manage infrastructure, you should spend some time <a href="http://golang.org/">learning
Go</a>.</p>

<p>Doozer got some initial excitement from the developer community but seems to
have stalled more recently, with many forks being sporadically maintained and
no active core development.</p>

<p>It is composed of <a href="https://github.com/ha/doozerd">a daemon</a> and <a href="https://github.com/ha/doozer">a
client</a>. Once you have at least one Doozer
server up, you can add any number of servers and have clients get and set data
by talking to any of the servers within that cluster.</p>

<p>It was one of the first practical implementations (as far as I know) of the
<a href="http://en.wikipedia.org/wiki/Paxos_(computer_science">Paxos algorithm</a>). This
means operations can be slow when compared to dealing with a straight database
since cluster-wide consensus needs to be reached before committing any
operation.</p>

<p>Doozer was a step in the right direction. It is simple to use and setup.
However, after using it for a while we started noticing that a lot of its
parts felt unfinished. Moreover, it wasn&rsquo;t answering some of our needs very
well (encryption and ACL).</p>

<ul>
<li><p><strong>Pros</strong>:</p>

<ul>
<li><strong>Easy to deploy, setup and use</strong> (Go, yay!)</li>
<li><strong>It works</strong>; lots of people have actually used it in production.</li>
</ul>
</li>
<li><p><strong>Cons</strong>:</p>

<ul>
<li><strong>Pretty much dead</strong>: the core project hasn&rsquo;t been active in a while (1 commit since May) and is pretty fragmented (150 forks…).</li>
<li><strong>Security</strong>; no encryption and a fairly simple secure-word based authentication.</li>
<li><strong>No ACL</strong>; and we badly needed this.</li>
</ul>
</li>
</ul>


<h3>etcd</h3>

<p>After experiencing the shortcomings of Doozer, we stumbled upon a new
distributed configuration storage called
<a href="https://github.com/coreos/etcd">etcd</a>. It was first released by the
<a href="http://coreos.com">CoreOS</a> team a month ago.</p>

<p>Etcd and Doozer look pretty similar, at least on the surface. The most obvious
technical difference is that ectd uses the <a href="http://en.wikipedia.org/wiki/Raft_%28computer_science%29">Raft
algorithm</a> instead
of Paxos. Raft is designed to be <a href="https://ramcloud.stanford.edu/wiki/%0Adownload/attachments/11370504/raft.pdf">simpler</a> and
<a href="http://kellabyte.com/2013/05/09/an-alternative-to-paxos-the-raft-%0Aconsensus-algorithm/">easier</a> to implement than Paxos.</p>

<p>Etcd&rsquo;s architecture is similar to Doozer&rsquo;s. It does, however, store data
persistently (writes log and snapshots), which was of value to us for some
edge cases. It also has a better take on security, with CA&rsquo;s, certs and
private keys. While setting it up is not straightforward, it adds conveniency
and safety of mind.</p>

<p>Beyond the fact that it answered some of our more advanced needs, we were
seduced (and impressed) by the development pace of the project.</p>

<ul>
<li><p><strong>Pros</strong>:</p>

<ul>
<li><strong>Easy to deploy, setup and use</strong> (yay Go and yay HTTP interfaces!).</li>
<li><strong>Data persistence</strong>.</li>
<li><strong>Secure</strong>: encryption and authentication by private keys.</li>
<li><strong>Good documentation</strong> (if a little bit obscure at times).</li>
<li>Planned ACL implementation.</li>
</ul>
</li>
<li><p><strong>Cons</strong>:</p>

<ul>
<li>(Very) <strong>young project</strong>; interfaces are still moving pretty quickly.</li>
<li>Still not a perfect match, especially in the way that data is spread.</li>
</ul>
</li>
</ul>


<h3>The DIY approach (yeah, right..?)</h3>

<p>It is only fair that technical teams may rely on their understanding of their
infrastructure and coding skills to get <em>something that just works™</em> in place.
We haven&rsquo;t seriously considered this approach as we felt that getting security
and distributed state sharing right was going to be a bigger endeavor than we
could afford (the backlog is full enough for now).</p>

<h3>Conclusion</h3>

<p>In the end, we decided to give etcd a try. So far it seems to work well for
our needs and the very active development pace seems to validate our choice.
It has proven resilient and will likely hold well until we have the resources
to either customize its data propagation approach, or build our own solution
that will answer some needs it is not likely to answer (we&rsquo;ve already looked
into doing so with ZeroMQ and Go).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploy reveal.js slideshow on github-pages]]></title>
    <link href="http://bvajjala.github.io/blog/2013/11/05/deploy-reveal-dot-js-slideshow-on-github-pages/"/>
    <updated>2013-11-05T21:03:00-05:00</updated>
    <id>http://bvajjala.github.io/blog/2013/11/05/deploy-reveal-dot-js-slideshow-on-github-pages</id>
    <content type="html"><![CDATA[<p>Originally posted on <a href="http://www.tikalk.com">Tikal Knolwedge&rsquo;s webiste</a> @ this <a href="http://www.tikalk.com/alm/deploy-revealjs-slideshow-github-pages">link</a><br></p>

<p><img class="left" src="http://bvajjala.github.io/assets/images/gh-pages.png" title="'gh-pages'" >Deploy <a href="https://github.com/hakimel/reveal.js/">reveal.js slideshow</a> on github-pages was ridiculously easy.</p>

<p>Cloned reveal.js master branch to my local machine:</p>

<pre><code>git clone git@github.com:hakimel/reveal.js.git
</code></pre>

<p>Remove the history &ndash; of you want to preserve history ( <strong>git remote rm origin</strong> and add your new remote)</p>

<pre><code>rm -Rf .git 
</code></pre>

<p>Add your new remote (you need to create a repository first)</p>

<pre><code>git remote add origin git@github.com:tikalk-cookbooks/chef_workshop_presentation.git
</code></pre>

<p>You know the rest &hellip;</p>

<pre><code>git add .
git commit -m "Initial commit"
git push origin master
</code></pre>

<p>In order to push master to github pages you should create a branch names <em><strong>gh-pages</strong></em> and it&nbsp;<strong>must</strong> be named that, the commit &amp; push to this branch and about 2 minuets later you can browse to your presentation on gh-pages.</p>

<p><em><strong>Create a branch</strong></em>:</p>

<pre><code>git branch hg-pages
git push origin gh-pages&lt;/pre&gt;
</code></pre>

<p>updating this &quot;site&quot; can be done by editing the stuff you want on the &quot;master&quot; branch, then merge     those changes in the gh-pages branch and finally push those changes to the remote gh-pages which will automatically deploy youe reveal.js presentaion.</p>

<p>the merge sould look somthing like:</p>

<pre><code>hagzag-Lap-hagzag:chef_workshop_presentaion(master) $ git checkout gh-pages&amp;nbsp;

hagzag-Lap-hagzag:chef_workshop_presentaion(gh-pages) $ git merge master&amp;nbsp;
Updating 0f4f1e1..fb0d73d

Fast-forward

css/custom.css | &amp;nbsp; 4 ++++

index.html &amp;nbsp; &amp;nbsp; | 104 +++++++++++++++++++++++++++++++++++++++++++++++++-----------    --------------------------------------------

2 files changed, 53 insertions(+), 55 deletions(-)

create mode 100644 css/custom.css

git push origin gh-pages
</code></pre>

<p>the url is built from the followng pattern:</p>

<pre><code>[github_username].github.io/[repo_name]
</code></pre>

<p>in this case:</p>

<p><a href="http://tikalk-cookbooks.github.io/chef_workshop_presentation/#/">http://tikalk-cookbooks.github.io/chef_workshop_presentation/</a></p>

<p>Hope you find this useful !</p>

<p>HP</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Manage Data Bags in Solo mode]]></title>
    <link href="http://bvajjala.github.io/blog/2013/10/27/manage-data-bags-in-solo-mode/"/>
    <updated>2013-10-27T15:08:00-04:00</updated>
    <id>http://bvajjala.github.io/blog/2013/10/27/manage-data-bags-in-solo-mode</id>
    <content type="html"><![CDATA[<p><img class="left" src="http://bvajjala.github.io/assets/images/Opscode_chef_logo.png" title="'Chef Logo'" > For the past ~6 monthes or so I have been working solely with <a href="http://docs.opscode.com/chef_solo.html">chef-solo</a>.
There are quite a few helpers for solo out there such as:</p>

<ol>
<li>soloist &ndash; <a href="https://github.com/mkocher/soloist">https://github.com/mkocher/soloist</a></li>
<li>knife-solo &ndash; <a href="https://github.com/matschaffer/knife-solo">https://github.com/matschaffer/knife-solo</a>
&amp; a few more.</li>
</ol>


<p>What kept annoying me for some time is I couldn&rsquo;t manage databas with knife whilst working in solo mode &hellip; ARRRRRRGH!!!</p>

<p>It looked somthing like:</p>

<pre><code>[oper@sandbox chef_repo]$ knife data bag create admins
ERROR: The object you are looking for could not be found
Response: &lt;!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN"&gt;
&lt;html&gt;&lt;head&gt;
&lt;title&gt;404 Not Found&lt;/title&gt;
&lt;/head&gt;&lt;body&gt;
&lt;h1&gt;Not Found&lt;/h1&gt;
&lt;p&gt;The requested URL /data was not found on this server.&lt;/p&gt;
&lt;hr&gt;
&lt;address&gt;Apache/2.2.15 (CentOS) Server at : Port 80&lt;/address&gt;
&lt;/body&gt;&lt;/html&gt;
</code></pre>

<p>Then came <a href="https://github.com/thbishop/knife-solo_data_bag">knife-solo_data_bag</a> I am quite embaressed to say ;) I haven&rsquo;t found this sooner.
Now with a totally bogus knife.rb file I can generate / edit / manager databags with knife-solo &amp; knife-solo_data_bag.</p>

<p>Again, hope you find this useful, I know I do ;)</p>

<p>HP</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[S3 - Mounting s3 buckets with S3fs]]></title>
    <link href="http://bvajjala.github.io/blog/2013/10/20/mounting-s3-buckets-with-f3fs/"/>
    <updated>2013-10-20T20:32:00-04:00</updated>
    <id>http://bvajjala.github.io/blog/2013/10/20/mounting-s3-buckets-with-f3fs</id>
    <content type="html"><![CDATA[<p><img class="left" src="http://bvajjala.github.io/assets/images/amazon-s3.png" title="'S3 logo'" >A <code>cheap</code> solution on Amazon s3 for uploading files to s3 via SSH</p>

<p>I needed to upload files from my Corporate environment in a <code>push mode</code> meaning I do not open any &ldquo;special&rdquo; ports in my environemnt in order to enable users to put files on S3.
I am curentelly testing to see how robust this solution really is but basically what I found myself doing is install s3fs (<a href="https://code.google.com/p/s3fs/">link to project page</a>) like so:</p>

<pre><code>wget http://s3fs.googlecode.com/files/s3fs-1.73.tar.gz
tar xvzf s3fs-1.73.tar.gz
cd s3fs-1.73
./configure --prefix=/usr
make &amp;&amp; make install
</code></pre>

<p>I was installing this for Suse but for Amazon linux/redhat etc you might find a package see: <a href="http://rpmfind.net/linux/rpm2html/search.php?query=fuse-s3fs">here</a>.</p>

<p>Once the package is installed you can use <code>s3fs</code></p>

<pre><code>s3fs BUCKET:[PATH] MOUNTPOINT [OPTION]...

s3fs dev_tools /mnt/dev_tools/ 
</code></pre>

<p>Worth noting you do not need to specify the s3 url, you only specify the bucket name !</p>

<p>The same thing / very similir in <code>/etc/fstab</code> will look like so:</p>

<pre><code>s3fs#dev_tools /mnt/dev_tools fuse allow_other,user=youruser 0 0
</code></pre>

<p>The mount opts are <strong>extreemly important</strong> &ndash; without the <code>allow_other</code> flag the user cannot write to the directory.</p>

<p>This is really awesome &ndash; we now jsut need to make sure the connectivity is reliable / fast enough and this will become very usefull.</p>

<p>As always hope you find this useful.</p>
]]></content>
  </entry>
  
</feed>
