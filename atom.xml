<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Balaji Vajjala's Blog]]></title>
  <link href="http://bvajjala.github.io/atom.xml" rel="self"/>
  <link href="http://bvajjala.github.io/"/>
  <updated>2014-04-16T12:49:58-04:00</updated>
  <id>http://bvajjala.github.io/</id>
  <author>
    <name><![CDATA[Balaji Vajjala]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 4]]></title>
    <link href="http://bvajjala.github.io/blog/2014/03/25/autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-4/"/>
    <updated>2014-03-25T11:23:17-04:00</updated>
    <id>http://bvajjala.github.io/blog/2014/03/25/autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-4</id>
    <content type="html"><![CDATA[<h3>Autoscaling for LAMP on AWS | Choosing an instance type: Part 4</h3>

<p>As mentioned in <a href="../autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-1/index.html" title="Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 1">part 1 of this
series</a> (Creating
a LAMP Stack AMI), a common concern among most customers is to choose
the right instance type.</p>

<p>It is important to do the capacity planning. Before you choose instance
type ask yourself the following three questions:</p>

<ol>
<li><p>Is the application Memory intensive CPU intensive or Network
intensive? For this question to be meaningful put a monitoring
system in place and collect the data for a few weeks of real usage.</p></li>
<li><p>What is the expected request count at peak hours?</p></li>
<li><p>What is the minimum number of instances you want to run in
non-business hours?</p></li>
</ol>


<p><a href="https://docs.google.com/a/flux7.com/spreadsheet/ccc?key=0AkmUqlScRGp8dGJTNndjY0VjTURid3FTV2dNMEljOUE#gid=0">https://docs.google.com/a/flux7.com/spreadsheet/ccc?key=0AkmUqlScRGp8dGJTNndjY0VjTURid3FTV2dNMEljOUE#gid=0</a></p>

<p>Make sure to run at least 2 servers for high availability.  During times
of fewer loads, choose two m1.small instances for non-business hours
like weekends. This would optimize cost instead of using 2 large
instances for the same request.</p>

<p>Different instance types have different capacity levels. If the
application is memory intensive choose and use m1. class. If the
application is CPU intensive then choose and use c1. class.</p>

<p>Network bandwidth varies between different instance types. If the
application requires more bandwidth (ex: video streaming applications)
choose higher instance types irrespective of Memory or CPU to get better
bandwidth.</p>

<p>To know more about each instance types visit the following link.</p>

<p><a href="http://aws.amazon.com/ec2/instance-types/">http://aws.amazon.com/ec2/instance-types/</a></p>

<p>Start with micro/small instance for any workload. Do the Load run
starting with the minimum number of users and increase the load
gradually. Also scale the servers vertically or horizontally gradually
until it reaches the maximum capacity. This would give a clearer picture
of the number of instances required to serve the maximum or minimum
load. Closely watch the CloudWatch graphs to understand usage statistics
better.</p>

<p>An interesting question at this juncture could be as follows: Can 4
m1.medium instances be preferred to 2 large instances? Yes of course.
However, network bandwidth varies from one instance type to other. Given
that, it is wise to choose 2 large instances instead of 4 m1.medium
instances as it is easier to handle lesser number of instances.</p>

<p>(Note: DB server health needs to be checked when a load test is run.
Increasing the app servers count may not improve the performance all the
time. If DB queries are the bottleneck, chances are high for a bad
performance. Consider scaling up the DB server capacity as well.)</p>

<p>Based on the instance type identified from the load run, tune the PHP
memory settings and Apache prefork MPM client connections.</p>

<p>Check out the following links to know more about fine tuning Apache and
PHP</p>

<p><a href="http://www.hosting.com/support/linux/tuning-the-apache-prefork-mpm/">http://www.hosting.com/support/linux/tuning-the-apache-prefork-mpm/</a></p>

<p><a href="http://icreatestuff.co.uk/blog/article/apache-performance-tuning">http://icreatestuff.co.uk/blog/article/apache-performance-tuning</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 3]]></title>
    <link href="http://bvajjala.github.io/blog/2014/03/25/autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-3/"/>
    <updated>2014-03-25T11:23:14-04:00</updated>
    <id>http://bvajjala.github.io/blog/2014/03/25/autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-3</id>
    <content type="html"><![CDATA[<h3>Autoscaling for LAMP on AWS | Load Test the Environment: Part 3</h3>

<p>In part 3 of the Autoscaling for LAMP on AWS series,</p>

<p>After setting up your application autoscaling, it’s important to load
run the application in order to understand the minimum and maximum
number of instances required for each application.</p>

<p><a href="../autoscaling-for-lamp-on-aws-setting-up-austoscaling-groups-part-2/index.html" title="Autoscaling for LAMP on AWS | Setting Up Austoscaling Groups : Part 2">Read Part 2 to learn more about setting up autoscaling
groups.</a></p>

<p>First, fire the load run. For this article we’ve used HP LoadRunner
because it provides more detailed results than others, but there are
also other load runner tools to choose from. Be sure to run the load
from multiple IP addresses or else the ELB may not distribute it equally
across all instances. HP LoadRunner helps you use multiple nodes in
order to generate the load.</p>

<p>Update the Auto Scaling Group’s minimum and maximum instance counts.\</p>

<p><img src="https://lh5.googleusercontent.com/nXhTs4kEqjzxfAy8aGdgqLvC_yKninSiyM3pClGWhemLXPWFjr_6Y5NU8xnf2j7twuJgVx2TO4tvs8fjm2K1c8y6C23NN8lacgef-JEH4621AXZITbYAAKRF4w" alt="" /></p>

<p>For the this article, we’ve set the minimum and maximum instance counts
at 2 and 8 respectively.</p>

<p>Now change your Auto Scaling Policies. A scaling policy specifies
whether to scale the auto scaling group up or down, and by how much.
Here we’ve chosen 2 instances for Scale Up and Scale Down policies.
These policies automatically increase or decrease the instances
according to your defined conditions in order to maintain performance
and minimize cost.</p>

<p><img src="https://lh3.googleusercontent.com/3G9JdQa1GNjF_2fWOrZ10y-cpc3m8_6gtTH2Y046OK3LoiaMYGx0Nm_N4LsBcBhhRhA3cxr7D1sMlCv90ewW9rZ8QCr8amOHqivVcZZmML7Bx-XMIeM7wdN1Nw" alt="" /></p>

<p>Next, <strong>increase the load</strong> gradually with LoadRunner. Once the CPU
reaches 75% of the instances, Autoscaling will trigger the scale up
policy to launch 2 more instances.</p>

<p>Now <strong>decrease the load</strong> gradually and the CPU instances load should
come down to less than 45%. Autoscaling will then trigger the scale down
policy and delete the additional instances that were launched.</p>

<p>Consider a scenario in which a 2,000-concurrent-request count is reached
during peak hours and 500 users are reached during non-peak hours. For
best load run results, try the following.</p>

<p>Capture load test results and CloudWatch metrics for each of the
following scenarios:</p>

<p>​1. Start the load test with 500 users. Set the minimum instance count
to 2 if you choose large or medium types, and set the maximum instance
count to 4.</p>

<p>​2. Increase the load to 1,000 users. Set the minimum and maximum
instance counts to 2 and 6 respectively.</p>

<p>​3. Increase the load to 2,000 users. Set the minimum and maximum
instance counts to 4 and 10 respectively.</p>

<p>​4. Increase the load to 3,000 users. Set the minimum and maximum
instance counts to 6 and 12 respectively.</p>

<p>Analyze the results for each scenario and note which gives better
throughput and response times. Also check the DB metrics.</p>

<p>Now it will be easy to choose the best instance type and max/min
instance counts..</p>

<p><strong>Shutdown Autoscaling</strong></p>

<p>Update the autoscale group to resize the instance count to 0. This will
terminate all instances launched within the lamp-asg autoscale group.</p>

<p><em>Watch out for the final part of this series tomorrow on how to choose
an instance type. </em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 2]]></title>
    <link href="http://bvajjala.github.io/blog/2014/03/25/autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-2/"/>
    <updated>2014-03-25T11:23:10-04:00</updated>
    <id>http://bvajjala.github.io/blog/2014/03/25/autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-2</id>
    <content type="html"><![CDATA[<h3>Autoscaling for LAMP on AWS | Setting Up Austoscaling Groups : Part 2</h3>

<p>In part 2 of the Autoscaling LAMP in AWS series, let’s discuss how to
create autoscaling launch configuration, autoscaling groups and how to
verify the setup autoscaling.</p>

<h2><strong>Autoscale Implementation</strong> </h2>

<p>Autoscale configuration is now available in console. AWS command lines
are no longer needed for implementation.</p>

<p>Complete the following steps as detailed in <a href="../autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-1/index.html" title="Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 1">Part
1</a>of
this series in order to set up autoscaling :</p>

<ul>
<li><p>Configure AMI to launch the Instances.</p></li>
<li><p>Configure Instance type to launch the instances. (Example:
m1.small,m1.large).</p></li>
<li><p>Configure KeyPair Name to access the machines.</p></li>
<li><p>Configure Security Group to allow the Instances to communicate with
other components.</p></li>
<li><p>Keep the ELB name readily available.</p></li>
<li><p>Keep your availability zones ready. (Example: us-east-1a,
us-east-1b).</p></li>
<li><p>Set the minimum number of instances for Maximum and Desired
Capacity. (Start with zero).</p></li>
<li><p>Set Health Check Type. (ELB).</p></li>
<li><p>Set Region.</p></li>
<li><p>Change Capacity Cooldown time.</p></li>
<li><p>Adjust for scale up and scale down.</p></li>
</ul>


<h2><strong>1. Create the Autoscaling Launch Config</strong> </h2>

<p>Log in to the AWS console and navigate to Services-> EC2-> Launch
Configuration.</p>

<p><img src="https://lh5.googleusercontent.com/-E7LhgP_GnMeKXQHtHPieeiqZBbrD3TauY5FHvA90y2hvnWMoncO5gEJASGmYd_v1HjDIzIr949-gjX7N2DlCtzCyO5iwZnUd481qEyiHKVkxhH40xDL3xiTzQ" alt="" /></p>

<p>Click on “Create Autoscaling Group”.\</p>

<p><img src="https://lh6.googleusercontent.com/8MJM4YJXpOnamUnIZdfPkHZDJJ_iyVXg1qj373NTmPKZbihu4C2nqYcm2zvCacxycBN1OjzxbpANVSI7W4t7a8LYo15FbooKxx0UlXXkpJwuWxSNX7ZfSYm-PA" alt="" /></p>

<p>On the next screen, click on “Create Launch Configuration”.\</p>

<p><img src="https://lh6.googleusercontent.com/8h6Tn5ncvwmpG6qqI5vqulA_qoG6ijs4wtOOWA6nDwuCUCF1_OpD1VSWhlJDimgNNyhp_dmEsVx4DNJkls043s_CbBJ3HK0VUsSUphYjNeM3Qnlx1clgXZ5jAA" alt="" /></p>

<p>Create a new launch configuration. The name of the launch configuration
must be unique within the scope of the client’s AWS account.</p>

<p><strong>Choose AMI:</strong> Go to My AMIs and select the LAMP AMI created.\</p>

<p><img src="https://lh4.googleusercontent.com/SJXQcn4aWSm83eKHuKKZHJomaUvLwJShg8w5M1Q6wEXXNiUv61JNoiaKMYlaacP4TYT8kTvJju2vDKK23RReWFLgLxRGb68xaSuMbHzjQ0H74evkNtINH_T_xQ" alt="" /></p>

<p><strong>Choose Instance Type:</strong> We’ve selected a micro instance for our
example.\</p>

<p><img src="https://lh3.googleusercontent.com/e7lh5P4VR8i9qJ5QgxUK1PBN6in9HQNpU0cjznY6Giemq8RZrF2wBEfmlL_cCS_cDUBKKUHkmU__YC6YPiNncXXhbXMq9BRE3tLSbDewJR8MZ8upoWi0OwIp2w" alt="" /></p>

<p><strong>Configure Details:</strong> Give a name for the Launch Configuration\</p>

<p><img src="https://lh4.googleusercontent.com/kP646CGpnJLDO-44macZIU6A4FMSeLpu8kSq6hpFIstb_heGHWMCRwtn5EpOw8YZdvCANbd-5Pur5SNw_OaWg7432_WdQ1Rbi9QUsR6FMUIo71Y6REwKA5wJ2w" alt="" /></p>

<p><strong>Add Storage:</strong> Keep the values on default.</p>

<p><strong>Configure the Security Group:</strong>Select the Security group to launch the
autoscaling instances. Review the details and create the Launch
Configuration.\</p>

<p><img src="https://lh5.googleusercontent.com/rd0tjYvJyMzG0irkmsj9LZSXrxigf93tfOhOLUtcrQ-c1Pk2XXsOF7ksRbu4BY_6PvPfMWqC9bF4NZcy1aNLBOfUd91vjV9shjfA9IRAA5MNSJXb6sPuVa8gHA" alt="" /></p>

<p>In the next window, select “KeyPair” to access the instances.</p>

<h2><strong>2. Create the Auto Scaling Group</strong> </h2>

<p>Create a new Auto Scaling group with a specified name and other
attributes. When you make the creation request, the Auto Scaling group
is ready for use in other calls.</p>

<p>Navigate to EC2-> Auto Scaling Groups-> Create Autoscaling Group.</p>

<p>Select the existing Launch Configuration (lamp-launch-1) and go to “Next
Step”.\</p>

<p><img src="https://lh4.googleusercontent.com/N9-2P0wrXtd1IKWO58H1i7s6MRx0w_toDG5wjmtvsw0v5wUamXz6oIC88g-YYqnP5H9OTobjDc_X9Hf1shLrn_5tTO4HLuHHUomu8qE8OYv3oLEDp02urv0U4g" alt="" /></p>

<p><strong>Configure Autoscaling Group Details:</strong></p>

<p>Give a name to the Auto Scaling Group.</p>

<p>Group Size: Start with zero instances in order to avoid the immediate
creation of instances. The exact number of required instances can be set
after these steps are completed, but just set cost optimization values
to 0 for now.</p>

<p><strong>Availability Zones:</strong></p>

<p>Choose 2 availability zones in order to maintain high availability.</p>

<p>In “Advanced Details” choose the values as shown below.\</p>

<p><img src="https://lh3.googleusercontent.com/YL14QA7srKzFlnQLXuXtIxo0wrB51aDQuOj8fR5YXDYErg2usKCEaKJ57fKN6UahjFP0KFOTkr_y8odlPVWqDh2z3OMwy_iUCRJ5ubiQ4EsOTTNSLKbz8IU2UQ" alt="" /></p>

<p><strong>Configure Scaling Policies:</strong> Keep minimum and maximum instances count
to zero.</p>

<p><strong>Increase Group Size:</strong> Keep the name at default.</p>

<p><strong>Execute Policy:</strong> Click on “Add new alarm”.\</p>

<p><img src="https://lh3.googleusercontent.com/ysaxf1OR2wazrUt1lvdkYI-YRoVdijKoJD4m6DDlGOHld-LO_mtz24-kOmvyTHybJPIEK_VOKV_UC5ybIwwt9jKtJhZ_3N3J1B_4e5P1iAyIsynEpODx6S4gKQ" alt="" /></p>

<p>A popup window will appear for creating a “Cloud Watch Alarm”.\</p>

<p><img src="https://lh3.googleusercontent.com/eqvhUKlpyNKv7vWQiAI95g5oUQxp9KpqQJ0WCEJjFNfCfn-PIBtaYEPf7k8T2BrlCxN8EoF6utdR5c67tMMk-K0CW-upVZIKR1jOTumOoiwic5fEvZndn8_aEA" alt="" /></p>

<p>For this example, we chose CPU Load average as the autoscaling trigger.
Whenever the average CPU load of app servers goes beyond 75% for 5
minutes, autoscaling will trigger the auto-scale-up-policy to launch the
1 instance and attach to load balancer. The application here is CPU
intensive and requires more computing power, so we chose the CPU Load
Average as the autoscale triggering event.</p>

<p>You can choose any Cloudwatch metric to trigger the autoscaling policy.
For example, Disk read/writes, Network In/Out, ELB request count, ELB
latency, etc.</p>

<p><strong>Decrease Group size:</strong>\</p>

<p><img src="https://lh3.googleusercontent.com/wxwMuiWm-72K4zGTx-oT56LdXvjhQXNQZPLg_HXPMb89ey5CEtmOV76euYKqLwbYKm88kOAlmSrW6xfRImjIefcQ7LVFOcUIRoyZ5t3xF_JffVy7R96Ypk8lMg" alt="" /></p>

<p><strong>Configure Notifications:</strong> If you’ve already created notifications,
select one to receive the notifications of Autoscaling events, else skip
this step for now.</p>

<p>Review the details and click on “Create Autoscaling Group”.</p>

<p>Auto Scaling evaluates the health of each Amazon EC2 instance in the
Auto Scaling Group and automatically replaces unhealthy instances in
order to keep the Auto Scaling Group size fixed. That ensures that the
application is getting the expected compute capacity.</p>

<p>In this example, we’ve chosen to scale down the environment when the
average CPU load lowers to 40% for 5 minutes.</p>

<h2><strong>Verify Autoscaling</strong> </h2>

<p>After completing the previous steps, it’s time to test autoscaling.</p>

<p>​1. Update the Autoscaling group minimum and maximum instances count to
2 and 4. You can change these later to fit your specific requirements.</p>

<p>Navigate to EC2-> Auto Scaling Group- > lamp-asg-1.</p>

<p>Right click on “lamp-asg-1” and select “Edit”.</p>

<p>In the “Details” tab, update Desired Min and Max values to 1, and then
save.\</p>

<p><img src="https://lh5.googleusercontent.com/hxxgnuVeJGgoMHPJvsr5oTeghIkuhJv4_hX0U1YtdjpO57N84IN4KmuSk4cO9cYeTH7JiSD9EMVAXCwS8z5v0-lZ3Gf4jW67oGNflBhgr10tiUVHbn9lpjE8oQ" alt="" /></p>

<p>Now, navigate to Services->EC2->Instances.</p>

<p>You can see that a new instance has been launched and attached to ELB.\</p>

<p><img src="https://lh3.googleusercontent.com/GPJHyEzUrO4LtbxCSzT6rHiz_RLPIIkmeIS--__XDuzFiW2dLSZwZhSwkvHG2KvW5CJ6Kx8DpvjEXo8Xgbw7Q8HBa6cfNUu2Mf6XAM6jpx-8LVKxEUMNeXCVHA" alt="" /></p>

<p>\</p>

<p><img src="https://lh3.googleusercontent.com/9_cNWARusMs6T9rypbPZWakid_pmJpMwBWLXuPhwaRlpiXLU5XFjQDGzBAYbPTtjSiupY8tELI5s8ROwgZtKCvfKhxVsagLXYoKCl_MnWqvnqrOGAoqj-wHGOg" alt="" /></p>

<p>If your health check is configured properly, the instance status will
turn into “In Service” rather quickly. Always have a minimum of 2
instances running in order to maintain high availability.</p>

<p>​2. How to Update Launch Configuration: It’s not possible to update the
existing Launch configuration, so you’ll have to create a new Launch
configuration and edit the Autoscaling Group to use that new
configuration.</p>

<p><img src="https://lh5.googleusercontent.com/_bDZv08DtcohHQkG5s0e2_0rd3VnkzImL-87T5wfku5cmvL69tgMhMwKc71Vo-zgVbVImT-HaD0f_fo7dcv5F0JmkbXZGpllnhldKILcPVdgYR7nKZ0qaRniwQ" alt="" /></p>

<p>Now, start the application load run and find out the minimum and maximum
instances required for your application to handle the load. Then update
the AutoScaling Group to meet your needs.</p>

<p>Watch out for  Part 3 of this series to understand <a href="../autoscaling-for-lamp-on-aws-load-test-the-environment-part-3/index.html" title="Autoscaling for LAMP on AWS | Load Test the Environment: Part 3">how to load run an
application</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Autoscaling for LAMP on AWS : Creating a LAMP Stack AMI : Part 1]]></title>
    <link href="http://bvajjala.github.io/blog/2014/03/25/autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-1/"/>
    <updated>2014-03-25T11:23:04-04:00</updated>
    <id>http://bvajjala.github.io/blog/2014/03/25/autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-1</id>
    <content type="html"><![CDATA[<h3>Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 1</h3>

<p>This is part 1 of the Autoscaling for LAMP in AWS series. The
step-by-step guide would walk you through</p>

<p><a href="index.html" title="Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 1">Part 1: Creating a LAMP Stack
AMI</a></p>

<p><a href="../autoscaling-for-lamp-on-aws-setting-up-austoscaling-groups-part-2/index.html" title="Autoscaling for LAMP on AWS | Setting Up Austoscaling Groups : Part 2">Part 2: Setting up Autoscaling
Groups. </a></p>

<p><a href="../autoscaling-for-lamp-on-aws-load-test-the-environment-part-3/index.html" title="Autoscaling for LAMP on AWS | Load Test the Environment: Part 3">Part 3: Load Test the
environment</a></p>

<p><a href="../autoscaling-for-lamp-on-aws-choosing-an-instance-type-part-4/index.html" title="Autoscaling for LAMP on AWS | Choosing an instance type: Part 4">Part 4: Choose instance
type</a></p>

<p>First, let’s discuss how to prepare the AMI, create an ELB and RDS,
verify the application and terminate the instance.</p>

<h2><strong>Prepare AMI</strong> </h2>

<p>In Autoscaling, create an AMI with all required packages installed. This
AMI will be used as a template to launch instances in autoscaling. For
LAMP Stack, you should install and fine tune the latest versions of
Apache and PHP.</p>

<p>​1. Log in to AWS console.</p>

<p>​2. Navigate to EC2 services. Make sure to switch to the desired region
before launching instances.</p>

<p>​3. Click on “Launch instance”.</p>

<p><img src="https://lh4.googleusercontent.com/hRoQeC1-aimLHiAt0ptacAxdMqlX8w0N19bEmMo8vm31e0n3-PRzWJLd6gg5TUn2FgokMj6n9y72S6gE_XKS0jNx9tHGv5kqD9TKz5qM0vYhu9TIF5FBAG6C-Q" alt="" /></p>

<p>​4. Select your favorite <strong>Linux AMI</strong> in Classic Wizard.</p>

<p><img src="https://lh6.googleusercontent.com/aWfL4g9PfAqaFTg3uI0Ntv52hSxUyPCNKN5Sf8Y4R5zYyjGOmHeA2zi6pGk59F9NiNJa16n27rSk111xlUJUt24oCLPdQfBZMr0k5yhCZpQ6B_ibxzU9hQRKeA" alt="" /></p>

<p>For this article we used CentOS 6.3 from the community AMIs, but
OpenSuse and Ubuntu are also good choices.</p>

<p>​5. Launch a <strong>micro</strong> instance from the selected AMI with the desired
<strong>Security group</strong> and <strong>Key pair</strong>. Remember, this instance is only for
creating an AMI and will be terminated once <strong>LAMP Stack</strong> is installed
on it.</p>

<p><img src="https://lh3.googleusercontent.com/_U5YNCpt3CelshRRvlBG8CLKp8tYd64u1lwFWqqZcN48v3UjL9GBOfHoxL3EXJ9pXOk3tNz0Ig049xbwJa4gIsNvr4Z869PFqwkSfBDkJ8GK1PU9olC7ktBn7w" alt="" /></p>

<p>​6. Now log in to the instance and perform the <strong>PHP</strong> and Apache
installation steps. For this article, we used <strong>Cygwin</strong> to connect the
<strong>Linux</strong> servers. Make sure you open port 22 to allow <strong>SSH</strong> access
using security groups.</p>

<p><img src="https://lh4.googleusercontent.com/Al62wHOtPAxqNRUh_kr47FmQqMSycjLKMJ9zSz47OfIWUVDENcSi6ypdoBaiABQ9Rlr33kid0Be8a-1KuyLDYhFbea4Fw4rdJKxGSoDfbQZP6iClUbDTnUmFGA" alt="" /></p>

<p>We modified the “default” security group to open the SSH port. We
recommend that you not completely open port 22 for SSH, but rather allow
incoming requests to SSH from your specific IP address. In our case the
IP address was 49.206.166.12.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ssh –I LAMP_AMI.pem root@ec2-54-254-37-17.ap-southeast-1.compute.amazonaws.commailto:root@ec2-54-254-37-17.ap-southeast-1.compute.amazonaws.com
</span><span class='line'>mailto:root@ec2-54-254-37-17.ap-southeast-1.compute.amazonaws.com
</span><span class='line'>$ yum install httpd.x86_64 httpd-devel.x86_64 mod_ssl.x86_64 php.x86_64 php-common.x86_64 php-devel.x86_64 php-gd.x86_64 php-mbstring.x86_64 php-mysql.x86_64 php-pdo.x86_64 php-soap.x86_64 php-xml.x86_64 php-xmlrpc.x86_64 php-pecl-apc-devel.x86_64 php-pecl-memcache.x86_64 -y</span></code></pre></td></tr></table></div></figure>


<p>Once you’ve successfully executed these commands, Apache and PHP will
install. Note that the commands we’ve used are specific to CentOS 6.3.
Choosing a different OS or a different CentOS may cause these commands
to fail to execute. T<strong>he php modules listed above are required for
WordPress setup and they may vary depending upon your requirements.</strong></p>

<p>Now update the PHP and Apache configurations for your Instance types and
for the expected load on application. We used t1.Micro for this article.
Keep in mind that a production system’s instance types need to be chosen
very carefully, and that the software must be tuned to those particular
types.</p>

<p><strong><em>Read more on selecting instance types in <a href="../autoscaling-for-lamp-on-aws-choosing-an-instance-type-part-4/index.html" title="Autoscaling for LAMP on AWS | Choosing an instance type: Part 4">Part 4 of this
series.</a></em></strong></p>

<p>Install Postfix to send email notification for the script used in the
next step.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ yum install postfix  
</span><span class='line'>$ chkconfig –level 2345 postfix on
</span><span class='line'>$ chkconfig –level 2345 httpd off</span></code></pre></td></tr></table></div></figure>


<p>​7. Now all of the required packages are installed and fine-tuned.</p>

<p>So, how does one deploy the code into AMI?</p>

<p>The method described here automates the deployment. One cron job will be
included in AMI, and it will download and deploy the latest available S3
code. That means that code updates will deploy to S3 every time, but not
to instances. Every time a new instance is up, the instance will
automatically download updates from the S3 bucket and deploy them.</p>

<p>What follows is a script scheduled for machine boot-up. It requires
updating based on specific user configurations and file paths.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/bash
</span><span class='line'>S3_URL= “http://s3-ap-southeast-1.amazonaws.com/lamp-deploy/wordpress.zip” #Change the following paths as per your requirement.
</span><span class='line'>DOCUMENT_ROOT=&lt;/var/www/html&gt;
</span><span class='line'>TEMP_LOC=&lt;/tmp&gt;
</span><span class='line'>APACHE_RESTART=`/etc/init.d/httpd restart`
</span><span class='line'>#Get the instance ID from AWS metadata.
</span><span class='line'>INSTANCE_ID=` curl http://169.254.169.254/latest/meta-data/instance-id`
</span><span class='line'>###download the latest code from s3 and deploy###
</span><span class='line'>cd /tmp
</span><span class='line'>wget $S3_URL
</span><span class='line'>if [ $? –ne 0 ]
</span><span class='line'>then
</span><span class='line'>pkill -9 httpd
</span><span class='line'>mail –s “Code Download Failed from S3##$INSTANCE_ID”  test@test.com #Provide valid email ID here”
</span><span class='line'>exit 1
</span><span class='line'>fi
</span><span class='line'>unzip wordpress.zip
</span><span class='line'>rm –rf  &lt;/var/www/html/*&gt;
</span><span class='line'>cp –r wordpress/* $DOCUMENT_ROOT
</span><span class='line'>chmod 775 –R $DOCUMENT_ROOT
</span><span class='line'>chown –R apache.root $DOCUMENT_ROOT
</span><span class='line'>$APACHE_RESTART
</span><span class='line'>if [ $? –ne 0 ]
</span><span class='line'>then
</span><span class='line'>pkill -9 httpd
</span><span class='line'>mail –s “Apache startup Failed- $INSTANCE_ID”  test@test.com #Provide valid email ID here”
</span><span class='line'>exit 1
</span><span class='line'>fi
</span><span class='line'>##script ends here####</span></code></pre></td></tr></table></div></figure>


<p>Put this bash script into a file in /opt/ directory:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$vim /opt/auto-deploy.sh
</span><span class='line'>$chmod +x /opt/auto-deploy.sh</span></code></pre></td></tr></table></div></figure>


<p>Open the Crontab and add this script to start at machine boot up:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ crontab –e</span></code></pre></td></tr></table></div></figure>


<p>Add the following script:</p>

<p><img src="https://lh4.googleusercontent.com/ZiqV0hI1ocrhmDqqLubt01OQdBcmJZqWhNciW6sbPfEz2Iwr99cxoNK4U5actZ6Vqdqat8asF-IwxjJQrVKiutlBZq6A74t9sp1wA0ezqCDbSR09YOjTQrJblX_8VShJfMA" alt="" /></p>

<p>Save your changes and deploy the code by running the script to test
whether or not the application is working as expected. This step is
mandatory before creating an AMI. If you find any issues with the code
or PHP/Apache settings, be sure to fix them before creating an AMI.</p>

<p>Bundle and Upload the Code to S3</p>

<ol>
<li><p>Update the code with RDS DB configuration parameters.</p></li>
<li><p>Now zip the working copy of the code and upload it to the S3
location. The S3 location should match the location in the script,
and also match the zip file name.</p></li>
<li><p>Upload the zip file to the S3 bucket that matches the script. You
can upload the zip file by using the console or any third party tool
like Cloudberry.</p></li>
<li><p>Grant read permissions to download the code from S3 to app servers.</p></li>
</ol>


<h2><strong>Create the ELB</strong> </h2>

<p>Go to Services-> EC2 &ndash;> Load Balancers and select “<strong>Create Load
Balancer</strong>”.</p>

<p><img src="https://lh5.googleusercontent.com/w-ZRahemXgWVTJcIwpwJYfdXgp7kcmUs23IYzjgdEpwUuYiCkoLJg2dnLs0-5fe8U0UFG1_3qyaNggPesGGHGUq1PwQH5MMo-QowBXfuhGSNthk65DDg5Oc1OA" alt="" /></p>

<p><img src="https://lh5.googleusercontent.com/JU0AmjRaIgFbWa8mdGz2qTlbYTU7wKWiOa93uUVuu-RDO5PPfEVhV1ggUBZ95rr5_z637SffLdwYgHb5srqZlj9GfcL-or3aG3xWsqb9Sd4kJ24LKZ8_84d_YA" alt="" /></p>

<p>Keep Healthy/Unhealthy threshold limits to 4 because that’s proven to be
an ideal value. Also make sure index.html is available in the Document
root. ELB looks for the index.html as a health check and removes the
instance from the load balancer if it doesn’t get the 200 response.
Create the required security groups and key pair for the autoscaling
instances.</p>

<h2><strong>Create RDS</strong> </h2>

<p>​1. Go to Services &ndash;> RDS and select the <strong>MySQL</strong> engine.</p>

<p><img src="https://lh5.googleusercontent.com/dgMcoX4gkePTWwEYT8soz9RdvW3iMPmmlVlnkgczHmwIDpfmh39BhRUmWRBEdbR0f6FMJi3LtaSW6z_sRhBmk_WYLfls0bv8VQ8r9PFPl52WOX0B12-vCLKBEQ" alt="" /></p>

<p>​2. Select instance types, provide the details for all required fields
and launch the instance. The RDS configuration details chosen for this
article can be seen below.</p>

<p>How should you select the DB instance type? There are many monitoring
tools available for monitoring MySQL performance. If AWS RDS is used for
MySQL, then it’s important to consider CloudWatch metrics. Since it’s
not possible to install agents in RDS, you can rely upon CloudWatch
metrics.</p>

<p>By default, CloudWatch contains all required RDS monitoring metrics. Be
sure to monitor the memory, CPU, DB connections and network I/O before
you scale up the server.</p>

<p>Ideally, m1.small RDS can handle up to 75 concurrent connections easily.
but that depends upon the DB queries and the code.</p>

<p>​3. After launching RDS, open the 3306 port to the app servers group.</p>

<p>​4. Import the DB and try connecting to the application. Then use
standard MySQL commands to import the dump.</p>

<p><strong>Example:</strong> mysql –h rds-endpoint –u user –ppasswwd databasename &lt;
dump.sql</p>

<h2><strong>Verify the Application</strong> </h2>

<p>At this point, it’s mandatory that you thoroughly verify the
application. Make sure to check Login/logout, upload/download and any
other relevant verifications. Create an AMI only after making sure that
the application is working as expected. Fix any issues before proceeding
to the next step.</p>

<h3><strong>Create an AMI</strong></h3>

<p>​1. Delete the command history:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ history -c</span></code></pre></td></tr></table></div></figure>


<p>Delete the deployed code and logs files and make things as clean as
possible.</p>

<p>​2. Create an AMI using the micro instance that you’ve configured.</p>

<p>Go to AWS Console->Services->EC2, right click on the instance, and
select “Create Image” (EBS AMI).</p>

<p><img src="https://lh4.googleusercontent.com/pR5iFmQYgO6FygwUB1SNh52dtHhrTr4TDk_G9fd6wlh1ds3V5gnbvhqYdn4yPOPRuQ7JZXwO54xuguOEoczq9yhVx1gsuHuZbVmCRgVFbida6ciJxv0EhNhwdQ" alt="" /></p>

<p>Name the AMI using a date as the naming convention. <strong>Do not</strong> select
“<strong>No reboot</strong>”. Instead, allow it to reboot while creating the AMI. The
reboot will give you a consistent snapshot of EBS.</p>

<p><img src="https://lh5.googleusercontent.com/5vWSfAF_enz-JCR8PlW-hDts2pvtUSI8yRFYkx8H27K0UdRCntJjTgcHGODGJpiGkXfFpe-XGva-EZUFF5gEDRcPcw5o8xc_8n1ueqYQaK8hpY9WolFLUdrHRw" alt="" /></p>

<p>In a few minutes an AMI will be created and made available.</p>

<p>Keep the root partition as minimal as possible because that helps to
prevent bugs. You don’t need large space for root partition in autoscale
because these instances keep rotating.</p>

<h2><strong>Terminate the Instance</strong> </h2>

<p>Finally, terminate the instance by right clicking on the instance and
selecting “Terminate”. Keep the ELB and RDS running.</p>

<p><strong><em>Watch out for Part 2, same time tomorrow, on how to setup autoscaling
groups!</em></strong></p>

<p><strong>Update</strong>: Check part 2 on how to setup autoscaling groups
<a href="../autoscaling-for-lamp-on-aws-setting-up-austoscaling-groups-part-2/index.html" title="Autoscaling for LAMP on AWS | Setting Up Austoscaling Groups : Part 2">here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Deep Dive into AWS Reserved Instances]]></title>
    <link href="http://bvajjala.github.io/blog/2014/03/25/a-deep-dive-into-aws-reserved-instances/"/>
    <updated>2014-03-25T09:35:27-04:00</updated>
    <id>http://bvajjala.github.io/blog/2014/03/25/a-deep-dive-into-aws-reserved-instances</id>
    <content type="html"><![CDATA[<h3>A Deep Dive into AWS Reserved Instances</h3>

<p>One of our primary goals at Flux7 Labs is to help our clients reduce
their AWS costs. In fact, our product VyScale is based entirely on cost
optimization using Spot instances. We inform our clients when it makes
economic sense for them to buy instance reservations because
reservations for periods of unexpected minimum usage can be beneficial.
Reserved instances function exactly like on-demand instances, except
that you pay an upfront fee to gain cheaper hourly rates.</p>

<h3>Reserved Instance Pricing</h3>

<p>There are several levels of reservations. Higher ones allow you to pay
more up front in order to achieve a lower hourly cost. The table below
shows the rates for various types of reserved and on-demand instances
for m1.large instances.</p>

<hr />

<pre><code>           Upfront   Hourly in cents
</code></pre>

<p>  On-demand    0         24
  Light Util   243       13.6
  Med Util     554       8.4
  Heavy Util   676       5.6</p>

<hr />

<p>At some levels of utilization it makes sense to purchase reservations,
rather than to rely solely upon on-demand instances. By factoring in
upfront costs, we can determine which levels warrant purchasing
reservations. What’s surprising is that those levels are fairly low,
especially in the case of light reservations. Even at 30% utilization,
light-reservation costs start to break even with those of on-demand
instances. The following graph shows equivalent hourly costs for
reservations at various utilization levels</p>

<p><a href="../../../wp-content/uploads/AWS-reserved-instances-1.png"><img src="../../../wp-content/uploads/AWS-reserved-instances-1.png" alt="Hourl rates of reserved
instances" /></a></p>

<p>To better understand these numbers in terms of total cost, as opposed to
incremental cost, the figure below shows total annual expenditures at
various levels. For a 100% utilization of an instance, one can reduce
costs to almost half of Amazon’s standard pricing for reserved
instances, and that’s without any of the bulk discounts made available
to high-volume AWS customers. At lower levels, reservations cost almost
twice as much as those at higher levels. However, keep in mind that, as
a fraction of overall cost, the amount is still not very high. So, in
cases where growth is expected, it can make sense to purchase
reservations early.</p>

<p><a href="../../../wp-content/uploads/AWS-reserved-instances-2.png"><img src="../../../wp-content/uploads/AWS-reserved-instances-2.png" alt="Annual rates of reserved
instances" /></a></p>

<h3>Do You Have A Reservation?</h3>

<p>Another thing to know about Amazon’s policy is that you receive a
guaranteed instance when you buy a reservation, whereas there is no such
guarantee for on-demand instances. When trying to acquire an on-demand
instance, AWS may return an error message stating that capacity is not
available for that instance type. On the other hand, once you purchase a
reservation Amazon guarantees that the instance will be made available
to you at the moment you request it. If you’re using an autoscaling
solution it can make sense to make light utilization reservations to
handle your excess capacity, even if it costs more to guarantee uptime
to your customers. With Amazon’s guarantee it’s no wonder that Netflix
runs almost exclusively on reservations. As a matter of policy,
Netflix’s use of on-demand instances indicates that more reservations
need to be purchased.</p>

<p>One thing to note is that Amazon always uses reservations first, so you
can’t keep unused light reservations as backups while using on-demand
instances for capacity.</p>

<h3>How Amazon Handles Unused Reservations</h3>

<p>Spot instances are one of the best uses of unutilized reserved capacity
with AWS. They come with no guarantee of availability, and can be taken
away from you at Amazon’s discretion at any time in order to fulfill
other customers’ needs. Spot prices hover around 15% of the on-demand
price, which allows Amazon to make a decent return on unused
reservations and while offering reservations at a relatively low price.</p>

<h3>Conclusion</h3>

<p>The most common reason to use reservations is price, but there are also
other considerations. The final decision should be based on capacity
analysis from a business perspective.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[6 reasons why large enterprises should move to Amazon Web Services]]></title>
    <link href="http://bvajjala.github.io/blog/2014/03/25/6-reasons-why-large-enterprises-should-move-to-amazon-web-services/"/>
    <updated>2014-03-25T09:13:48-04:00</updated>
    <id>http://bvajjala.github.io/blog/2014/03/25/6-reasons-why-large-enterprises-should-move-to-amazon-web-services</id>
    <content type="html"><![CDATA[<h2>6 reasons why large enterprises should move to Amazon Web Services</h2>

<h3>6 reasons why large enterprises should move to Amazon Web Services</h3>

<p>Amazon has changed the face of the world of startups with its cloud services. Now it’s possible for two men in a garage to set up large
computer clusters for zero capital cost.</p>

<p><a href="https://twitter.com/share?text=Amazon+has+its+sights+set+on+enterprise,+which+they+will+conquer+slowly+but+surely.+&amp;via=OurLabs&amp;related=Flux7Labs&amp;url=http://wp.me/p4sEOD-o6">Amazon has its sights set on enterprise, which they will conquer slowly
but
surely.</a></p>

<p>At Our Labs, one of our specialties is cloud migration for enterprise
clients, and we’ve received considerable feedback about both their
concerns and delights. In this post I’ll explain why I strongly believe
large enterprises should consider moving to the cloud.</p>

<h2><strong>Agility and Responsiveness</strong> </h2>

<p>Let’s face it, if a startup is a speedboat, then a large enterprise is
the Titanic. While startups can pivot any time there’s danger and large
enterprises turn more slowly, there are icebergs that can sink either.
Disasters happen to everyone, but a large enterprise has significantly
more skin in the game, and the old model of waiting a month before
buying a new rack just doesn’t cut it anymore. By shifting to the cloud,
large enterprises can bring up new servers in minutes, which means
shorter downtimes, rapid experimentation, more innovation, increased
global reach, improved demand-surge handling and more successful
short-lived-but-resource-intensive projects.</p>

<h2><strong>Innovation</strong> </h2>

<p>Business is a cutthroat world in which we must always strive to
out-innovate our competitors. This article is about the cloud in
general, but I want to explain why it’s also Amazon focused. Amazon has
been blazing the cloud-computing trail longer than anyone, which is
reflected by its 80% market share, and is clearly ahead of its
competitors. For example, check out Gartner’s analysis of different
cloud providers in the chart below and you’ll see Amazon in a corner of
their own. In fact, Amazon so dominated the competition that Gartner had
to artificially lower the scale. Amazon is innovating at such a rapid
pace in providing new services and broadening its customer base that
competitors are struggling to keep pace.</p>

<p>The effect of this innovation is obvious to developers. Do you want to
provide Active Directory integration with Amazon? You can do that. Do
you want to save on data costs by using the Bittorrent protocol for
content distribution? You can do that. Do you want to put heavy compute
behind your mobile games? You can do that. Do you want to ramp up your
clients quickly with remote workstations? You can do that, too. With
Amazon Web Services you can do all of that and more. I’m a
certified<a href="file:///C:/Users/Vishnu/Documents/Our/SMM/Blog/WhyEnterprisemustAmazon_(REVISED_CLEAN">[1]</a>.docx#_msocom_1) 
AWS instructor, yet I learn something new about AWS every day as Amazon
constantly releases new features. The advantage of moving your business
to the cloud is that you can let your developers focus on your company’s
area of expertise while leaving the boilerplate to Amazon.</p>

<p>\</p>

<p> <img src="https://lh3.googleusercontent.com/GmdBmLBmcGW9TIBqUCVkf403uIIf9arZ6-brIjBOt8tus6n_7YaKdOW3kcQqSIUbKnQd9vthqX1nyHHGouT8xbmDF3-xkpXUbJV0UJLpguSzg7EKB6QuaqR92uw_og" alt="" /></p>

<h2><strong>A Data Center That Hosts A Top Website</strong> </h2>

<p>One thing we mustn’t forget is what the AWS cloud really is—a product of
years of innovation by one of the largest e-commerce websites in the
world. Its data centers are distributed globally in numerous parts of
the world, with multiple, independently-operating data centers in each
region that provide different failure domains while being close enough
together to provide cheap communication. And that’s just the compute
side of the equation. Amazon also shares its global content distribution
network, its DNS servers. it compiles traffic data from across the globe
to find the best route for your network traffic. There are only a
handful of companies today with that kind of global reach, and they,
too, are developing cloud capabilities. But unless you’re in that elite
club, you won’t be able to create a data center that can compete with
AWS. So by using a cloud solution, rather than an in-house solution,
you’ll clearly benefit in terms of better performance, better fault
tolerance, and better disaster recovery.</p>

<h2><strong>Security</strong> </h2>

<p>Security is the biggest concern about cloud computing for most
enterprises, but Amazon holds many of the most important certifications,
including PCI, HIPAA, Sarbanes-Oxley, and ISO. Since it has so much at
stake, it maintains separation of logical and physical access to data in
order to limit the impact of disgruntled employees. While I certainly
understand someone hesitating to entrust one’s most valuable information
with a third party, it’s certainly debatable whether on-premise storage
is more secure than cloud storage.</p>

<h2><strong>In-house Expertise Not Required</strong> </h2>

<p>Anyone that’s had to hire people knows that good employees are worth
their weight in platinum. Moving to the cloud allows you to offload much
of your data-center maintenance onto Amazon. Let it do what it’s best at
while you focus on what you’re best at.</p>

<p>As we know, you often have to pay dearly to hire someone outside of your
area of expertise. For example, say you’re a director of IT at a company
like Schlumberger. Your company is great at what it does and has strong
brand value in its area of expertise. Do you think you can possibly
poach someone like <a href="http://mvdirona.com/jrh/work/">James Hamilton</a> for
your company? No, you can’t. With core expertise in data centers, Amazon
will likely offer a more intriguing challenge and a deeper sense of
mission to the kind of people you’ll want to hire for your team. And we
all know that employee engagement is not about the money, but rather
about being involved in a greater mission.</p>

<h2><strong>Lower Costs</strong> </h2>

<p>While I’m convinced that moving to the cloud will lower your costs, I
acknowledge that getting there requires a lot of expertise and hard
work. Additionally you may very easily find yourself comparing apples
and oranges. Yet cloud solutions are cheaper than on-premise solutions
when played right. First, when you pay for a machine on AWS you’re also
paying for Amazon’s years of expertise in setting up resilient data
centers, something that’s not true for in-house departments lacking core
IT expertise. Second, there’s great potential for saving money by
scaling to your variable demand needs instead of designing for max
capacity. That’s why Netflix, even though it comprises one-third of the
Internet in terms of data volume, has gone “all in” on AWS, or they’d
have to provision enough machines to handle 9PM Friday traffic.
Alternatively you can have an expected area of needing high demand, as
say for a chip company close to tapeout, a movie studio needing to
render for a year
(<a href="http://gigaom.com/2014/03/02/the-oscars-how-american-hustles-fx-team-made-2013-boston-look-like-1980s-new-york/">http://gigaom.com/2014/03/02/the-oscars-how-american-hustles-fx-team-made-2013-boston-look-like-1980s-new-york/</a>),
or a game company handling launch day demand
(<a href="http://www.respawn.com/news/lets-talk-about-the-xbox-live-cloud/">http://www.respawn.com/news/lets-talk-about-the-xbox-live-cloud/</a>).</p>

<h2><strong>Conclusion</strong> </h2>

<p>If you’re a large enterprise and on the fence about whether or not to
move to the cloud, we highly recommend trying it out. Try it on a
contained project, rather than one that’s on a critical path, something
that fits well within the cloud’s capabilities. After implementation, do
a post-mortem and analyze the results in terms of cost and
time-to-market. What is the expectation as you build in-house
expertise<strong>.</strong> The results may well surprise you.</p>

<h2><strong>Let’s Talk</strong> </h2>

<p>Our Research Labs has helped Fortune 100 and Fortune 500 companies move
successfully to AWS. We’d love to hear any questions, comments or
concerns you may have, so feel free to contact us anytime during weekly
<a href="http://ohours.org/aatersuleman">office hours</a> to discuss your specific
needs or situation. We offer this service free of charge with no
obligation or strings attached.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Creating a Secure Deployment Pipeline in Amazon Web Services]]></title>
    <link href="http://bvajjala.github.io/blog/2014/03/23/creating-a-secure-deployment-pipeline-in-amazon-web-services/"/>
    <updated>2014-03-23T11:24:27-04:00</updated>
    <id>http://bvajjala.github.io/blog/2014/03/23/creating-a-secure-deployment-pipeline-in-amazon-web-services</id>
    <content type="html"><![CDATA[<p>Many organizations require a secure infrastructure. I’ve yet to meet a customer that says that security isn’t a concern. But, the decision on “how secure?” should be closely associated with a risk analysis for your organization.</p>

<p>Since Amazon Web Services (AWS) is often referred to as a “public cloud”, people sometimes infer that “public” must mean it’s “out in the public” for all to see. I’ve always seen “public/private clouds” as an
unfortunate use of terms. In this context, public means more like
“Public Utility”. People often interpret “private clouds” to be
inherently more secure. Assuming that “public cloud” = less secure and
“private cloud” = more secure couldn’t be further from the truth. Like
most things, it’s all about how you architect your infrastructure. While
you can define your infrastructure to have open access, AWS provides
many tools to create a truly secure infrastructure while eliminating
access to all but only authorized users.</p>

<p>I’ve created an initial list of many of the practices we use. We don’t
employ all these practices in all situations, as it often depends on our
customers’ particular security requirements. But, if someone asked me
“How do I create a secure AWS infrastructure using a Deployment
Pipeline?”, I’d offer some of these practices in the solution. I’ll be
expanding these over the next few weeks, but I want to start with some
of our practices.</p>

<p><strong>AWS Security</strong></p>

<p>* After initial AWS account creation and login, configure
<a href="https://aws.amazon.com/iam/" title="AWS IAM">IAM</a> so that there’s no need to
use the AWS root account\
 * Apply least privilege to all IAM accounts. Be very careful about who
gets Administrator access.\
 * Enable all IAM password rules\
 * Enable MFA for all users\
 * Secure all data at rest\
 * Secure all data in transit\
 * Put all AWS resources in a <a href="https://aws.amazon.com/vpc/" title="Virtual Private Cloud">Virtual Private
Cloud</a> (VPC).\
 * No EC2 Key Pairs should be shared with others. Same goes for Access
Keys.\
 * Only open required ports to the Internet. For example, with the
exception of, say, port 80, no security groups should have a CIDR Source
of 0.0.0.0/0). The bastion host might have access to port 22 (SSH), but
you should enable
<a href="https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing" title="CIDR">CIDR</a>
to limit access to specific subnets. Using a VPC is a part of a solution
to eliminate Internet access. No canonical environments should have
SSH/RDP access.\
 * Use IAM to limit access to specific AWS resources and/or
remove/limit AWS console access\
 * Apply a bastion host configuration to reduce your attack profile\
 * Use <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html" title="IAM Roles">IAM
Roles</a>
so that there’s no need to configure Access Keys on the instances\
 * Use resource-level permissions in EC2 and RDS\
 * Use SSE to secure objects in S3 buckets\
 * Share initial IAM credentials with others through a secure mechanism
(e.g. <a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard" title="AES">AES-256
encryption</a>)\
 * Use and monitor AWS
<a href="https://aws.amazon.com/cloudtrail/" title="AWS CloudTrail">CloudTrail</a> logs</p>

<p><strong>Deployment Pipeline</strong></p>

<p>A deployment pipeline is a staged process in which the complete software
system is built and tested with every change. Team members receive
feedback as it completes each stage. With most customers, we usually
construct between 4-7 deployment pipeline stages and the pipeline only
goes to the next stage if the previous stages were successful. If a
stage fails, the whole pipeline instance fails. The first stage (often
referred to as the “Commit Stage”) will usually take no more than 10
minutes to complete. Other stages may take longer than this. Most stages
require no human intervention as the software system goes through more
extensive testing on its way to production. With a deployment pipeline,
software systems can be released at any time the business chooses to do
so. Here are some of the security-based practices we employ in
constructing a deployment pipeline.</p>

<p>* Automate everything: Networking (VPC, Route 53) Compute (EC2),
Storage, etc. All <em>AWS</em> automation should be defined in
<a href="https://aws.amazon.com/cloudformation/" title="CloudFormation">CloudFormation</a>.
All environment configuration should be defined using infrastructure
automation scripts – such as Chef, Puppet, etc.\
 * Version Everything: Application Code, Configuration, Infrastructure
and Data\
 * Manage your binary dependencies. Be specific about binary version
numbers. Ensure you have control over these binaries.\
 * Lockdown pipeline environments. Do not allow SSH/RDP access to any
environment in the deployment pipeline\
 * For project that require it, use permissions on the CI server or
Deployment application to limit who can run deployments in certain
environments – such as QA, Pre-Production and Production. When you have
a policy in which all changes are applied through automation and
environments are locked down, this usually becomes less of a concern.
But, it can still be a requirements on some teams.\
 * Use the Disposable Environments pattern – instances are terminated
once every few days. This approach reduces the attack profile\
 * Log everything outside of the EC2 instances (so that they can be
access later). Ensure these log files are encrypted e.g. securely
through S3)\
 * All canonical changes are only applied through automation that are
part of the deployment pipeline. This includes application,
configuration, infrastructure and data change. Infrastructure patch
management would be a part of the pipeline just like any outer software
system change.\
 * No one has access to nor can make direct changes to pipeline
environments\
 * Create high-availability systems Multi-AZ, <a href="https://aws.amazon.com/autoscaling/" title="Auto Scaling">Auto
Scaling</a>, <a href="https://aws.amazon.com/elasticloadbalancing/" title="ELB">Elastic
Load Balancing</a> and
Route 53\
 * For non-Admin AWS users, only provide access to AWS through a secure
<a href="https://en.wikipedia.org/wiki/Continuous_integration" title="Continuous Integration">Continuous
Integration</a>
(CI) server or a self-service application\
 * Use Self-Service Deployments and give developers full SSH/RDP access
to their self-service deployment. Only their particular EC2 Key Pair can
access the instance(s) associated with the deployment. Self-Service
Deployments can be defined in the CI server or a lightweight
self-service application.\
 * Provide capability for any authorized user to perform a self-service
deployment with full SSH/RDP access to the environment they created
(while eliminating outside access)\
 * Run two active environments – We’ve yet to do this for customers,
but if you want to eliminate all access to the canonical production
environment, you might choose to run two active environments at once so
that engineers can access the non-production environment to troubleshoot
a problem in which the environment has the exact same configuration and
data so you’re troubleshooting accurately.\
 * Run automated infrastructure tests to test for security
vulnerabilities (e.g. cross-site scripting, SQL injections, etc.) with
every change committed to the version-control repository as part of the
deployment pipeline.</p>

<p><strong>FAQ</strong></p>

<p>* <strong>What is a canonical environment?</strong> It’s your system of record. You
want your canonical environment to be solely defined in source code and
versioned. If someone makes a change to the canonical system and it
affects everyone it should only be done through automation. While you
can use a self-service deployment to get a copy of the canonical system,
any direct change you make to the environment is isolated and never made
part of the canonical system unless code is committed to the
version-control repository.\
 * <strong>How can I troubleshoot if I cannot directly access canonical
environments?</strong> Using a self-service deployment, you can usually
determine the cause of the problem. If it’s a data-specific problem, you
might import a copy of the production database. If this isn’t possible
for time or security reasons, you might run multiple versions of the
application at once.\
 * <strong>Why should we dispose of environments regularly?</strong> Two primary
reasons. The first is to reduce your attack profile (i.e. if
environments always go up and down, it’s more difficult to hone in on
specific resources. The second reason is that it ensures that all team
members are used to applying all canonical changes through automation
and not relying on environments to always be up and running somewhere.\
 * <strong>Why should we lockdown environments?</strong> To prevent people from
making disruptive environment changes that don’t go through the
version-control repository.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CI and Test Automation utilizing OpenStack]]></title>
    <link href="http://bvajjala.github.io/blog/2014/03/14/ci-and-test-automation-utilizing-openstack/"/>
    <updated>2014-03-14T09:54:08-04:00</updated>
    <id>http://bvajjala.github.io/blog/2014/03/14/ci-and-test-automation-utilizing-openstack</id>
    <content type="html"><![CDATA[
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Creating a Secure Deployment Pipeline in AWS]]></title>
    <link href="http://bvajjala.github.io/blog/2014/03/11/creating-a-secure-deployment-pipeline-in-aws/"/>
    <updated>2014-03-11T10:11:47-04:00</updated>
    <id>http://bvajjala.github.io/blog/2014/03/11/creating-a-secure-deployment-pipeline-in-aws</id>
    <content type="html"><![CDATA[<p><img src="http://bvajjala.github.io/downloads/code/opsworks_automation_stack.jpg" alt="" />
<a href="https://aws.amazon.com/opsworks/">Amazon Web Services (AWS) OpsWorks</a> was released one year ago this month. In the past year, we’ve used OpsWorks on several Cloud Delivery projects at <a href="http://stelligent.com/">Stelligent</a> and at some of our customers. This article describes what’s worked for us and our customers. One of our core aims with any customer is to create a fully repeatable process for delivering software. To us, this translates into several more specific objectives. For each process we automate, the process must be fully documented, tested, scripted, versioned and continuous. This article describes how we achieved each of these five objectives in delivering OpsWorks solutions to our customers. In creating any solution, we version any and every asset required to create the software system. With the exception of certain binary packages, the entire software system gets described in code. This includes the application code, configuration, infrastructure and data.</p>

<p>As a note, we’ve developed other AWS solutions without OpsWorks using CloudFormation, Chef, Puppet and some of the other tools mentioned here, but the purpose of this is to describe our approach when using OpsWorks.</p>

<h2>AWS Tools</h2>

<p>AWS has over 30 services and we use a majority of these services when creating deployment pipelines for continuous delivery and automating infrastructure. However, we typically use only a few services directly when building these infrastructure. For instance, when creating infrastructure with OpsWorks, we’ll use the AWS Ruby SDK to provision the OpsWorks resources and CloudFormation for the resources we cannot provision through OpsWorks. We use these three services to access services such as EC2, Route 53, VPC, S3, Elastic Load Balancing, Auto Scaling, etc. These three services are described below.</p>

<h2>AWS OpsWorks</h2>

<p>– OpsWorks is an infrastructure orchestration and event modeling service for provisioning infrastructure resources. It also enables you to call out to Chef cookbooks (more on Chef later). The OpsWorks model logically defines infrastructure in terms of stacks, layers and apps. Within stacks, you can define layers; within layers you can define applications and within applications, you can run deployments. An event model automatically triggers events against these stacks (e.g. Setup, Configure, Deploy, Undeploy, Shutdown). As mentioned, we use the AWS API (through the Ruby SDK) to script the provisioning of all OpsWorks behavior. We never manually make changes to OpsWorks through the console (we make these changes to the versioned AWS API scripts).</p>

<h2>CloudFormation</h2>

<p>– We use CloudFormation to automatically provision resources that we cannot provision directly through OpsWorks. For example, while OpsWorks connects with Virtual Private Clouds (VPC)s and Elastic Load Balancer (ELB)s, you cannot provision VPC or ELB directly through OpsWorks. Since we choose to script all infrastructure provisioning and workflow, we wrote CloudFormation templates for defining VPCs, ELBs, Relational Database Service (RDS) and Elasticache. We orchestrate the workflow in Jenkins so that these resources are automatically provisioned prior to provisioning the OpsWorks stacks. This way, the OpsWorks stacks can consume these resources that were provisioned in the CloudFormation templates. As with any other program, these templates are version-controlled.</p>

<h2>AWS API</h2>

<p>(using Ruby SDK) – We use the AWS Ruby SDK to script the provisioning of OpsWorks stacks. While we avoid using the SDK directly for most other AWS services (because we can use CloudFormation), we chose to use the SDK for scripting OpsWorks because CloudFormation does not currently support OpsWorks. Everything that you might do using the OpsWorks dashboard – creating stacks, JSON configuration, calling out to Chef, deployments – are all written in Ruby programs that utilize the OpsWorks portion of the AWS API.</p>

<h2>Infrastructure Automation</h2>

<p>There are other non-AWS specific tools that we use in automating infrastructure. One of them is the infrastructure automation tool, Chef. Chef Solo is called from OpsWorks. We use infrastructure automation tools to script and as a way to document the process of provisioning infrastructure.</p>

<h2>Chef</h2>

<p>– OpsWorks is designed to run Chef cookbooks (i.e. scripts/programs). Ultimately, Chef is where a bulk of the behavior for provisioning environments is defined – particularly once the EC2 instance is up and running. In Chef, we write recipes (logically stored in cookbooks) to install and configure web servers such as Apache and Nginx or application servers such as Rails and Tomcat. All of these Chef recipes are version-controlled and called from OpsWorks or CloudFormation.</p>

<h2>Ubuntu</h2>

<p> – When using OpsWorks and there’s no specific operating system flavor requirement from our customer, we choose to use Ubuntu 12.04 LTS. We do this for two reasons. The first is that at the time of this writing, OpsWorks supports two Linux flavors: Amazon Linux and Ubuntu 12.04 LTS. The reason we choose Ubuntu is because it allows us to use Vagrant (more on Vagrant later). Vagrant provides us a way to test our Chef infrastructure automation scripts locally – increasing our infrastructure development speed.</p>

<h2>Supporting Tools</h2>

<p>Other supporting tools such as Jenkins, Vagrant and Cucumber help with Continuous Integration, local infrastructure development and testing. Each are described below.</p>

<h2>Jenkins</h2>

<p> – Jenkins is a Continuous Integration server, but we also use it to orchestrate the coarse-grained workflow for the Cloud Delivery system and infrastructure for our customers. We use Jenkins fairly regularly in creating Cloud Delivery solutions for our customers. We configure Jenkins to run Cucumber features, build scripts, automated tests, static analysis, AWS Ruby SDK programs, CloudFormation templates and many more activities. Since Jenkins is an infrastructure component as well, we’ve automated the creation in OpsWorks and Chef and it also runs Cucumber features that we’ve written. These scripts and configuration are stored in Git as well and we can simply type a single command to get the Jenkins environment up and running. Any canonical changes to the Jenkins server are made by modifying the programs or configuration stored in Git.</p>

<h2>Vagrant##</h2>

<p>– Vagrant runs a virtualized environment on your desktop and comes with support for certain OS flavors and environments. As mentioned, we use Vagrant to run and test our infrastructure automation scripts locally to increase the speed of development. In many cases, what might take 30-40 minutes to run the same Chef cookbooks can take 4-5 minutes to run locally in Vagrant – significantly increase our infrastructure development productivity.</p>

<h2>Cucumber</h2>

<p> – We use Cucumber to write infrastructure specifications in code called features. This provides executable documented specifications that get run with each Jenkins build. Before we write any Chef, OpsWorks or CloudFormation code, we write Cucumber features. When completed, these features are run automatically after the Chef, OpsWorks and/or CloudFormation scripts provision the infrastructure to ensure the infrastructure is meeting the specifications described in the features. At first, these features are written without step definitions (i.e. they don’t actually verify behavior against the infrastructure), but then we iterate through a process of writing programs to automate the infrastructure provisioning while adding step definitions and refining the Cucumber features. Once all of this is hooked up to the Jenkins Continuous Integration server, it provisions the infrastructure and then runs the infrastructure tests/features written in Cucumber. Just like writing XUnit tests for the application code, this approach ensures our infrastructure behaves as designed and provides a set of regression tests that are run with every change to any part of the software system. So, Cucumber helps us document the feature as well as automate infrastructure tests. We also write usage and architecture documentation in READMEs, wikis, etc.</p>

<p>A list of the AWS tools we utilized are enumerated below.</p>

<p>We applied the on-demand usability of the cloud with a proven continuous delivery approach to build an automated one click method for building and deploying software into scripted production environments.</p>

<table>
<thead>
<tr>
<th></th>
<th> Tool </th>
<th> What is it? </th>
<th> Our Use</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>  AWS EC2      </td>
<td> Cloud-based virtual hardware instances       </td>
<td> We use EC2 for all of our virtual hardware needs. All instances, from development to production are run on EC2</td>
</tr>
<tr>
<td></td>
<td> AWS S3 </td>
<td> Cloud-based storage </td>
<td> We use S3 as both a binary repository and a place to store successful build artifacts.</td>
</tr>
<tr>
<td></td>
<td> AWS IAM </td>
<td> User-based access to AWS resources </td>
<td> We create users dynamically and use their AWS access and secret access keys so we don’t have to store credentials as properties.</td>
</tr>
<tr>
<td></td>
<td> AWS CloudWatch </td>
<td>System monitoring</td>
<td> Monitors all instances in production. If an instance takes an abnormal amount of strain or shuts down unexpectedly, SNS sends an email to designated parties.</td>
</tr>
<tr>
<td></td>
<td> AWS SNS </td>
<td> Email notifications </td>
<td> When an environment is created or a deployment is run, SNS is used to send notifications to affected parties.</td>
</tr>
<tr>
<td></td>
<td>Cucumber </td>
<td> Acceptance testing </td>
<td> Cucumber is used for testing at almost every step of the way. We use Cucumber to test infrastructure, deployments and application code to ensure correct functionality. Cucumber’s unique english-ess  verbiage allows both technical personnel and customers to communicate using an executable test.</td>
</tr>
<tr>
<td></td>
<td> Liquibase </td>
<td> Automated database change management </td>
<td> Liquibase is used for all database changesets. When a change is necessary within the database, it is made to a liquibase changelog.xml.</td>
</tr>
<tr>
<td></td>
<td>AWS CloudFormation </td>
<td> Templating language for orchestrating all AWS resources </td>
<td> CloudFormation is used for creating a fully working Jenkins environment and Target environment. For instance for the Jenkins environment it creates the EC2 instance with CloudWatch monitoring alarms, associated IAM user, SNS notification topic, everything required for Jenkins to build. This along with Jenkins are the major pieces of the infrastructure.</td>
</tr>
<tr>
<td></td>
<td>AWS SimpleDB </td>
<td> Cloud-based NoSQL database </td>
<td> SimpleDB is used for storing dynamic property configuration and passing properties through the CD Pipeline. As part of the environment creation process, we store multiple values such as IP addresses that we need when deploying the application to the created environment.</td>
</tr>
<tr>
<td></td>
<td>Jenkins </td>
<td> We’re using Jenkins to implement a CD pipeline using the Build Pipeline plugin.</td>
<td> Jenkins runs the CD pipeline which does the building, testing, environment creation and deploying. Since the CD pipeline is also code (i.e. configuration code), we version our Jenkins configuration.</td>
</tr>
<tr>
<td></td>
<td> Capistrano </td>
<td> Deployment automation </td>
<td> Capistrano orchestrates and automates deployments. Capistrano is a Ruby-based deployment DSL that can be used to deploy to multiple platforms including Java, Ruby and PHP. It is called as part of the CD pipeline and deploys to the target environment.</td>
</tr>
<tr>
<td></td>
<td>Puppet </td>
<td> Infrastructure automation </td>
<td> Puppet takes care of the environment provisioning. CloudFormation requests the environment and then calls Puppet to do the dynamic configuration. We configured Puppet to install, configure, and manage the packages, files and services.</td>
</tr>
<tr>
<td></td>
<td> Git </td>
<td> Version control system </td>
<td> Git is the version control repository where every piece of the Manatee infrastructure is stored. This includes the environment scripts such as the Puppet modules, the CloudFormation templates, Capistrano deployment scripts, etc.</td>
</tr>
</tbody>
</table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Continuous Delivery Patterns]]></title>
    <link href="http://bvajjala.github.io/blog/2014/03/11/continuous-delivery-patterns/"/>
    <updated>2014-03-11T00:00:00-04:00</updated>
    <id>http://bvajjala.github.io/blog/2014/03/11/continuous-delivery-patterns</id>
    <content type="html"><![CDATA[<p>ABOUT CONTINUOUS DELIVERY</p>

<p>With Continuous Delivery (CD), teams continuously deliver new  versions of software to production by decreasing the cycle time between an idea and usable software through the automation of the entire delivery system: build, deployment, test, and release. CD is enabled through the Deployment Pipeline, which encompasses a collection of patterns described in this Refcard.</p>

<p>CD is concerned with “…how all the moving parts fit together: configuration management, automated testing, continuous integration and deployment, data management, environment management, and release management.” (1)</p>

<p>THE DEPLOYMENT PIPELINE</p>

<p>The purpose of the deployment pipeline is threefold:</p>

<p> • Visibility: All aspects of the delivery system &ndash; building, deploying, testing, and releasing – are visible to all team members promoting collaboration.</p>

<p> • Feedback: Team members learn of problems as soon as they occur so that issues are fixed as soon as possible.</p>

<p> • Continually Deploy: Through a fully automated process, you can deploy and release any version of the software to any environment. (1)</p>

<p>In the Deployment Pipeline diagram above, all of the patterns are shown in context. There are some patterns that span multiple stages of the pipeline, so I chose the stage where it’s most predominately used.</p>

<p>BENEFITS
 • Empowering Teams: Because the deployment pipeline is a pull system, testers, developers, operations, and others can self service the application version into an environment of their choice.</p>

<p> • Reducing Errors: Ensuring the correct version, configuration, database schema, etc. are applied the same way every time through automation.</p>

<p> • Lowering Stress: Through push-button releases to production and Rehearsing Deployments, a release becomes commonplace without the typical stress.</p>

<p> • Deployment Flexibility: Instantiate a new environment or configuration by making a few changes to the automated delivery system.</p>

<p> • Practice makes Perfect: Through the deployment pipeline, the final deployment into production is being rehearsed every single time the software is deployed to any target environments. (1)</p>

<h1>CONFIGURATION MANAGEMENT</h1>

<p>Configuration Management is “the process by which all artifacts relevant to your project, and the relationships between them, are stored, retrieved, uniquely identified, and modified”. (1)</p>

<p>Note: Each pattern is cited with a number in parentheses that corresponds to the source in the References section.</p>

<h2>Configurable Third-Party Software (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Evaluate and use third-party software that can be easily configured, deployed, and automated. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td> Procuring software that cannot be externally configured. Software without an API or command-line interface that forces teams to use the GUI only. |</td>
</tr>
</tbody>
</table>


<h2>Configuration Catalog (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern  </th>
<th>  Maintain a catalog of all options for each application, how to change these options and storage locations for each application. Automatically create this catalog as part of the build process. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td> Configuration options are not documented. The catalog of applications and other assets is “tribal knowledge”. |</td>
</tr>
</tbody>
</table>


<h2>Mainline (3)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern  </th>
<th>  Minimize merging and keep the number of active code lines manageable by developing on a mainline. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td> Multiple branches per project.</td>
</tr>
</tbody>
</table>


<h2>Merge Daily (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Changes committed to the mainline are applied to each branch on at least a daily basis. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Merging every iteration once a week or less often than once a day. |</td>
</tr>
</tbody>
</table>


<h2>Protected Configuration (5) ,(1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Store configuration information in secure remotely accessible locations such as a database, directory, or registry.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Open text passwords and/or single machine or share.</td>
</tr>
</tbody>
</table>


<h2>Repository (3) , (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>All source files &ndash; executable code, configuration, host environment, and data &ndash; are committed to a version-control repository.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Some files are checked in, others, such as environment configuration or data changes, are not. Binaries – that can be recreated through the build and deployment process – are checked in.</td>
</tr>
</tbody>
</table>


<h2>Short-Lived Branches (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Branches must be short lived – ideally less than a few days and never more than an iteration.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Branches that last more than an iteration. Branches by product feature that live past a release.</td>
</tr>
</tbody>
</table>


<h2>Single Command Environment (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Check out the project’s version-control repository and run a single command to build and deploy the application to any accessible environment, including the local development.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Forcing the developer to define and configure environment variables. Making the developer install numerous tools in order for the build/deployment to work.</td>
</tr>
</tbody>
</table>


<h2>Single Path to Production (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Configuration management of the entire system &ndash; source, configuration, environment and data. Any change can be tied back to a single revision in the version-control system.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Parts of system are not versioned. Inability to get back to a previously configured software system.</td>
</tr>
</tbody>
</table>


<h1>CONTINUOUS INTEGRATION (CI)</h1>

<h2>Build Threshold (5)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Fail a build when a project rule is violated – such as architectural breaches, slow tests, and coding standard violations. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Manual code reviews. Learning of code quality issues later in the development cycle.</td>
</tr>
</tbody>
</table>


<h2>Commit Often (6)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Each team member checks in regularly to trunk &ndash; at least once a day but preferably after each task to trigger the CI system.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Source files are committed less frequently than daily due to the number of changes from the developer.</td>
</tr>
</tbody>
</table>


<h2>Continuous Feedback (6)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Send automated feedback from CI system to all Cross-Functional Team members.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Notifications are not sent; notifications are ignored; CI system spams everyone with information they cannot use.</td>
</tr>
</tbody>
</table>


<h2>Continuous Integration (6)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Building and testing software with every change committed to a project’s version control repository.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Scheduled builds, nightly builds, building periodically, building exclusively on developer’s machines, not building at all. </td>
</tr>
</tbody>
</table>


<h2>Stop the Line (5) , (1) , (4), (12)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Fix software delivery errors as soon as they occur; stop the line. No one checks in on a broken build as the fix becomes the highest priority.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Builds stay broken for long periods of time, thus preventing developers from checking out functioning code. </td>
</tr>
</tbody>
</table>


<h2>Independent Build (6)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Write build scripts that are decoupled from IDEs. These build scripts are executed by a CI system so that software is built at every change. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Automated build relies on IDE settings. Builds are unable to be run from the command line.</td>
</tr>
</tbody>
</table>


<h2>Visible Dashboards</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Provide large visible displays that aggregate information from your delivery system to provide high-quality feedback to the Cross-Functional Team in real time.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Email-only alerts or not publicizing the feedback to the entire team.</td>
</tr>
</tbody>
</table>


<h1>TESTING</h1>

<h2>Automate Tests</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Automate the verification and validation of software to include unit, component, capacity, functional, and deployment tests</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Manual testing of units, components, deployment, and other types of tests.</td>
</tr>
</tbody>
</table>


<blockquote><p>Unit- Automating tests without any dependencies.</p>

<p>Component- Automating tests with dependencies to other components and heavyweight dependencies such as the database or file system.</p>

<p>Deployment- Automating tests to verify the deployment and configuration were successful. Sometimes referred to as a “smoke tests”.</p>

<p>Functional- Automating tests to verify the behavior of the software from a user’s perspective.</p>

<p>Capacity- Automating load and performance testing in near- production conditions.</p></blockquote>

<h2>Isolate Test Data (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Use transactions for database-dependent tests (e.g., component tests) and roll back the transaction when done. Use a small subset of data to effectively test behavior </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td> Using a copy of production data for Commit Stage tests. Running tests against a shared database.</td>
</tr>
</tbody>
</table>


<h2>Parallel Tests (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Run multiple tests in parallel across hardware instances to decrease the time in running tests.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Running tests on one machine or instance. Running dependent tests that cannot be run in parallel.</td>
</tr>
</tbody>
</table>


<h2>Stub Systems (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Use stubs to simulate external systems to reduce deployment complexity.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Manually installing and configuring interdependent systems for Commit Stage build and deployment.</td>
</tr>
</tbody>
</table>


<h1>DEPLOYMENT PIPELINE</h1>

<h2>Deployment Pipeline (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> A deployment pipeline is an automated implementation of your application’s build, deploy, test, and release process. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Deployments require human intervention (other than approval or clicking a button). Deployments are not production ready. </td>
</tr>
</tbody>
</table>


<h2>Value-Stream Map (4) ##</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Create a map illustrating the process from check in to the version-control system to the software release to identify process bottlenecks.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Separately defined processes and views of the checkin to release process.</td>
</tr>
</tbody>
</table>


<p>BUILD AND DEPLOYMENT SCRIPTING</p>

<h2>Dependency Management (5)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Centralize all dependent libraries to reduce bloat, classpath problems, and repetition of the same dependent libraries and transitive dependencies from project to project. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Multiple copies of the same binary dependencies in each and every project. Redefining the same information for each project. Classpath hell!</td>
</tr>
</tbody>
</table>


<h2>Common Language (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> As a team, agree upon a common scripting language &ndash; such as Perl, Ruby, or Python &ndash; so that any team member can apply changes to the Single Delivery System </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Each team uses a different language making it difficult for anyone to modify the delivery system reducing cross-functional team effectiveness.</td>
</tr>
</tbody>
</table>


<h2>Externalize Configuration (5)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Changes between environments are captured as configuration information. All variable values are externalized from the application configuration into build/deployment-time properties </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Hardcoding values inside the source code or per target environment.</td>
</tr>
</tbody>
</table>


<h2>Fail Fast (6)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Fail the build as soon as possible. Design scripts so that processes that commonly fail run first. These processes should be run as part of the Commit Stage.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Common build mistakes are not uncovered until late in the deployment process.</td>
</tr>
</tbody>
</table>


<h2>Fast Builds (6)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> The Commit Build provides feedback on common build problems as quickly as possible &ndash; usually in under 10 minutes.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Throwing everything into the commit stage process, such as running every type of automated static analysis tool or running load tests such that feedback is delayed.</td>
</tr>
</tbody>
</table>


<h2>Scripted Deployment (5)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> All deployment processes are written in a script, checked in to the version-control system, and run as part of the Single Delivery System.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Deployment documentation is used instead of automation. Manual deployments or partially manual deployments. Using GUI to perform a deployment.</td>
</tr>
</tbody>
</table>


<h2>Unified Deployment (5)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> The same deployment script is used for each deployment. The Protected Configuration – per environment &ndash; is variable but managed.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td> Different deployment script for each target environment or even for a specific machine. Manual configuration after deployment for each target environment. </td>
</tr>
</tbody>
</table>


<h1>DEPLOYING AND RELEASING APPLICATIONS</h1>

<h2>Binary Integrity (5)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Build your binaries once, while deploying the binaries to multiple target environments, as necessary.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td> Software is built in every stage of the deployment pipeline. </td>
</tr>
</tbody>
</table>


<h2>Canary Release</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Release software to production for a small subset of users (e.g. , 10%) to get feedback prior to a complete rollout. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td> Software is released to all users at once.|</td>
</tr>
</tbody>
</table>


<h2>Blue-Green Deployments (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Deploy software to a non-production environment (call it blue) while production continues to run. Once it’s deployed and “warmed up”, switch production (green) to non-production and blue to green simultaneously. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Production is taken down while the new release is applied to production instance(s).</td>
</tr>
</tbody>
</table>


<h2>Dark Launching (11)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Launch a new application or features when it affects the least amount of users.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Software is deployed regardless of number of active users.</td>
</tr>
</tbody>
</table>


<h2>Rollback Release (5)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Provide an automated single command rollback of changes after an unsuccessful deployment.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Manually undoing changes applied in a recent deployment. Shutting down production instances while changes are undone. </td>
</tr>
</tbody>
</table>


<h2>Self-Service Deployment (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Any Cross-Functional Team member selects the version and environment to deploy the latest working software. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Deployments released to team are at specified intervals by the “Build Team”. Testing can only be performed in a shared state without isolation from others. </td>
</tr>
</tbody>
</table>


<h1>INFRASTRUCTURE AND ENVIRONMENTS</h1>

<h2>Automate Provisioning (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Automate the process of configuring your environment to include networks, external services, and infrastructure.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Configured instances are “works of art” requiring team members to perform partially or fully manual steps to provision them.</td>
</tr>
</tbody>
</table>


<h2>Behavior-Driven Monitoring (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Automate tests to verify the behavior of the infrastructure. Continually run these tests to provide near real-time alerting. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>No real-time alerting or monitoring. System configuration is written without tests.</td>
</tr>
</tbody>
</table>


<h2>Immune Systems</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Deploy software one instance at a time while conducting Behavior-Driven Monitoring. If an error is detected during the incremental deployment, a Rollback Release is initiated to revert changes.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Non-incremental deployments without monitoring.</td>
</tr>
</tbody>
</table>


<h2>Lockdown Environments (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Lock down shared environments from unauthorized external and internal usage, including operations staff. All changes are versioned and applied through automation.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>The “Wild West”: any authorized user can access shared environments and apply manual configuration changes, putting the environment in an unknown state leading to deployment errors.</td>
</tr>
</tbody>
</table>


<h2>Production-Like Environments (1) ##</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Target environments are as similar to production as possible.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Environments are “production like” only weeks or days before a release. Environments are manually configured and controlled.</td>
</tr>
</tbody>
</table>


<h2>Transient Environments</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Utilizing the Automate Provisioning, Scripted Deployment and Scripted Database patterns, any environment should be capable of terminating and launching at will.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Environments are fixed to “DEV, QA” or other pre-determined environments.</td>
</tr>
</tbody>
</table>


<h1>DATA</h1>

<h2>Database Sandbox (7)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Create a lightweight version of your database – using the Isolate Test Data pattern. Each developer uses this lightweight DML to populate his local database sandboxes to expedite test execution.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Shared database. Developers and testers are unable to make data changes without it potentially adversely affecting other team members immediately.</td>
</tr>
</tbody>
</table>


<h2>Decouple Database (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Ensure your application is backward and forward compatible with your database so you can deploy each independently</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Application code data are not capable of being deployed separately.</td>
</tr>
</tbody>
</table>


<h2>Database Upgrade (7)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Use scripts to apply incremental changes in each target environment to a database schema and data. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Manually applying database and data changes in each target environment.</td>
</tr>
</tbody>
</table>


<h2>Scripted Database (7)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Script all database actions as part of the build process. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Using data export/import to apply data changes. Manually applying schema and data changes to the database.</td>
</tr>
</tbody>
</table>


<h1>INCREMENTAL DEVELOPMENT</h1>

<h2>Branch by Abstraction (2)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Instead of using version-control branches, create an abstraction layer that handles both an old and new implementation. Remove the old implementation.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Branching using the version-control system leading to branch proliferation and difficult merging. Feature branching.</td>
</tr>
</tbody>
</table>


<h2>Toggle Features (10)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Deploy new features or services to production but limit access dynamically for testing purposes.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Waiting until a feature is fully complete before committing the source code.</td>
</tr>
</tbody>
</table>


<h1>COLLABORATION</h1>

<h2>Delivery Retrospective (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>For each iteration, hold a retrospective meeting where everybody on the Cross-Functional Team discusses how to improve the delivery process for the next iteration. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Waiting until an error occurs during a deployment for Dev and Ops to collaborate. Having Dev and Ops work separately.</td>
</tr>
</tbody>
</table>


<h2>Cross-Functional Teams (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Everybody is responsible for the delivery process. Any person on the Cross-Functional Team can modify any part of the delivery system.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Siloed teams: Development, Testing, and Operations have their own scripts and processes and are not part of the same team. </td>
</tr>
</tbody>
</table>


<pre><code>Amazon.com has an interesting take on this approach. They call it “You build it, you run it”. Developers take the software they’ve written all the way to production. 
</code></pre>

<h2>Root-Cause Analysis (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Learn the root cause of a delivery problem by asking “why” of each answer and symptom until discovering the root cause.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Accepting the symptom as the root cause of the problem.</td>
</tr>
</tbody>
</table>


<h1>TOOLS</h1>

<p>This is meant to be an illustrative list, not an exhaustive list, to give you an idea of the types of tools and some of the vendors that help to enable effective Continuous Delivery. The Java, .NET and Ruby platforms are represented. The tools that span categories have been assigned to the most appropriate category or duplicated when necessary.</p>

<table>
<thead>
<tr>
<th></th>
<th> Category </th>
<th> Example Software Tools </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Configuration Management </td>
<td> &lsquo;Subversion (SVN), git, Perforce, PassPack, PasswordSafe, ESCAPE, ConfigGen&rsquo; |</td>
</tr>
<tr>
<td></td>
<td> Continuous Integration </td>
<td> &lsquo;Bamboo, Jenkins, AntHill Pro, Go, TeamCity, TFS 2010, Electric Commander&rsquo; &lsquo;Supporting tools: Doxygen, Grand, GraphViz, JavaDoc, NDoc, SchemaSpy, UmlGraph, CheckStyle, Clover, Cobertura, FindBugs,FxCop, JavaNCSS, JDepend, PMD, Sonar, Simian&rsquo; |</td>
</tr>
<tr>
<td></td>
<td> Testing </td>
<td> Twist , AntUnit, Cucumber, DbUnit, webrat, easyb, Fitnesse, JMeter, JUnit, NBehave, SoapUI, Selenium, RSpec, SauceLabs |</td>
</tr>
<tr>
<td></td>
<td> Deployment Pipeline </td>
<td> Go, AntHill Pro |</td>
</tr>
<tr>
<td></td>
<td> Build and Deployment Scripting </td>
<td> Ant, AntContrib, NAnt, MSBuild, Buildr, Gant, Gradle, make, Maven, Rake, Java Secure Channel, ControlTier, Altiris, Capistrano, Fabric, Func |</td>
</tr>
<tr>
<td></td>
<td> Infrastructure and Environments </td>
<td> AWS EC2, AWS S3, Windows Azure, Google App Engine, AWS Elastic Beanstalk, Heroku, Capistrano, Cobbler, BMC Bladelogic, CFEngine, IBM Tivoli Provisioning Manager, Puppet, Chef, Bcfg2, AWS Cloud Formation, Windows Azure AppFabric, rPath, JEOS, BoxGrinder, CLIP, Eucalyptus, AppLogic, CloudKick, CloudWatch, Nagios, Zabbix, Zenoss |</td>
</tr>
<tr>
<td></td>
<td> Data </td>
<td> Hibernate, MySQL, Liquibase, Oracle, PostgreSQL, SQL Server, SimpleDB, SQL Azure, Ant, MongoDB, dbdeploy |</td>
</tr>
<tr>
<td></td>
<td> Components and Dependencies </td>
<td> Ivy, Archiva, Nexus, Artifactory, Bundler |</td>
</tr>
<tr>
<td></td>
<td> Collaboration </td>
<td> Mingle, Greenhopper, JIRA |</td>
</tr>
</tbody>
</table>


<h1>REFERENCES</h1>

<ol>
<li><p>Jez Humble and David Farley, “Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation”, Addison Wesley Professional, 2010</p></li>
<li><p>Paul Hammant and www.continuousdelivery.com</p></li>
<li><p>Stephen P. Berczuk and Brad Appleton, “Software Configuration Management Patterns.”, Addison Wesley Professional, 2003</p></li>
<li><p>Mary and Tom Poppendieck, “Leading Lean Software Development”, Addison Wesley, 2009</p></li>
<li><p>Paul M. Duvall, “Continuous integration. Patterns and Antipatterns”, DZone refcard #84, 2010 <a href="http://bit.ly/l8rfVS">http://bit.ly/l8rfVS</a></p></li>
<li><p>Paul M. Duvall, “Continuous integration. Improving Software Quality and Reducing Risk”, Addison Wesley, 2007</p></li>
<li><p>Scott W. Ambler and Pramodkumar J. Saladage, “Refactoring Databases. Evolutionary Database Design”, Addison Wesley, 2006.</p></li>
<li><p>Paul M. Duvall, IBM developerWorks series “Automation for the people” <a href="http://ibm.co/iwwvPX">http://ibm.co/iwwvPX</a></p></li>
<li><p>IMVU: <a href="http://bit.ly/jhqP5f">http://bit.ly/jhqP5f</a></p></li>
<li><p>Martin Fowler and Facebook: <a href="http://on.fb.me/miBrOM">http://on.fb.me/miBrOM</a></p></li>
<li><p>Facebook Engineering: <a href="http://on.fb.me/miBrOM">http://on.fb.me/miBrOM</a></p></li>
<li><p>Paul Julius, Enterprise Continuous Integration Maturity Model, <a href="http://bit.ly/m7h5vC">http://bit.ly/m7h5vC</a></p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Jenkins Job Builder and How to Extned it]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/22/jenkins-job-builder-and-how-to-extned-it/"/>
    <updated>2014-02-22T08:57:36-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/22/jenkins-job-builder-and-how-to-extned-it</id>
    <content type="html"><![CDATA[<h1>What is jenkins job builder</h1>

<p>Jenkins job builder is extreme good tool to manage your jenkins CI jobs, it takes simple description from YAML files, and use them to configure jenkins.</p>

<pre><code>#set free style job
#job-template.yml
- job:
    name: testjob
    project-type: freestyle
    defaults: global
    disabled: false
    display-name: 'Fancy job name'
    concurrent: true
    quiet-period: 5
    workspace: /srv/build-area/job-name
    block-downstream: false
    block-upstream: false
</code></pre>

<p>Then put your jenkins access into jenkins.ini file</p>

<pre><code>[jenkins]
user=USERNAME
password=USER_TOKEN
url=JENKINS_URL
ignore_cache=IGNORE_CACHE_FLAG
</code></pre>

<p>Based on the job configuration above, you just need to type command</p>

<pre><code>$ jenkins-jobs --conf jenkins.ini update job-template.yaml 
</code></pre>

<p>Then your job <em>testjob</em> is created in your jenkins server.</p>

<p>The project is created by <a href="https://wiki.openstack.org/wiki/InfraTeam">openstack-infrastructure team</a>, it is used to manage the openstack environment, fairly good.</p>

<h1>How it works</h1>

<p>There is no magic behind it, <em>jenkins-jobs</em> just convert the <em>job-template.yaml</em> to jenkins XML request file, and use jenkins remote API to send create request.</p>

<p>Try to do below to understand this.</p>

<pre><code>$ jenkins-jobs test job-template.yaml -o .
</code></pre>

<p>Then xml file <em>testjob</em> is created, see</p>

<pre><code>&lt;?xml version="1.0" ?&gt;
&lt;project&gt;
  &lt;actions/&gt;
  &lt;description&gt;

&amp;lt;!-- Managed by Jenkins Job Builder --&amp;gt;&lt;/description&gt;
  &lt;keepDependencies&gt;false&lt;/keepDependencies&gt;
  &lt;disabled&gt;false&lt;/disabled&gt;
  &lt;displayName&gt;Fancy job name&lt;/displayName&gt;
  &lt;blockBuildWhenDownstreamBuilding&gt;false&lt;/blockBuildWhenDownstreamBuilding&gt;
  &lt;blockBuildWhenUpstreamBuilding&gt;false&lt;/blockBuildWhenUpstreamBuilding&gt;
  &lt;concurrentBuild&gt;true&lt;/concurrentBuild&gt;
  &lt;customWorkspace&gt;/srv/build-area/job-name&lt;/customWorkspace&gt;
  &lt;quietPeriod&gt;5&lt;/quietPeriod&gt;
  &lt;canRoam&gt;true&lt;/canRoam&gt;
  &lt;properties/&gt;
  &lt;scm class="hudson.scm.NullSCM"/&gt;
  &lt;builders/&gt;
  &lt;publishers/&gt;
  &lt;buildWrappers/&gt;
&lt;/project&gt;
</code></pre>

<p>Now you can use curl command to send the request (testjob) directly !!</p>

<pre><code>$ curl --user USER:PASS -H "Content-Type: text/xml" -s --data "@testjob" "http://jenkins-server/createItem?name=testjob"
</code></pre>

<h2>How to recreate your jenkins job</h2>

<p>Looks great, finally you need think about how to re-create your jenkins job, it is also simple, just download the config.xml</p>

<pre><code>$ curl --user USER:PASS http://jenkins-server/testjob/config.xml
</code></pre>

<p>Or open the configuration page in broswer *<a href="http://jenkins-server/testjob/configure*">http://jenkins-server/testjob/configure*</a> and map from YAML file.</p>

<p>You need to read <a href="http://ci.openstack.org/jenkins-job-builder/configuration.html">jenkins job builder&rsquo;s guideline</a> to know the map, generate it had level Macro like <a href="https://wiki.openstack.org/wiki/InfraTeam">builders</a>, which is connected to the <a href="https://github.com/openstack-infra/jenkins-job-builder/blob/master/jenkins_jobs/modules/builders.py">real python builders module</a> to do transformation from YAML to XML.</p>

<p>What you stated in YAML file like</p>

<pre><code>-job:
  name: test_job
  builders:
- shell: "make test"
</code></pre>

<p>it will be converted to</p>

<pre><code>&lt;builders&gt;
&lt;hudson.tasks.Shell&gt;
  &lt;command&gt;make test&lt;/command&gt;&lt;/hudson.tasks.Shell&gt;
&lt;/builders&gt;
</code></pre>

<h2>How to extend</h2>

<p>Greatly to see jenkins job builder already had lots of default modules to support your normal jenkins jobs, but there is exceptions like some none popular jenkins plugins or your own plugins.</p>

<p>Then it is time to extend the module, the existing document: Extending is not clear enough, I will use example to show how it works, code is in <a href="https://github.com/bv2012/jenkins-buddy">github jenkins-buddy</a> project</p>

<p><a href="https://wiki.jenkins-ci.org/display/JENKINS/ArtifactDeployer+Plugin">ArtifactDeployer</a> Plugin is used as example, this plugin is the popular plugin to deploy the artifacts to other folder.</p>

<p>Artifact Deploy Plugin</p>

<p><img src="../downloads/code/artifactdeploy.png" alt="" /></p>

<p>And I want to have .YAML like below</p>

<pre><code>*#artifactdeploy.yaml*
- job:
name: test-job
publishers:
  - artifactdeployer: 
  includes: 'buddy-*.tar.gz'
  remote: '/project/buddy'
</code></pre>

<h2>write codes to transform</h2>

<p>Now I need to download the existing jobs to see how XML looks like, using curl above, I got it like</p>

<pre><code>&lt;publishers&gt;
   ...  
  &lt;org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerPublisher plugin="artifactdeployer@0.27"&gt;
&lt;entries&gt;
  &lt;org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerEntry&gt;
&lt;includes&gt;buddy-*.tar.gz&lt;/includes&gt;
&lt;basedir&gt;&lt;/basedir&gt;
&lt;excludes&gt;&lt;/excludes&gt;
&lt;remote&gt;/project/buddy&lt;/remote&gt;
&lt;flatten&gt;false&lt;/flatten&gt;
&lt;deleteRemote&gt;false&lt;/deleteRemote&gt;
&lt;deleteRemoteArtifacts&gt;false&lt;/deleteRemoteArtifacts&gt;
&lt;deleteRemoteArtifactsByScript&gt;false&lt;/deleteRemoteArtifactsByScript&gt;
&lt;failNoFilesDeploy&gt;false&lt;/failNoFilesDeploy&gt;
  &lt;/org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerEntry&gt;
&lt;/entries&gt;
&lt;deployEvenBuildFail&gt;false&lt;/deployEvenBuildFail&gt;
  &lt;/org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerPublisher&gt;
..
&lt;/publishers&gt; 
</code></pre>

<p>It belongs the section publishers So I write the jenkins_buddy/modules/publishers.py module to add one function artifactdeployer:</p>

<pre><code>def artifactdeployer(parser, xml_parent, data):
    logger = logging.getLogger("%s:artifactdeployer" % __name__)
    artifactdeployer = XML.SubElement(xml_parent, 'org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerPublisher')
    entries = XML.SubElement(artifactdeployer, 'entries')
    entry = XML.SubElement(entries, 'org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerEntry')
    print data
    XML.SubElement(entry, 'includes').text = data['includes']
    XML.SubElement(entry, 'remote').text = data['remote']
</code></pre>

<p>It is the core part handling convert.</p>

<h3>Hook into jenkins-job builder</h3>

<p>Now you need hook this script into jenkins-jobs builder, thank for the entry_points in python, it can be used for this.</p>

<p>Create the plugin related script and structure, add new entry_point in setup.py</p>

<pre><code>#setup.py in jenkins-buddy
entry_points={
    'jenkins_jobs.publishers': [
    'artifactdeployer=jenkins_buddy.modules.publishers:artifactdeployer',
    ],
}
</code></pre>

<p>it tells jenkins-jobs if you meet new keyword artifactdeployer in publishers, please let me jenkins_buddy.modules.publishers:artifactdeployer to handle.</p>

<h3>Verify it</h3>

<p>Build the pip package local and install it</p>

<pre><code>$ python setup.py sdist
$ pip install dist/jenkins-buddy-0.0.5.zip
</code></pre>

<p>And verify the new job, Bingo, it works.</p>

<pre><code>$ jenkins-jobs test artifactdeploy.yaml -o . 
</code></pre>

<h3>###Make it more complete by checking jenkins plugin java code</h3>

<p>Maybe you noticed, it is hack solution, since I skipped some parameter converting and guess what the XML will look like, if you want to make it more complete, we need to check the java codes directly.</p>

<p>src/main/java/org/jenkinsci/plugins/artifactdeployer/ArtifactDeployerPublisher.java is the class we need to take care.</p>

<pre><code>@DataBoundConstructor
public ArtifactDeployerPublisher(List&lt;ArtifactDeployerEntry&gt; deployedArtifact, boolean deployEvenBuildFail) {
    this.entries = deployedArtifact;
    this.deployEvenBuildFail = deployEvenBuildFail;
    if (this.entries == null)
    this.entries = Collections.emptyList();
}
</code></pre>

<p>It is directly mapping from XML into internal data, if you need know more, learn how to develop jenkins plugin.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Continuous Delivery with Docker and Jenkins - part II]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/21/continuous-delivery-with-docker-and-jenkins-part-ii/"/>
    <updated>2014-02-21T14:41:15-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/21/continuous-delivery-with-docker-and-jenkins-part-ii</id>
    <content type="html"><![CDATA[<p>A few weeks ago I started talking about how we use <a href="2014-02-21-continuous-delivery-with-docker-and-jenkins-part-i">Docker and Jenkins for Continuous Delivery</a> in our staging environment. Today, we are open-sourcing a simple bash utility for managing inter-container dependencies, <a href="https://github.com/bv2012/dockerize">Dockerize</a>.</p>

<p>Before I go into specifics, I want to describe our workflow with Jenkins and Docker from a high-level perspective.</p>

<ul>
<li><p>let’s take the <a href="https://github.com/bv2012/hi_sinatra-docker">hi_sinatra</a> Ruby example app. It has its own GitHub repository and we have a simple, non-git Jenkins job for it.</p></li>
<li><p>every commit pushed to GitHub, regardless of the branch, triggers a Jenkins build (via Amazon SQS). All Jenkins builds will result in a Docker image. A successful build will produce a running Docker container. A failed build will produce a stopped container which can be investigated by either looking at the logs or starting it with a tty attached.</p></li>
<li><p>if Docker doesn’t have a <strong>hi_sinatra:master</strong> pre-built image, a new one will be created from the master branch. This master image gets re-built every time there’s a commit against the master branch. Having a master image speeds up image builds considerably (eg. installing Ruby gems, installing node modules, C extensions etc). The resulting image won’t use any caching and all intermediary images will be removed. Just to clarify, this image will not be shipped into production.</p></li>
<li><p>if a Docker image with that app’s name, branch name and git commit sha doesn’t exist, we want Docker to build it for us. At this point, we’re interested to have the eg. <strong>hi_sinatra:second-blog-post.a8e8e83 </strong>Docker image available.</p></li>
<li><p>before a new container can be started from the image that we’ve just built, all services that the app requires must be running in their own independent containers. Our <strong>hi_sinatra</strong> example app requires a running Redis server.</p></li>
<li><p>when all dependent services are running in their own containers, we start a container from the newly built app image (in our example, <strong>hi_sinatra:second-blog-post.a8e8e83</strong>). All dependent containers will have their IPs exposed via env options, eg. docker run -e REDIS_HOST=172.17.0.8 -d &hellip;</p></li>
<li><p>before our <strong>hi_sinatra app</strong> starts in its new Docker container, all tests must pass both unit, integration and acceptance. Full stack tests (also known as acceptance tests) use sandbox services, but they are setup via the same Docker containers that will be made available in production. Code portability is Docker’s strongest point, we’re making full use of it.</p></li>
<li><p>if everything worked as expected, including interactions with all external services, this Docker image will be tagged as production. The service responsible for bringing up new Docker containers from the latest production images will take it from here.</p></li>
</ul>


<p>Docker containers running on the CI are available only on our office network, anyone inside it can connect to them. All that it takes to get an instance for a specific app (and all its dependencies) is to push a new branch to GitHub.</p>

<h2>Dockerize</h2>

<p>Dockerize acts as a Docker proxy, meaning that all commands which it does not understand get forwarded to the docker binary. Dockerize has just 2 dependencies: bash &amp; git.</p>

<p>The previously described workflow as a single shell command:</p>

<pre><code>dockerize boot cambridge-healthcare/hi_sinatra-docker hi_sinatra
</code></pre>

<p>The hi_sinatra app comes with 2 files that Dockerize picks up on:</p>

<ul>
<li><p>dockerize.containers which defines dependencies on other containers (another service such as Redis server or another app)</p></li>
<li><p>dockerize.envs which will forward specific environment variables from the Docker host into the container</p></li>
</ul>


<p>The Vagrantfile that comes with hi_sinatra will get you up and running with Docker, Jenkins and now Dockerize. The quickest way to try the whole setup (<a href="2014-02-21-continuous-delivery-with-docker-and-jenkins-part-i">provided you have Vagrant installed</a>):</p>

<pre><code>git clone https://github.com/cambridge-healthcare/hi_sinatra-docker.git
cd hi_sinatra-docker
vagrant up
</code></pre>

<p>By the time the VM gets provisioned, there will be a running version of <strong>hi_sinatra</strong> inside a Docker container using a Redis server running in a separate container for tracking requests. Use the IP address and port displayed at the end of the Vagrant run to access the hi_sinatra app in your browser.</p>

<h2>Jenkins + Dockerize</h2>

<p>Dockerize makes Jenkins integration with Docker incredibly simple. In the Jenkins instance running on the Vagrant VM that we have just built, add the following job through the Jenkins web interface:</p>

<p>| Job name | hi_sinatra  |
| Job type | Build a free-style software project |
| Build| Execute shell   |</p>

<p>This is the shell command which you will need to use for the build execution:</p>

<pre><code>/bin/bash -c "source $HOME/.profile &amp;&amp; dockerize boot cambridge-healthcare/hi_sinatra-docker hi_sinatra"
</code></pre>

<p>Every successful Jenkins build will now result in a running Docker container.</p>

<p>CI setups are always opinionated. We have a few more additions such as Campfire notifications, Amazon SQS integration with GitHub and a few others which are specific to our infrastructure. The above Jenkins integration example with Docker is meant to be a most conservative starting point for your own setup.</p>

<p>Until next time!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Continuous Delivery with Docker and Jenkins - part I]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/21/continuous-delivery-with-docker-and-jenkins-part-i/"/>
    <updated>2014-02-21T14:40:59-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/21/continuous-delivery-with-docker-and-jenkins-part-i</id>
    <content type="html"><![CDATA[<p>We have been using Docker in our staging environment for nealrt several months now and are right now planning to make it part of our production setup once the first stable version gets released. We’ll be discussing the staging environment setup today with the promise of following up on the production environment at a later date.</p>

<!--more-->


<p>Docker is a utility for creating virtualized Linux containers for shipping self-contained applications. As opposed to a traditional VM which runs a full-blown operating system on top of the host, Docker leverages LinuX Containers (LXC) which run on the same operating system. This results in a more efficient usage of system resource by trading some of the isolation specific to hypervisors. What makes Docker appealing is that applications can be packaged as self-contained containers, shipped around as small data blobs and brought up as fully independent hosts in a matter of seconds. If an Amazon Machine Image (AMI) takes a few minutes to boot, the equivalent Docker images take a few seconds at most (normally ~1s). To find out more about Docker internals, see Docker, The Whole Story.</p>

<p>We have converted our entire staging environment from a handful of AMIs to a single bare metal host running Docker. We have made it more efficient and faster to bring up versions of services which undergo rigorous testing before they get shipped into production.</p>

<p>Whenever a new github branch gets started, Jenkins, our Continuous Integration server, automatically attempts to build a new Docker container from it. If all tests pass, this container becomes available on our office network and we receive a Campfire notification. If tests fail, we leave a Docker image for our engineers to examine. For Service Oriented Architectures (SOA), this approach saves a lot of time when working on features that span multiple services and cannot be isolated to a particular component. The extra confidence that we get from integrating features at a platform level means that we are more effective and don’t need to wait on one another.</p>

<p>We couldn’t find any clear guide on integrating Docker with Jenkins so we’ve decided to contribute one. We have included a Vagrantfile which automates the entire setup except creating Jenkins jobs. We provide an example Sinatra app which includes all the required configuration to get everything working end-to-end, feel free to use it as the starting point for your own setup.</p>

<h2>1. Install VirtualBox, Vagrant &amp; git</h2>

<p>Either install using your package manager or use the official downloads:</p>

<ul>
<li><a href="https://www.virtualbox.org/">install virtualbox</a></li>
<li><a href="http://www.vagrantup.com/">install vagrant</a></li>
<li><a href="http://git-scm.com/downloads">install git</a></li>
</ul>


<h2>2. Create Vagrant VM</h2>

<p>This <a href="https://github.com/bv2012/hi_sinatra-docker/blob/master/Vagrantfile">Vagrantfile</a> will get everything setup for you. Cloning the repository and running vagrant up inside it will create a VM with the latest stable Docker and Jenkins services running side-by-side. Jenkins belongs to the docker group and can run Docker commands directly.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone https://github.com/bv2012/hi_sinatra-docker.git
</span><span class='line'>cd hi_sinatra-docker
</span><span class='line'>vagrant up</span></code></pre></td></tr></table></div></figure>


<h2>3. Setup Jenkins job</h2>

<p>Find the Jenkins Server running at <a href="http://localhost:8080/,">http://localhost:8080/,</a> install the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Git+Plugin">Git plugin</a>.</p>

<p>Once this is successfully installed and Jenkins is restarted, add the following job:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>| Job name               | hi_sinatra                                                    |
</span><span class='line'>| Job type               | Build a free-style software project                           |
</span><span class='line'>| Source Code Management | Git                                                           |
</span><span class='line'>| Repository URL         | https://github.com/bv2012/hi_sinatra-docker.git |
</span><span class='line'>| Build                  | Execute shell</span></code></pre></td></tr></table></div></figure>


<p>This is the shell command which you will need to use for the build execution:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>service=$JOB_NAME
</span><span class='line'>service_port=8000
</span><span class='line'>branch=$(echo $GIT_BRANCH | cut -d/ -f 2)
</span><span class='line'>
</span><span class='line'>docker build -t $service:$branch $WORKSPACE
</span><span class='line'>
</span><span class='line'>container_id=$(docker run -d -p $service_port $service:$branch)
</span><span class='line'>container_port=$(docker inspect $container_id | awk 'BEGIN { FS = "\"" } ; /"'$service_port'":/ { print $4 }')
</span><span class='line'>
</span><span class='line'>echo "App running on http://localhost:$container_port"</span></code></pre></td></tr></table></div></figure>


<p>The above app includes a Dockerfile which builds a Docker image. The first Docker build will take longer (depending on your internet connection), but as Docker caches build steps (pro tip: apart from ADD), subsequent builds will be significantly quicker.</p>

<h2>4. Successful build results in a running Docker container</h2>

<p>Building the project for the first time (truncated output):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Building in workspace /home/jenkins/.jenkins/jobs/hi_sinatra/workspace
</span><span class='line'>Cloning repository https://github.com/bv2012/hi_sinatra-docker.git
</span><span class='line'>Commencing build of Revision bbb5383939cf719745c232c67f0dffe99b639d91 (origin/master, origin/HEAD)
</span><span class='line'>.
</span><span class='line'>.
</span><span class='line'>.
</span><span class='line'>Step 1 : FROM howareyou/ruby_2.0.0-p247
</span><span class='line'>Pulling repository howareyou/ruby_2.0.0-p247
</span><span class='line'>.
</span><span class='line'>.
</span><span class='line'>.
</span><span class='line'>Step 9 : RUN cd /var/apps/$SERVICE && bin/test
</span><span class='line'> ---> Running in bbaaf476e848
</span><span class='line'>Run options: include {:focus=>true}
</span><span class='line'>
</span><span class='line'>All examples were filtered out; ignoring {:focus=>true}
</span><span class='line'>.
</span><span class='line'>
</span><span class='line'>Finished in 0.02125 seconds
</span><span class='line'>1 example, 0 failures
</span><span class='line'>.
</span><span class='line'>.
</span><span class='line'>App running on http://localhost:49153
</span><span class='line'>
</span><span class='line'>Finished: SUCCESS</span></code></pre></td></tr></table></div></figure>


<p>While the first build takes 2 mins and 26 secs, the second one takes a mere 5 secs. That is 5 seconds to install all the ruby gems, run all the tests, build a Docker image and start a new Docker container that makes that app version available for further testing (eg. integration tests, stress tests). The resulting app image is a mere 12.29kB. That’s the only new content which needs deploying into production.</p>

<h3>github service hooks</h3>

<p>For integrating github with a Jenkins server not accessible from the outside world, we have found Amazon SQS to be an elegant solution. There is a <a href="https://wiki.jenkins-ci.org/display/JENKINS/GitHub+SQS+Plugin">Github SQS plugin</a> that is installable from within Jenkins, setup is straightforward.</p>

<p>The only gotcha is that the SQS must be setup in the us-east-1 region. We had set it up initially in eu-west-1 and were puzzled as to why it wasn’t working.</p>

<p>&ldquo;How are you?&rdquo; base Docker images
During our use of Docker, we have used the public Docker images on the <a href="https://index.docker.io/u/howareyou/">public Docker index</a>. The app which we have given as an example makes use of howareyou/ruby_2.0.0-p247 and all its dependencies.</p>

<p>If you have found this tutorial useful, please help us to improve it by adding your contributions to hi_sinatra-docker.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Docker To Run Ruby Rspec CI In Jenkins]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/17/using-docker-to-run-ruby-rspec-ci-in-jenkins/"/>
    <updated>2014-02-17T17:01:40-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/17/using-docker-to-run-ruby-rspec-ci-in-jenkins</id>
    <content type="html"><![CDATA[<p>In this post, I am going to give a step-by-step introduction into how you can do continuous integration testing with Docker. I will be running the rspec test suite of the CloudFoundry project&rsquo;s Cloud Controller component, although the same process can be applied to any Ruby project. I will show how to build Docker images to easily run repeatable tests and how to set-up Jenkins to do it for you in an automated manner.</p>

<h1>Continuous Integration Using Docker</h1>

<p>The goal of this post is to show Jenkins running a project’s test-suite using Docker. This will occur following every code check-in or every N minutes or whenever it is needed.</p>

<p>Why use Docker to do this? Having a clean environment to run tests is one of the ten commandments of running tests. With Docker&rsquo;s Dockerfile, you can specify a series of steps to create the full stack of the test environment you need. Docker can follow the steps to pre-build the test environment, then stash that environment for disposable re-use. Since a running Docker image, or [LXC] &ldquo;container&rdquo;, is ephemeral, you can blow it away and re-create it very quickly. Perfect for continuous integration!</p>

<!--more-->


<p>My Docker usage will be two-step. First, I will create the Docker image. This will have all the basics required by any test run from this project. I am basing my assumptions on system requirements from the current state of the project.</p>

<p>It will not have everything installed, because I cannot predict what a developer will do during a day of hacking on code. They may change code dependencies (gem dependencies in this case) and so I cannot install those dependencies until the time I run that version of the code.</p>

<p>The second step will be to take my built Docker image and run it every time a new version of the project’s code is created. I do not have access to create a GitHub code commit hook, which would tell Jenkins to run the tests on each code check-in, so instead I will run it periodically.</p>

<p>Since I can re-use the Docker image for all my subsequent test runs, I will be creating my Docker-based test environment (step 1) far less frequently than running my tests (step 2).</p>

<p>I can use Jenkins to perform both these tasks. In one Jenkins job, run maybe once a day, it can recreate the base Docker image and push it to a local Docker repository. In a second Jenkins job, which is run each time a developer commits code, I can run the Docker image, which will pull it from the local Docker repository.</p>

<h2>Guinea Pig</h2>

<p>I am going run the test suite of Cloud Foundry&rsquo;s Cloud Controller. This is a core component of the Cloud Foundry project and one of the most complex pieces. The test suite is very large, so it takes more time to run than a developer would have patience for, which for me is about 2 hours. This makes it ideal for continuous testing in the background to confirm that nobody has checked in code that breaks the test suite.</p>

<h2>CI Docker Image</h2>

<p>My continuous-integration Docker image has 3 parts&hellip;</p>

<p>1) Specify a base image</p>

<p>I am going to use the &ldquo;ubuntu&rdquo; image from <a href="http://index.docker.io.">http://index.docker.io.</a>
2) Install dependencies</p>

<p>Dependencies will be installed via apt-get, wget, rbenv, rubygems and Ruby&rsquo;s bundler.
3) Specify the command that &ldquo;docker run&rdquo; executes when this Docker image is used</p>

<p>I want to ensure I have the latest code (via &ldquo;git pull&rdquo;) and that we install any code-level dependencies (via &ldquo;bundle install&rdquo;). Finally, it should run the test suite.</p>

<p>The exit code of the test suite will be returned by &ldquo;docker run&rdquo; and Jenkins will use this to determine if the tests passed or failed. If the test run fails Jenkins will inform relevant people via email, if we configure it to do so.</p>

<p>Dockerfile</p>

<p>A Dockerfile is a cross between assembler and a bash script. There are certain action keywords that each non-whitespace non-comment line starts with. I like to uppercase these, so they stand out, but uppercasing these is not mandatory. The remainder of each line is the content used by that action keyword.</p>

<p>For instance, &ldquo;FROM&rdquo; is used to specify the base image, so &ldquo;FROM ubuntu&rdquo; specifies that I am using the &ldquo;ubuntu&rdquo; base image.</p>

<p>&ldquo;RUN&rdquo; is used to run a shell command and is commonly used to install dependencies.</p>

<p>&ldquo;ENV&rdquo; can set environment variables, which can be used in subsequent actions, but also persists to the &ldquo;CMD&rdquo; action.</p>

<p>&ldquo;CMD&rdquo; is called when &ldquo;docker run&rdquo; is run against your created image. &ldquo;CMD&rdquo; is ignored during the image building.</p>

<p>Here is my Dockerfile (gist here)&hellip;</p>

<h1>docker image for running CC test suite</h1>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>FROM ubuntu
</span><span class='line'>
</span><span class='line'>RUN apt-get -y install wget
</span><span class='line'>RUN apt-get -y install git
</span><span class='line'>
</span><span class='line'># install Ruby 1.9.3-p484
</span><span class='line'>RUN apt-get -y install build-essential zlib1g-dev libreadline-dev libssl-dev libcurl4-openssl-dev
</span><span class='line'>RUN git clone https://github.com/sstephenson/rbenv.git ~/.rbenv
</span><span class='line'>RUN git clone https://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build
</span><span class='line'>RUN echo 'export PATH="$HOME/.rbenv/bin:$PATH"' >> ~/.bash_profile
</span><span class='line'>RUN echo 'eval "$(rbenv init -)"' >> ~/.bash_profile
</span><span class='line'>ENV PATH /.rbenv/bin:/.rbenv/shims:$PATH
</span><span class='line'>RUN echo PATH=$PATH
</span><span class='line'>RUN rbenv init -
</span><span class='line'>RUN rbenv install 1.9.3-p484 && rbenv global 1.9.3-p484
</span><span class='line'>
</span><span class='line'># never install a ruby gem docs
</span><span class='line'>RUN echo "gem: --no-rdoc --no-ri" >> ~/.gemrc
</span><span class='line'>
</span><span class='line'># Install bundler and the "bundle" shim
</span><span class='line'>RUN gem install bundler && rbenv rehash
</span><span class='line'>
</span><span class='line'># Checkout the cloud_controller_ng code
</span><span class='line'>RUN git clone -b master git://github.com/cloudfoundry/cloud_controller_ng.git /cloud_controller_ng
</span><span class='line'>
</span><span class='line'># mysql gem requires these
</span><span class='line'>RUN apt-get -y install libmysqld-dev libmysqlclient-dev mysql-client
</span><span class='line'># pg gem requires this
</span><span class='line'>RUN apt-get -y install libpq-dev
</span><span class='line'># sqlite gem requires this
</span><span class='line'>RUN apt-get -y install libsqlite3-dev
</span><span class='line'>
</span><span class='line'># Optimization: Pre-run bundle install.
</span><span class='line'># It may be that some gems are installed that never get cleaned up,
</span><span class='line'># but this will make the subsequent CMD runs faster
</span><span class='line'>RUN cd /cloud_controller_ng && bundle install
</span><span class='line'>
</span><span class='line'># Command to run at "docker run ..."
</span><span class='line'>CMD if [ -z $BRANCH ]; then BRANCH=master; fi; \
</span><span class='line'>cd /cloud_controller_ng \
</span><span class='line'>&& git checkout $BRANCH \
</span><span class='line'>&& git pull \
</span><span class='line'>&& git submodule init && git submodule update \
</span><span class='line'>&& bundle install \
</span><span class='line'>&& bundle exec rspec spec</span></code></pre></td></tr></table></div></figure>


<p>The above installs Ruby 1.9.3 at a specific patch-level and any known system-level dependencies that may be needed by gems. If a developer added gems that required additional system dependencies, then those would need to be added to the Dockerfile and the Docker image would need to be rebuilt. This happens rarely, but for this reason it would be desirable to have developers own this Dockerfile and put it alongside the code and check it in with the code. This would then be updated in-step and could trigger a re-build, via Jenkins, of the Docker image.</p>

<h2>Installed Gems Optimization</h2>

<p>Earlier I said that I cannot install code dependencies (gem dependencies), since they may change from one version of the code to the next, but you may have noticed that I have pre-installed them anyway, via &ldquo;bundle install&rdquo;.</p>

<p>As an optimization, I assume that most of the gems will rarely change. I will still install them just prior to running the tests, via another &ldquo;bundle install&rdquo;, so some will become redundant over time. But since most, if not all, will already be there, the &ldquo;bundle install&rdquo; at test run time will be fast.</p>

<p>Luckily, I am using Jenkins to build the Docker image, probably once a night, so any installed gems that become redundant will not be around for long.</p>

<p>You may think this adds an extra variable in the test run, so this can be skipped for purity at the cost of longer time for each test run.</p>

<h2>Docker With Jenkins</h2>

<p>Very little was needed to getting Docker working with Jenkins. I just needed to ensure that the unix user &ldquo;jenkins&rdquo; belonged to the &ldquo;docker&rdquo; group.</p>

<p>Docker runs as the &ldquo;root&rdquo; user and the &ldquo;docker&rdquo; group. When the docker daemon starts up it creates a unix socket owned by the &ldquo;root&rdquo; user and the &ldquo;docker&rdquo; group. Therefore, the docker command-line client needs to be run via &ldquo;root&rdquo; user or someone in the &ldquo;docker&rdquo; group.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ls -l /var/run/docker.sock
</span><span class='line'>srw-rw---- 1 root docker 0 Dec 27 09:45 /var/run/docker.sock</span></code></pre></td></tr></table></div></figure>


<p>Simply add the jenkins user to the docker group to be able to create and run Docker images without sudo.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo usermod -a -G docker jenkins</span></code></pre></td></tr></table></div></figure>


<p>Please consider any security concerns with doing this. I am doing this in a trusted environment.</p>

<h2>Local Docker Registry</h2>

<p>Docker images can get quite large, so it is useful to have a local version of the Docker registry on the same network, or same machine, as you are running Docker. I am going to be running it on the same machine that I am running Jenkins on.</p>

<p>I do not have to worry about the volatility of where I put the repository, as the built Docker images are disposable. As long as I put my Dockerfile somewhere safe (GitHub?), then I can recreate the Docker image anywhere at any time.</p>

<p>Luckily the Docker registry is very simple to setup. It is just a Docker image itself, found on the <a href="http://index.docker.io">http://index.docker.io</a> Docker registry. Yes, things start getting very &ldquo;Inception&rdquo; quickly.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker run -p 5000:5000 samalba/docker-registry</span></code></pre></td></tr></table></div></figure>


<p>Note, if you do not belong to the &ldquo;docker&rdquo; group, you will have to run this as sudo. I added myself to the “docker” group as follows&hellip;</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo usermod -a -G docker phil</span></code></pre></td></tr></table></div></figure>


<p>The &ldquo;-p 5000:5000&rdquo; specifies that the docker-registry process should listen on the port 5000 internally in the Docker container and Docker should map that to port 5000 on the host machine.</p>

<p>We can check it is running by using the &ldquo;docker ps&rdquo; command&hellip;</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker ps
</span><span class='line'>CONTAINER ID   IMAGE                            COMMAND                CREATED             STATUS              PORTS                    NAMES
</span><span class='line'>81bbfc81f7f9   samalba/docker-registry:latest   /bin/sh -c cd /docke   48 seconds ago      Up</span></code></pre></td></tr></table></div></figure>


<h2>Jenkins Job: Build The Docker Image</h2>

<p>Creating a Docker image is quite simple. It requires 3 commands: &ldquo;build&rdquo;, &ldquo;tag&rdquo; and &ldquo;push&rdquo;.</p>

<p>&ldquo;docker build&rdquo;, if successful, will output &ldquo;Successfully built &rdquo;, where &ldquo;&rdquo; is a hex string. You can then use this build-id to &ldquo;docker tag&rdquo; the image with a human-readable name. You then use this image name to &ldquo;docker push&rdquo; it to a Docker registry.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker build &lt;directory containing Dockerfile>
</span><span class='line'>docker tag &lt;built-id> &lt;image-name>
</span><span class='line'>docker push &lt;registry-address>:&lt;image-name></span></code></pre></td></tr></table></div></figure>


<p>Automating this involves extracting the &ldquo;&rdquo; from the &ldquo;docker build&rdquo; output, so I created a small bash script called build_and_push.sh to help with this and manage the whole process of building the Docker image and getting it into the local repository.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#/bin/env bash
</span><span class='line'>
</span><span class='line'># Builds the docker image and pushs to
</span><span class='line'># repository (local by default)
</span><span class='line'>
</span><span class='line'># Usage:
</span><span class='line'>#   build_and_push &lt;directory of Dockerfile> &lt;resultant docker image name>
</span><span class='line'>
</span><span class='line'>DOCKERFILE_DIRECTORY=$1
</span><span class='line'>DOCKER_IMAGE_NAME=$2
</span><span class='line'>
</span><span class='line'>if [ "$DOCKER_REPO_SERVER" = "" ]; then
</span><span class='line'>  DOCKER_REPO_SERVER=localhost:5000
</span><span class='line'>fi
</span><span class='line'>DOCKER_REPO_NAME=$DOCKER_REPO_SERVER/$DOCKER_IMAGE_NAME
</span><span class='line'>
</span><span class='line'># Build docker image
</span><span class='line'>rm -f docker-built-id
</span><span class='line'>docker build $DOCKERFILE_DIRECTORY \
</span><span class='line'>  | perl -pe '/Successfully built (\S+)/ && `echo -n $1 > docker-built-id`'
</span><span class='line'>if [ ! -f docker-built-id ]; then
</span><span class='line'>  echo "No docker-built-id file found"
</span><span class='line'>  exit 1
</span><span class='line'>fi
</span><span class='line'>DOCKER_BUILD_ID=`cat docker-built-id`
</span><span class='line'>rm -f docker-built-id
</span><span class='line'>
</span><span class='line'># Publish built docker image to repo
</span><span class='line'>docker tag $DOCKER_BUILD_ID $DOCKER_REPO_NAME
</span><span class='line'>docker push $DOCKER_REPO_NAME</span></code></pre></td></tr></table></div></figure>


<p>Using this script and my Dockerfile, I now have everything I need to create my first of two Jenkins jobs.</p>

<p>Note, that for simplicity, I have put the Dockerfile and build_and_push.sh script in 2 public gists, which are downloaded at the time of running the Jenkins job.</p>

<p>Name:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cloud_controller_ng rspec docker build</span></code></pre></td></tr></table></div></figure>


<p>Build / Execute shell:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>DOCKERFILE_DIRECTORY=docker
</span><span class='line'>
</span><span class='line'># Fetch Dockerfile
</span><span class='line'>mkdir -p $DOCKERFILE_DIRECTORY
</span><span class='line'>wget https://gist.github.com/philwhln/8195797/raw/Dockerfile --directory-prefix=$DOCKERFILE_DIRECTORY
</span><span class='line'>
</span><span class='line'># Fetch build_and_push script
</span><span class='line'>wget https://gist.github.com/philwhln/8196116/raw/build_and_push.sh
</span><span class='line'>chmod +x build_and_push.sh
</span><span class='line'>
</span><span class='line'># Build the Docker image
</span><span class='line'>DOCKER_REPO_SERVER=localhost:5000 ./build_and_push.sh $DOCKERFILE_DIRECTORY cloud_controller_ng_rspec</span></code></pre></td></tr></table></div></figure>


<p>Build Triggers / Build periodically / Schedule :</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>15 3 * * *</span></code></pre></td></tr></table></div></figure>


<p>This will be run every day at 3:15am, so the next day tests will be run with a fresh docker image.</p>

<h2>Jenkins Job: Run The Docker Image</h2>

<p>Now that we have a Docker image primed and ready to run our Jenkins job, we just need to run it.</p>

<p>Name:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cloud_controller_ng rspec docker run</span></code></pre></td></tr></table></div></figure>


<p>Build / Execute shell:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker run localhost:5000/cloud_controller_ng_rspec</span></code></pre></td></tr></table></div></figure>


<p>The command is quite simple. &ldquo;docker run&rdquo; will checkout the latest &ldquo;cloudcontrollerng_rspec&rdquo; Docker image from our local Docker repository and run it. At this point the &ldquo;CMD&rdquo;, found in the Dockerfile, will be run.</p>

<p>To recap, that line looks like this&hellip;</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># Command to run at "docker run ..."
</span><span class='line'>CMD if [ -z $BRANCH ]; then BRANCH=master; fi; \
</span><span class='line'>    cd /cloud_controller_ng \
</span><span class='line'>    && git checkout $BRANCH \
</span><span class='line'>    && git pull \
</span><span class='line'>    && git submodule init && git submodule update \
</span><span class='line'>    && bundle install \
</span><span class='line'>    && bundle exec rspec spec</span></code></pre></td></tr></table></div></figure>


<p>We checkout the appropriate $BRANCH of cloudcontrollerng.git, if specified (left to the reader to add in Jenkins). It then does a &ldquo;git pull&rdquo; to ensure it has the latest code, then initializes the git submodules, which our project does have.</p>

<p>Then we see the Ruby specific commands, &ldquo;bundle install&rdquo; and finally &ldquo;bundle exec rspec spec&rdquo; to run our test suite.</p>

<p>If you are interested, here is roughly what you will see in the console output of the Jenkins job.</p>

<p>And finally we see&hellip;</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Finished in 121 minutes 1 second
</span><span class='line'>7638 examples, 62 failures, 3 pending</span></code></pre></td></tr></table></div></figure>


<p>&ldquo;docker run&rdquo; returns exit code of 1 (failure), since several tests failed. This causes Jenkins to report to use that the tests are failing.</p>

<p>We can see that this took just over 2 hours to run. Not something that most developers would have much patience for.</p>

<h2>Conclusion</h2>

<p>Since I am using a Dockerfile to specify my test environment, I can be sure that if you follow these steps you will be running the same test suite in an identical environment. It also means that if I hit a problem, I (or anyone else) can replicate it, because I have specified the full stack of my environment. In minutes you can be running it too.</p>

<p>This is a big win for DevOps. Developers can create an initial environment in a Dockerfile, check it into git and the Operations team can then collaborate on it. The Operations team may even send a pull request to the Developers that says, &ldquo;Hey, our production environment does not look like that. Try this instead&hellip;&rdquo;. The updated Dockerfile is then checked out by Jenkins, which builds the new test environment and subsequent test runs are run on a more production-like environment.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nodejs Deployment: Building and Configuring on Amazon Linux AMI]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/04/nodejs-deployment-building-and-configuring-on-amazon-linux-ami/"/>
    <updated>2014-02-04T13:47:50-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/04/nodejs-deployment-building-and-configuring-on-amazon-linux-ami</id>
    <content type="html"><![CDATA[<h2>Logging in and updating system to latest</h2>

<p>SSH your shiny new VM,</p>

<p>Now lets update the system to the latest:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo yum update</span></code></pre></td></tr></table></div></figure>


<h2>Install OS dependencies</h2>

<p>We’r going to build Node.js from sources, some dependencies (such as gcc) are required:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo yum install gcc-c++ make openssl-devel git</span></code></pre></td></tr></table></div></figure>




<!--more-->


<h2>Cloning n Building Node.js</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd
</span><span class='line'>git clone git://github.com/joyent/node.git
</span><span class='line'>cd node
</span><span class='line'>git checkout v0.10.13 #check for other stable tags by executing 'git tag'
</span><span class='line'>./configure --prefix=/usr/local/node
</span><span class='line'>make
</span><span class='line'>sudo make install</span></code></pre></td></tr></table></div></figure>


<h2>Configuration</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo useradd _yourappuser_
</span><span class='line'>passwd _yourappuser_
</span><span class='line'>sudo su - _yourappuser_</span></code></pre></td></tr></table></div></figure>


<h2>Put your app</h2>

<p>Now put your app in ~, for instance:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd
</span><span class='line'>pwd
</span><span class='line'>#/home/_yourappuser_
</span><span class='line'>git clone _https://myhost.com/myapp myapp_</span></code></pre></td></tr></table></div></figure>


<h2>Init.d</h2>

<p>We would like to have nodejs to start automatically as a service, to do so, lets create an init.d file Note: you have to change the properties in the file such as yourappuser, myapp to your app folder and server.js to your node app file.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cat &lt;&lt; 'EOF' > /etc/init.d/nodejs
</span><span class='line'>
</span><span class='line'>#!/bin/sh
</span><span class='line'>
</span><span class='line'>#
</span><span class='line'># chkconfig: 35 99 99
</span><span class='line'># description: Node.js /home/yourappuser/myapp/app.js
</span><span class='line'>#
</span><span class='line'>
</span><span class='line'>. /etc/rc.d/init.d/functions
</span><span class='line'>
</span><span class='line'>USER="_yourappuser_"
</span><span class='line'>
</span><span class='line'>NODE_ENV="production"
</span><span class='line'>DAEMON="/usr/local/node/bin/node"
</span><span class='line'>ROOT_DIR="/home/yourappuser/myapp"
</span><span class='line'>
</span><span class='line'>SERVER="$ROOT_DIR/server.js"
</span><span class='line'>LOG_FILE="$ROOT_DIR/app.js.log"
</span><span class='line'>
</span><span class='line'>LOCK_FILE="/var/lock/subsys/node-server"
</span><span class='line'>
</span><span class='line'>do_start()
</span><span class='line'>{
</span><span class='line'>        if [ ! -f "$LOCK_FILE" ] ; then
</span><span class='line'>                echo -n $"Starting $SERVER: "
</span><span class='line'>                runuser -l "$USER" -c "NODE_ENV=$NODE_ENV $DAEMON $SERVER >> $LOG_FILE &" && echo_success || echo_failure
</span><span class='line'>                RETVAL=$?
</span><span class='line'>                echo
</span><span class='line'>                [ $RETVAL -eq 0 ] && touch $LOCK_FILE
</span><span class='line'>        else
</span><span class='line'>                echo "$SERVER is locked."
</span><span class='line'>                RETVAL=1
</span><span class='line'>        fi
</span><span class='line'>}
</span><span class='line'>do_stop()
</span><span class='line'>{
</span><span class='line'>        echo -n $"Stopping $SERVER: "
</span><span class='line'>        pid=`ps -aefw | grep "$DAEMON $SERVER" | grep -v " grep " | awk '{print $2}'`
</span><span class='line'>        kill -9 $pid > /dev/null 2>&1 && echo_success || echo_failure
</span><span class='line'>        RETVAL=$?
</span><span class='line'>        echo
</span><span class='line'>        [ $RETVAL -eq 0 ] && rm -f $LOCK_FILE
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>case "$1" in
</span><span class='line'>        start)
</span><span class='line'>                do_start
</span><span class='line'>                ;;
</span><span class='line'>        stop)
</span><span class='line'>                do_stop
</span><span class='line'>                ;;
</span><span class='line'>        restart)
</span><span class='line'>                do_stop
</span><span class='line'>                do_start
</span><span class='line'>                ;;
</span><span class='line'>        *)
</span><span class='line'>                echo "Usage: $0 {start|stop|restart}"
</span><span class='line'>                RETVAL=1
</span><span class='line'>esac
</span><span class='line'>
</span><span class='line'>exit $RETVAL
</span><span class='line'>
</span><span class='line'>EOF</span></code></pre></td></tr></table></div></figure>


<p>Add execution permission to the nodejs init script</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo chmod +x /etc/init.d/nodejs</span></code></pre></td></tr></table></div></figure>


<h2>Pre Routing to port 80</h2>

<p>Linux does not allow non super users to listen to ports &lt; 1024, assuming your application listen to port 8080, You would probably like to pre route any traffic arriving from port 80 to your node app that listens to port 8080</p>

<p>You can do this by the pre routing nat capability of Iptables</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>chkconfig iptables on
</span><span class='line'>service iptables start
</span><span class='line'>iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 8080
</span><span class='line'>iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT --to-port 8443 #if you want SSL too
</span><span class='line'>service iptables save</span></code></pre></td></tr></table></div></figure>


<h2></h2>

<p>Configuring node-http-proxy ##</p>

<p>It is common to install http proxies such as nginx on front of nodejs, This architecture has many advantages such as raising security level, listening natively to port 80, load balancing, multiple node apps support via url rewrite, etc…</p>

<p>I personally think that the best approach, which is very native to node apps is to use <a href="https://github.com/nodejitsu/node-http-proxy,">https://github.com/nodejitsu/node-http-proxy,</a></p>

<p>Which have several advantages:</p>

<p>Reverse proxies incoming http.ServerRequest streams, WebSockets, HTTPS
Minimal request overhead and latency
Battled-hardened through production usage
Very native for nodejs apps
TODO: Will post more details in the future but you can simply visit “<a href="https://github.com/nodejitsu/node-http-proxy%E2%80%9D">https://github.com/nodejitsu/node-http-proxy%E2%80%9D</a> site.</p>

<p><a href="https://github.com/pkrumins/nodejs-proxy">https://github.com/pkrumins/nodejs-proxy</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploy/Release Workflow from GitHub]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/04/deploy-slash-release-workflow-from-github/"/>
    <updated>2014-02-04T09:50:50-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/04/deploy-slash-release-workflow-from-github</id>
    <content type="html"><![CDATA[<h1>## Workflow : Deploying/Release Apps from Development to Production  ##</h1>

<p>Deploying is a big part of the lives of most of our Engineering employees. We don&rsquo;t have a release manager and there are no set weekly deploys. Developers and designers are responsible for shipping new stuff themselves as soon as it&rsquo;s ready. This means that deploying needs to be as smooth and safe a process as possible.</p>

<p>The best system we&rsquo;ve found so far to provide this flexibility is to have people deploy branches. Changes never get merged to master until they have been verified to work in production from a branch. This means that master is always stable; a safe point that we can roll back to if there&rsquo;s a problem.</p>

<p>The basic workflow goes like this:</p>

<ul>
<li>Push changes to a branch in GitHub</li>
<li>Wait for the build to pass on our CI server (Jenkins)</li>
<li>Tell Hubot to deploy it</li>
<li>Verify that the changes work and fix any problems that come up</li>
<li>Merge the branch into master
Not too long ago, however, this system wasn&rsquo;t very smart. A branch could accidentally be deployed before the build finished, or even if the build failed. Employees could mistakenly deploy over each other. As the company has grown, we&rsquo;ve needed to add some checks and balances to help us prevent these kinds of mistakes.</li>
</ul>


<h2>Safety First</h2>

<p>The first thing we do now, when someone tries to deploy, is make a call to <a href="https://github.com/github/janky">Janky</a> to determine whether the current CI build is green. If it hasn&rsquo;t finished yet or has failed, we&rsquo;ll tell the deployer to fix the situation and try again.</p>

<p>Next we check whether the application is currently &ldquo;locked&rdquo;. The lock indicates that a particular branch is being deployed in production and that no other deploys of the application should proceed for the moment. Successful builds on the master branch would otherwise get deployed automatically, so we don&rsquo;t want those going out while a branch is being tested. We also don&rsquo;t want another developer to accidentally deploy something while the branch is out.</p>

<p>The last step is to make sure that the branch we&rsquo;re deploying contains the latest commit on master that has made it into production. Once a commit on master has been deployed to production, it should never be “removed” from production by deploying a branch that doesn’t have that commit in it yet.</p>

<p>We use the GitHub API to verify this requirement. An endpoint on the github.com application exposes the SHA1 that is currently running in production. We submit this to the GitHub compare API to obtain the &ldquo;merge base&rdquo;, or the common ancestor, of master and the production SHA1. We can then compare this to the branch that we&rsquo;re attempting to deploy to check that the branch is caught up. By using the common ancestor of master and production, code that only exists on a branch can be removed from production, and changes that have landed on master but haven&rsquo;t been deployed yet won&rsquo;t require branches to merge them in before deploying.</p>

<p>If it turns out the branch is behind, master gets merged into it automatically. We do this using the new :sparkles:Merging API:sparkles: that we&rsquo;re making available today. This merge starts a new CI build like any other push-style event, which starts a deploy when it passes.</p>

<p>At this point the code actually gets deployed to our servers. We usually deploy to all servers for consistency, but a subset of servers can be specified if necessary. This subset can be by functional role — front-end, file server, worker, search, etc. — or we can specify an individual machine by name, e.g, &lsquo;fe7&rsquo;.</p>

<h2>Watch it in action</h2>

<p>What now? It depends on the situation, but as a rule of thumb, small to moderate changes should be observed running correctly in production for at least 15 minutes before they can be considered reasonably stable. During this time we monitor exceptions, performance, tweets, and do any extra verification that might be required. If non-critical tweaks need to be made, changes can be pushed to the branch and will be deployed automatically. In the event that something bad happens, rolling back to master only takes 30 seconds.</p>

<h2>All done!</h2>

<p>If everything goes well, it&rsquo;s time to merge the changes. At GitHub, we use Pull Requests for almost all of our development, so merging typically happens through the pull request page. We detect when the branch gets merged into master and unlock the application. The next deployer can now step up and ship something awesome.</p>

<h1>How do we do it?</h1>

<p>Most of the magic is handled by an internal deployment service called Heaven. At its core, Heaven is a catalog of Capistrano recipes wrapped up in a Sinatra application with a JSON API. Many of our applications are deployed using generic recipes, but more complicated apps can define their own to specify additional deployment steps. Wiring it up to Janky, along with clever use of post-receive hooks and the GitHub API, lets us hack on the niceties over time. Hubot is the central interface to both Janky and Heaven, giving everyone in Campfire great visibility into what’s happening all of the time. As of this writing, 75 individual applications are deployed by Heaven.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenStack : Git Gerrit and Jenkins Workflow]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/03/openstack-git-gerrit-and-jenkins-workflow/"/>
    <updated>2014-02-03T14:54:23-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/03/openstack-git-gerrit-and-jenkins-workflow</id>
    <content type="html"><![CDATA[<h1>Gerrit Workflow</h1>

<p><img src="http://bvajjala.github.io/downloads/code/GerritGitJenkinsWorkflow.png" title="Git Gerrit Jenkins Workflow" alt="Alt text in case picture load fails" /></p>

<h2>Git Account Setup</h2>

<p>You&rsquo;ll need a <a href="https://login.launchpad.net">Launchpad account</a>, since this is how the Web interface for the Gerrit Code Review system will identify you. This is also useful for automatically crediting bug fixes to you when you address them with your code commits.</p>

<p>If you haven&rsquo;t already, <a href="https://www.openstack.org/join/">join The OpenStack Foundation</a> (it&rsquo;s free and required for all code contributors). Among other privileges, this also allows you to vote in elections and run for elected positions within The OpenStack Project. When signing up for Foundation Membership, make sure to give the same E-mail address you&rsquo;ll use for code contributions, since this will need to match your preferred E-mail address in Gerrit.</p>

<p>Visit <a href="https://review.openstack.org/">https://review.openstack.org/</a> and click the Sign In link at the top-right corner of the page. Log in with your Launchpad ID.</p>

<p>Because Gerrit uses Launchpad OpenID single sign-on, you won&rsquo;t need a separate password for Gerrit, and once you log in to one of Launchpad, Gerrit, or Jenkins, you won&rsquo;t have to enter your password for the others.</p>

<p>You&rsquo;ll also want to upload an SSH key while you&rsquo;re at it, so that you&rsquo;ll be able to commit changes for review later.</p>

<p>Ensure that you have run these steps to let git know about your email address:</p>

<figure class='code'><figcaption><span>Git Config </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git config --global user.name "Firstname Lastname"
</span><span class='line'>git config --global user.email "your_email@youremail.com"</span></code></pre></td></tr></table></div></figure>


<p>To check your git configuration:</p>

<figure class='code'><figcaption><span>Git Config </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git config --list</span></code></pre></td></tr></table></div></figure>


<h2>Git Review Installation</h2>

<p>We recommend using the &ldquo;git-review&rdquo; tool which is a git subcommand that handles all the details of working with Gerrit, the code review system used in OpenStack development. Before you start work, make sure you have git-review installed on your system.</p>

<p>On Ubuntu, MacOSx, or most other Unix-like systems, it is as simple as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>pip install git-review</span></code></pre></td></tr></table></div></figure>


<p>On Ubuntu Precise (12.04) and later, git-review is included in the distribution, so install it as any other package:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>apt-get install git-review</span></code></pre></td></tr></table></div></figure>


<p>On Fedora 16 and later, git-review is included into the distribution, so install it as any other package:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install git-review</span></code></pre></td></tr></table></div></figure>


<p>On Fedora 15 and earlier you have to install pip (its package name is <code>python-pip</code>), then install git-review using pip in a conventional way.</p>

<p>On Red Hat Enterprise Linux, you must first enable the EPEL repository, then install it as any other package:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install git-review</span></code></pre></td></tr></table></div></figure>


<p>On openSUSE 12.2 and later, git-review is included in the distribution under the name python-git-review, so install it as any other package:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>zypper in python-git-review</span></code></pre></td></tr></table></div></figure>


<p>All of git-review&rsquo;s interactions with gerrit are sequences of normal git commands. If you want to know more about what it&rsquo;s doing, just add -v to the options and it will print out all of the commands it&rsquo;s running.</p>

<h2>Project Setup</h2>

<p>Clone a project in the usual way, for example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone git://git.openstack.org/openstack/nova.git</span></code></pre></td></tr></table></div></figure>


<p>You may want to ask git-review to configure your project to know about Gerrit at this point. If you don&rsquo;t, it will do so the first time you submit a change for review, but you probably want to do this ahead of time so the Gerrit Change-Id commit hook gets installed. To do so (again, using Nova as an example):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd nova
</span><span class='line'>git review -s</span></code></pre></td></tr></table></div></figure>


<p>Git-review checks that you can log in to gerrit with your ssh key. It assumes that your gerrit/launchpad user name is the same as the current running user. If that doesn&rsquo;t work, it asks for your gerrit/launchpad user name. If you don&rsquo;t remember the user name go to the settings page on gerrit to check it out (it&rsquo;s not your email address).</p>

<p>Note that you can verify the SSH host keys for review.openstack.org here: <a href="https://review.openstack.org/#/settings/ssh-keys">https://review.openstack.org/#/settings/ssh-keys</a></p>

<p>If you get the error &ldquo;We don&rsquo;t know where your gerrit is.&rdquo;, you will need to add a new git remote. The url should be in the error message. Copy that and create the new remote.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git remote add gerrit ssh://&lt;username>@review.openstack.org:29418/openstack/nova.git</span></code></pre></td></tr></table></div></figure>


<p>In the project directory, you have a <code>.git</code> hidden directory and a <code>.gitreview</code> hidden file. You can see them with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ls -la</span></code></pre></td></tr></table></div></figure>


<h2>1.4 Normal Workflow</h2>

<p>Once your local repository is set up as above, you must use the following workflow.</p>

<p>Make sure you have the latest upstream changes:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git remote update
</span><span class='line'>git checkout master
</span><span class='line'>git pull --ff-only origin master</span></code></pre></td></tr></table></div></figure>


<p>Create a topic branch to hold your work and switch to it. If you are working on a blueprint, name your topic branch bp/BLUEPRINT where BLUEPRINT is the name of a blueprint in launchpad (for example, &ldquo;bp/authentication&rdquo;). The general convention when working on bugs is to name the branch bug/BUG-NUMBER (for example, &ldquo;bug/1234567&rdquo;). Otherwise, give it a meaningful name because it will show up as the topic for your change in Gerrit.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git checkout -b TOPIC-BRANCH</span></code></pre></td></tr></table></div></figure>


<p>To generate documentation artifacts, navigate to the directory where the pom.xml file is located for the project and run the following command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mvn clean generate-sources</span></code></pre></td></tr></table></div></figure>


<h3>1.4.1 Committing Changes</h3>

<p>Git commit messages should start with a short 50 character or less summary in a single paragraph. The following paragraph(s) should explain the change in more detail.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>If your changes addresses a blueprint or a bug, be sure to mention them in the commit message using the following syntax:
</span><span class='line'>
</span><span class='line'>Implements: blueprint BLUEPRINT
</span><span class='line'>Closes-Bug: ####### (Partial-Bug or Related-Bug are options)</span></code></pre></td></tr></table></div></figure>


<p>For example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Adds keystone support
</span><span class='line'>
</span><span class='line'>...Long multiline description of the change...
</span><span class='line'>
</span><span class='line'>Implements: blueprint authentication
</span><span class='line'>Closes-Bug: #123456
</span><span class='line'>Change-Id: I4946a16d27f712ae2adf8441ce78e6c0bb0bb657</span></code></pre></td></tr></table></div></figure>


<p>Note that in most cases the Change-Id line should be automatically added by a Gerrit commit hook that you will want to install. See Project Setup for details on configuring your project for Gerrit. If you already made the commit and the Change-Id was not added, do the Gerrit setup step and run:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git commit --amend</span></code></pre></td></tr></table></div></figure>


<p>The commit hook will automatically add the Change-Id when you finish amending the commit message, even if you don&rsquo;t actually make any changes.</p>

<p>Make your changes, commit them, and submit them for review:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git commit -a
</span><span class='line'>git review</span></code></pre></td></tr></table></div></figure>


<p><em>Caution: Do not check in changes on your master branch. Doing so will cause merge commits when you pull new upstream changes, and merge commits will not be accepted by Gerrit.</em></p>

<p>Prior to checking in make sure that you run &ldquo;<a href="http://testrun.org/tox/latest/">tox</a>&rdquo;.</p>

<h3>1.4.2 Review</h3>

<h3>1.4.3 Work in Progress</h3>

<h3>1.4.4 Long-lived Topic Branches</h3>

<h3>1.4.5 Updating a Change</h3>

<h3>1.4.6 Add dependency</h3>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Designing A RESTful API That Doesn't Suck]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/03/designing-a-restful-api-that-doesnt-suck/"/>
    <updated>2014-02-03T14:23:30-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/03/designing-a-restful-api-that-doesnt-suck</id>
    <content type="html"><![CDATA[<h2><a href="http://bvajjala.github.io/blog/2013/03/22/designing-a-restful-api-that-doesn-t-suck.html">Designing A RESTful API That Doesn&rsquo;t Suck</a></h2>

<p>As we&rsquo;re getting closer to shipping the first version of <a href="http://devo.ps">devo.ps</a> and we are joined by a few new team members, the team took the time to review the few principles we followed when designing our RESTful JSON API. A lot of these can be found on <a href="https://blog.apigee.com/taglist/rest_api_design">apigee&rsquo;s blog</a> (a recommended read). Let me give you the gist of it:</p>

<ul>
<li><p><strong>Design your API for developers first</strong>, they are the main users. In that respect, simplicity and intuitivity matter.</p></li>
<li><p><strong>Use <a href="http://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods">HTTP verbs</a></strong> instead of relying on parameters (e.g. <code>?action=create</code>). HTTP verbs map nicely with <a href="http://en.wikipedia.org/wiki/Create,_read,_update_and_delete">CRUD</a>:</p>

<ul>
<li><code>POST</code> for <em>create</em>,</li>
<li><code>GET</code> for <em>read</em>,</li>
<li><code>DELETE</code> for <em>remove</em>,</li>
<li><code>PUT</code> for <em>update</em> (and <code>PATCH</code> too).</li>
</ul>
</li>
<li><p><strong>Use <a href="http://en.wikipedia.org/wiki/List_of_HTTP_status_codes">HTTP status codes</a></strong>, especially for errors (authentication required, error on the server side, incorrect parameters)… There are plenty to choose from, here are a few:</p>

<ul>
<li><code>200</code>: <em>OK</em></li>
<li><code>201</code>: <em>Created</em></li>
<li><code>304</code>: <em>Not Modified</em></li>
<li><code>400</code>: <em>Bad Request</em></li>
<li><code>401</code>: <em>Unauthorized</em></li>
<li><code>403</code>: <em>Forbidden</em></li>
<li><code>404</code>: <em>Not Found</em></li>
<li><code>500</code>: <em>Internal Server Error</em></li>
</ul>
</li>
<li><p><strong>Simple URLs for resources: first a noun for the collection, then the item</strong>. For example <code>/emails</code> and <code>/emails/1234</code>; the former gives you the collection of emails, the second one a specific one identified by its internal id.</p></li>
<li><p><strong>Use verbs for special actions</strong>. For example, <code>/search?q=my+keywords</code>.</p></li>
<li><p><strong>Keep errors simple but verbose (and use HTTP codes)</strong>. We only send something like <code>{ message: "Something terribly wrong happened" }</code> with the proper status code (e.g. <code>401</code> if the call requires authentication) and log more verbose information (origin, error code…) in the backend for debugging and monitoring.</p></li>
</ul>


<p>Relying on HTTP status codes and verbs should already help you keep your API calls and responses lean enough. Less crucial, but still useful:</p>

<ul>
<li><strong>JSON first</strong>, then extend to other formats if needed and if time permits.</li>
<li><strong><a href="http://en.wikipedia.org/wiki/Unix_time">Unix time</a></strong>, or you&rsquo;ll have a bad time.</li>
<li><strong>Prepend your URLs with the API version</strong>, like <code>/v1/emails/1234</code>.</li>
<li><strong>Lowercase everywhere in URLs</strong>.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[I Can Haz Init Script]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/03/i-can-haz-init-script/"/>
    <updated>2014-02-03T14:21:03-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/03/i-can-haz-init-script</id>
    <content type="html"><![CDATA[<h2><a href="http://bvajjala.github.io/blog/2013/06/19/I-can-haz-init-script.html">I Can Haz Init Script</a></h2>

<p>Something went awfully wrong, and a rogue process is eating up all of the resources on one of your servers. You have no other choice but to restart it. No big deal, really; this is the age of disposable infrastructure after all. Except when it comes back up, everything starts going awry. Half the stuff supposed to be running is down and it&rsquo;s screwing with the rest of your setup.</p>

<p><img src="http://bvajjala.github.io/images/posts/y-u-no-guy.png" alt="INIT SCRIPTS, Y U NO LIKE?" /></p>

<p>You don&rsquo;t get to think about them very often, but init scripts are a key piece of a sound, scalable strategy for your infrastructure. It&rsquo;s a <a href="">mandatory best practice</a>. Period. And there are quite a few things in the way of getting them to work properly at scale in production environments. It&rsquo;s a tough world out there.</p>

<h3>What we&rsquo;re dealing with…</h3>

<h4>Packages</h4>

<p>Often enough, you&rsquo;re gonna end up installing a service using the package manager of your distro: <code>yum</code>, <code>apt-get</code>, you name it. These packages usually come with an init script that should get you started.</p>

<p>Sadly, as your architecture grows in complexity, you&rsquo;ll probably run into some walls. Wanna have multiple memcache buckets, or several instances of redis running on the same box? You&rsquo;re out of luck buddy. Time to hack your way
through:</p>

<ul>
<li>Redefine your start logic,</li>
<li>Load one or multiple config files from <code>/etc/defaults</code> or <code>/etc/sysconfig</code>,</li>
<li>Deal with the PIDs, log and lock files,</li>
<li>Implement conditional logic to start/stop/restart one or more of the services,</li>
<li>Realize you&rsquo;ve messed something up,</li>
<li>Same player shoot again.</li>
</ul>


<p>Honestly: PITA.</p>

<h4>Built from source</h4>

<p>First things first: <strong>you shouldn&rsquo;t be building from source</strong> (unless you really, really need to).</p>

<p>Now if you do, you&rsquo;ll have to be thorough: there may be samples of init scripts in there, but you&rsquo;ll have to dig them out. <code>/contrib</code>, <code>/addons</code>, …it&rsquo;s never in the same place.</p>

<p>And that makes things &ldquo;fun&rdquo; when you&rsquo;re <a href="http://devo.ps/blog/2013/03/06/troubleshooting-5minutes-on-a-yet-unknown-box.html">trying to unscrew things on a box</a>:</p>

<ul>
<li>You figured out that MySQL is running from <code>/home/user/src/mysql</code>,</li>
<li>You check if there&rsquo;s an init script: no luck this time…</li>
<li>You try to understand what exactly launched <code>mysqld_safe</code>,</li>
<li>You spend a while digging into the bash history smiling at typos,</li>
<li>You stumble on a <code>run.sh</code> script (uncommented, of course) in the home directory. Funny enough, it seems to be starting everything from MySQL, NGINX and php-fpm to the coffee maker.</li>
<li>You make a mental note to try and track down the &ldquo;genius&rdquo; who did that mess of a job, and get busy with converting everything to a proper init script.</li>
</ul>


<p>Great.</p>

<h3>Why existing solutions suck</h3>

<p>Well, based on what we&rsquo;ve just seen, you really only have two options:</p>

<ol>
<li> <strong>DIY</strong>; but if you&rsquo;re good at what you do, you&rsquo;re probably also lazy. You may do it the first couple times, but that&rsquo;s not gonna scale, especially when dealing with the various flavors of init daemons (upstart, systemd…),</li>
<li> <strong>Use that thing called &ldquo;the Internet&rdquo;</strong>; you read through forum pages, issue queues, gists and if you&rsquo;re lucky you&rsquo;ll find a perfect one (or more likely 10 sucky ones). Kudos to all those of whom shared their work, but you&rsquo;ll probably be back to option 1.</li>
</ol>


<h3>We can do better than this</h3>

<p>You&rsquo;ll find a gazillion websites for pictures of kittens, but as far as I know, there is no authoritative source for init scripts. That&rsquo;s just not right: we have to fix it. A few things I&rsquo;m aiming for:</p>

<ul>
<li><strong>Scalable</strong>; allow for multiple instances of a service to be started at once from different config files (see the memcache/redis example),</li>
<li><strong>Secure</strong>; ensure <code>configtest</code> is run before a restart/reload (because, you know, a faulty config file preventing the service to restart is kind of a bummer),</li>
<li><strong>Smart</strong>; ensuring for example that the cache is aggressively flushed before restarting your database (so that you don&rsquo;t end-up waiting 50 min for the DB to cleanly shutdown).</li>
</ul>


<p><a href="https://github.com/devo-ps/init-scripts">I&rsquo;ve just created a repo</a> where I&rsquo;ll be dumping various init scripts that will hopefully be helpful to others. I&rsquo;d love to get suggestions or help.</p>

<p>And by the way, things are not much better with applications, though we&rsquo;re trying our best to improve things there too with things like <a href="https://github.com/Unitech/pm2">pm2</a> (fresh and shinny, more about it in a later post).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Goodbye node-forever]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/03/goodbye-node-forever/"/>
    <updated>2014-02-03T14:15:10-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/03/goodbye-node-forever</id>
    <content type="html"><![CDATA[<h2><a href="http://bvajjala.github.io/blog/2013/06/26/goodbye-node-forever-hello-pm2.html">Goodbye node-forever, hello PM2</a></h2>

<p><img src="http://apps.hemca.com/pm2/pres/pm22.png" alt="pm2 logo" /></p>

<p>It&rsquo;s no secret that the devo.ps team has a crush on Javascript; node.js in the backend, AngularJS for our clients, there isn&rsquo;t much of our stack that isn&rsquo;t at least in part built with it. Our approach of building <a href="http://devo.ps/blog/2013/01/31/farewell-to-regular-web-development-approaches.html">static clients and RESTful JSON APIs</a> means that we run a lot of node.js and I must admit that, despite all of it awesomeness, node.js still is a bit of a
headache when it comes to running in production. Tooling and best practices (think monitoring, logging, error traces…) are still lacking when compared to some of the more established languages.</p>

<p>So far, we had been relying on the pretty nifty <a href="https://github.com/nodejitsu/forever">node-forever</a>. Great tool, but a few things were missing:</p>

<ul>
<li>Limited monitoring and logging abilities,</li>
<li>Poor support for process management configuration,</li>
<li>No support for clusterization,</li>
<li>Aging codebase (which meant frequent failures when upgrading Node).</li>
</ul>


<p>This is what led us to write <a href="https://github.com/Unitech/pm2">PM2</a> in the past couple months. We thought we&rsquo;d give you a quick look at it while we&rsquo;re nearing a production ready release.</p>

<h3>So what&rsquo;s in the box?</h3>

<p>First things first, you can install it with <code>npm</code>:</p>

<pre><code>npm install -g pm2
</code></pre>

<p>Let&rsquo;s open things up with the usual comparison table:</p>

<p>FeatureForeverPM2</p>

<p>Keep Alive</p>

<p>✔</p>

<p>✔</p>

<p>Coffeescript</p>

<p>✔</p>

<p>Log aggregation</p>

<p>✔</p>

<p>API</p>

<p>✔</p>

<p>Terminal monitoring</p>

<p>✔</p>

<p>Clustering</p>

<p>✔</p>

<p>JSON configuration</p>

<p>✔</p>

<p>And now let me geek a tad more about the main features…</p>

<h3>Native clusterization</h3>

<p>Node v0.6 introduced the cluster feature, allowing you to share a socket across multiple networked Node applications. Problem is, it doesn&rsquo;t work out of the box and requires some tweaking to handle master and children processes.</p>

<p>PM2 handles this natively, without any extra code: PM2 itself will act as the master process and wrap your code into a special clustered process, as Nodejs does, to add some global variables to your files.</p>

<p>To start a clustered app using all the CPUs you just need to type something like that:</p>

<pre><code>$ pm2 start app.js -i max
</code></pre>

<p>Then;</p>

<pre><code>$ pm2 list
</code></pre>

<p>Which should display something like (ASCII UI FTW);</p>

<p><img src="http://apps.hemca.com/pm2/pres/pm2-list.png" alt="pm2 list" /></p>

<p>As you can see, your app is now forked into multiple processes depending on the number of CPUs available.</p>

<h3>Monitoring <em>a la</em> termcaps-HTOP</h3>

<p>It&rsquo;s nice enough to have an overview of the running processes and their status with the <code>pm2 list</code> command. But what about tracking their resources consumption? Fear not:</p>

<pre><code>$ pm2 monit
</code></pre>

<p>You should get the CPU usage and memory consumption by process (and cluster).</p>

<p><img src="http://apps.hemca.com/pm2/pres/pm2-monit.png" alt="pm2 monit" /></p>

<p><strong>Disclaimer</strong>: <a href="https://github.com/arunoda/node-usage">node-usage</a> doesn&rsquo;t support MacOS for now (feel free to PR). It works just fine on Linux though.</p>

<p>Now, what about checking on our clusters and GC cleaning of the memory stack?
Let&rsquo;s consider you already have an HTTP benchmark tool (if not, you should definitely check <a href="https://github.com/wg/wrk">WRK</a>):</p>

<pre><code>$ express bufallo     // Create an express app
$ cd bufallo
$ npm install
$ pm2 start app.js -i max
$ wrk -c 100 -d 100 http://localhost:3000/
</code></pre>

<p>In another terminal, launch the monitoring option:</p>

<pre><code>$ pm2 monit
</code></pre>

<p>W00t!</p>

<h3>Realtime log aggregation</h3>

<p>Now you have to manage multiple clustered processes: one who&rsquo;s crawling data, another who is processing stuff, and so on so forth. That means logs, lots of it. You can still handle it the old fashioned way:</p>

<pre><code>$ tail -f /path/to/log1 /path/to/log2 ...
</code></pre>

<p>But we&rsquo;re nice, so we wrote the <code>logs</code> feature:</p>

<pre><code>$ pm2 logs
</code></pre>

<p><img src="http://apps.hemca.com/pm2/pres/pm2-logs.png" alt="pm2 monit" /></p>

<h3>Resurrection</h3>

<p>So things are nice and dandy, your processes are humming and you need to do a hard restart. What now? Well, first, dump things:</p>

<pre><code>$ pm2 dump
</code></pre>

<p>From there, you should be able to resurrect things from file:</p>

<pre><code>$ pm2 kill     // let's simulate a pm2 stop
$ pm2 resurect // All my processes are now up and running 
</code></pre>

<h3>API Health point</h3>

<p>Let&rsquo;s say you want to monitor all the processes managed by PM2, as well as the status of the machine they run on (and maybe even build a nice Angular app to consume this API…):</p>

<pre><code>$ pm2 web
</code></pre>

<p>Point your browser at <code>http://localhost:9615</code>, aaaaand… done!</p>

<h3>And there&rsquo;s more…</h3>

<ul>
<li>Full tests,</li>
<li>Generation of <code>update-rc.d</code> (<code>pm2 startup</code>), though still very alpha,</li>
<li>Development mode with auto restart on file change (<code>pm2 dev</code>), still very drafty too,</li>
<li>Log flushing,</li>
<li>Management of your applications fleet via JSON file,</li>
<li>Log uncaught exceptions in error logs,</li>
<li>Log of restart count and time,</li>
<li>Automated killing of processes exiting too fast.</li>
</ul>


<h3>What&rsquo;s next?</h3>

<p>Well first, you could show your love on Github (we love stars):
<a href="https://github.com/Unitech/pm2">https://github.com/Unitech/pm2</a>.</p>

<p>We developed PM2 to offer an advanced and complete solution for Node process management. We&rsquo;re looking forward to getting more people helping us getting there: pull requests are more than welcome. A few things already on the
roadmap that we&rsquo;ll get right at once we have a stable core:</p>

<ul>
<li>Remote administration/status checking,</li>
<li>Built-in inter-processes communication channel (message bus),</li>
<li>V8 GC memory leak detection,</li>
<li>Web interface,</li>
<li>Persistent storage for monitoring data,</li>
<li>Email notifications.</li>
</ul>


<p>Special thanks to <a href="https://github.com/makara">Makara Wang</a> for concepts/tools and <a href="https://github.com/rlidwka">Alex Kocharin</a> for advices and pull requests.</p>
]]></content>
  </entry>
  
</feed>
