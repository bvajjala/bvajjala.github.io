
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Balaji Vajjala's Blog</title>
  <meta name="author" content="Balaji Vajjala">

  
  <meta name="description" content="
">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://bvajjala.github.io/blog/page/2">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Balaji Vajjala's Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Fjalla+One' rel='stylesheet' type='text/css'>
  

</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner">
	<div class="header-title"><a href="/">Balaji Vajjala's Blog</a></div>


	<br><div class="header-subtitle">A DevOps Blog from Trenches</div>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:bvajjala.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/home/">Home</a></li> 
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2014/03/14/ci-and-test-automation-utilizing-openstack/">CI and Test Automation Utilizing OpenStack</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2014-03-14T09:54:08-04:00" pubdate data-updated="true">Mar 14<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content">
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2014/03/11/continuous-delivery-tools-list/">Continuous Delivery Tools List</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2014-03-11T12:27:44-04:00" pubdate data-updated="true">Mar 11<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content">
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2014/03/11/tools-in-aws-for-implementing-continuous-deployment-pipeline/">Tools in AWS for Implementing Continuous Deployment Pipeline</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2014-03-11T10:13:38-04:00" pubdate data-updated="true">Mar 11<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content">
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2014/03/11/creating-a-secure-deployment-pipeline-in-aws/">Creating a Secure Deployment Pipeline in AWS</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2014-03-11T10:11:47-04:00" pubdate data-updated="true">Mar 11<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><img src="/downloads/code/opsworks_automation_stack.jpg" alt="" />
<a href="https://aws.amazon.com/opsworks/">Amazon Web Services (AWS) OpsWorks</a> was released one year ago this month. In the past year, we’ve used OpsWorks on several Cloud Delivery projects at <a href="http://stelligent.com/">Stelligent</a> and at some of our customers. This article describes what’s worked for us and our customers. One of our core aims with any customer is to create a fully repeatable process for delivering software. To us, this translates into several more specific objectives. For each process we automate, the process must be fully documented, tested, scripted, versioned and continuous. This article describes how we achieved each of these five objectives in delivering OpsWorks solutions to our customers. In creating any solution, we version any and every asset required to create the software system. With the exception of certain binary packages, the entire software system gets described in code. This includes the application code, configuration, infrastructure and data.</p>

<p>As a note, we’ve developed other AWS solutions without OpsWorks using CloudFormation, Chef, Puppet and some of the other tools mentioned here, but the purpose of this is to describe our approach when using OpsWorks.</p>

<h2>AWS Tools</h2>

<p>AWS has over 30 services and we use a majority of these services when creating deployment pipelines for continuous delivery and automating infrastructure. However, we typically use only a few services directly when building these infrastructure. For instance, when creating infrastructure with OpsWorks, we’ll use the AWS Ruby SDK to provision the OpsWorks resources and CloudFormation for the resources we cannot provision through OpsWorks. We use these three services to access services such as EC2, Route 53, VPC, S3, Elastic Load Balancing, Auto Scaling, etc. These three services are described below.</p>

<h2>AWS OpsWorks</h2>

<p>– OpsWorks is an infrastructure orchestration and event modeling service for provisioning infrastructure resources. It also enables you to call out to Chef cookbooks (more on Chef later). The OpsWorks model logically defines infrastructure in terms of stacks, layers and apps. Within stacks, you can define layers; within layers you can define applications and within applications, you can run deployments. An event model automatically triggers events against these stacks (e.g. Setup, Configure, Deploy, Undeploy, Shutdown). As mentioned, we use the AWS API (through the Ruby SDK) to script the provisioning of all OpsWorks behavior. We never manually make changes to OpsWorks through the console (we make these changes to the versioned AWS API scripts).</p>

<h2>CloudFormation</h2>

<p>– We use CloudFormation to automatically provision resources that we cannot provision directly through OpsWorks. For example, while OpsWorks connects with Virtual Private Clouds (VPC)s and Elastic Load Balancer (ELB)s, you cannot provision VPC or ELB directly through OpsWorks. Since we choose to script all infrastructure provisioning and workflow, we wrote CloudFormation templates for defining VPCs, ELBs, Relational Database Service (RDS) and Elasticache. We orchestrate the workflow in Jenkins so that these resources are automatically provisioned prior to provisioning the OpsWorks stacks. This way, the OpsWorks stacks can consume these resources that were provisioned in the CloudFormation templates. As with any other program, these templates are version-controlled.</p>

<h2>AWS API</h2>

<p>(using Ruby SDK) – We use the AWS Ruby SDK to script the provisioning of OpsWorks stacks. While we avoid using the SDK directly for most other AWS services (because we can use CloudFormation), we chose to use the SDK for scripting OpsWorks because CloudFormation does not currently support OpsWorks. Everything that you might do using the OpsWorks dashboard – creating stacks, JSON configuration, calling out to Chef, deployments – are all written in Ruby programs that utilize the OpsWorks portion of the AWS API.</p>

<h2>Infrastructure Automation</h2>

<p>There are other non-AWS specific tools that we use in automating infrastructure. One of them is the infrastructure automation tool, Chef. Chef Solo is called from OpsWorks. We use infrastructure automation tools to script and as a way to document the process of provisioning infrastructure.</p>

<h2>Chef</h2>

<p>– OpsWorks is designed to run Chef cookbooks (i.e. scripts/programs). Ultimately, Chef is where a bulk of the behavior for provisioning environments is defined – particularly once the EC2 instance is up and running. In Chef, we write recipes (logically stored in cookbooks) to install and configure web servers such as Apache and Nginx or application servers such as Rails and Tomcat. All of these Chef recipes are version-controlled and called from OpsWorks or CloudFormation.</p>

<h2>Ubuntu</h2>

<p> – When using OpsWorks and there’s no specific operating system flavor requirement from our customer, we choose to use Ubuntu 12.04 LTS. We do this for two reasons. The first is that at the time of this writing, OpsWorks supports two Linux flavors: Amazon Linux and Ubuntu 12.04 LTS. The reason we choose Ubuntu is because it allows us to use Vagrant (more on Vagrant later). Vagrant provides us a way to test our Chef infrastructure automation scripts locally – increasing our infrastructure development speed.</p>

<h2>Supporting Tools</h2>

<p>Other supporting tools such as Jenkins, Vagrant and Cucumber help with Continuous Integration, local infrastructure development and testing. Each are described below.</p>

<h2>Jenkins</h2>

<p> – Jenkins is a Continuous Integration server, but we also use it to orchestrate the coarse-grained workflow for the Cloud Delivery system and infrastructure for our customers. We use Jenkins fairly regularly in creating Cloud Delivery solutions for our customers. We configure Jenkins to run Cucumber features, build scripts, automated tests, static analysis, AWS Ruby SDK programs, CloudFormation templates and many more activities. Since Jenkins is an infrastructure component as well, we’ve automated the creation in OpsWorks and Chef and it also runs Cucumber features that we’ve written. These scripts and configuration are stored in Git as well and we can simply type a single command to get the Jenkins environment up and running. Any canonical changes to the Jenkins server are made by modifying the programs or configuration stored in Git.</p>

<h2>Vagrant##</h2>

<p>– Vagrant runs a virtualized environment on your desktop and comes with support for certain OS flavors and environments. As mentioned, we use Vagrant to run and test our infrastructure automation scripts locally to increase the speed of development. In many cases, what might take 30-40 minutes to run the same Chef cookbooks can take 4-5 minutes to run locally in Vagrant – significantly increase our infrastructure development productivity.</p>

<h2>Cucumber</h2>

<p> – We use Cucumber to write infrastructure specifications in code called features. This provides executable documented specifications that get run with each Jenkins build. Before we write any Chef, OpsWorks or CloudFormation code, we write Cucumber features. When completed, these features are run automatically after the Chef, OpsWorks and/or CloudFormation scripts provision the infrastructure to ensure the infrastructure is meeting the specifications described in the features. At first, these features are written without step definitions (i.e. they don’t actually verify behavior against the infrastructure), but then we iterate through a process of writing programs to automate the infrastructure provisioning while adding step definitions and refining the Cucumber features. Once all of this is hooked up to the Jenkins Continuous Integration server, it provisions the infrastructure and then runs the infrastructure tests/features written in Cucumber. Just like writing XUnit tests for the application code, this approach ensures our infrastructure behaves as designed and provides a set of regression tests that are run with every change to any part of the software system. So, Cucumber helps us document the feature as well as automate infrastructure tests. We also write usage and architecture documentation in READMEs, wikis, etc.</p>

<p>A list of the AWS tools we utilized are enumerated below.</p>

<p>We applied the on-demand usability of the cloud with a proven continuous delivery approach to build an automated one click method for building and deploying software into scripted production environments.</p>

<table>
<thead>
<tr>
<th></th>
<th> Tool </th>
<th> What is it? </th>
<th> Our Use</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>  AWS EC2      </td>
<td> Cloud-based virtual hardware instances       </td>
<td> We use EC2 for all of our virtual hardware needs. All instances, from development to production are run on EC2</td>
</tr>
<tr>
<td></td>
<td> AWS S3 </td>
<td> Cloud-based storage </td>
<td> We use S3 as both a binary repository and a place to store successful build artifacts.</td>
</tr>
<tr>
<td></td>
<td> AWS IAM </td>
<td> User-based access to AWS resources </td>
<td> We create users dynamically and use their AWS access and secret access keys so we don’t have to store credentials as properties.</td>
</tr>
<tr>
<td></td>
<td> AWS CloudWatch </td>
<td>System monitoring</td>
<td> Monitors all instances in production. If an instance takes an abnormal amount of strain or shuts down unexpectedly, SNS sends an email to designated parties.</td>
</tr>
<tr>
<td></td>
<td> AWS SNS </td>
<td> Email notifications </td>
<td> When an environment is created or a deployment is run, SNS is used to send notifications to affected parties.</td>
</tr>
<tr>
<td></td>
<td>Cucumber </td>
<td> Acceptance testing </td>
<td> Cucumber is used for testing at almost every step of the way. We use Cucumber to test infrastructure, deployments and application code to ensure correct functionality. Cucumber’s unique english-ess  verbiage allows both technical personnel and customers to communicate using an executable test.</td>
</tr>
<tr>
<td></td>
<td> Liquibase </td>
<td> Automated database change management </td>
<td> Liquibase is used for all database changesets. When a change is necessary within the database, it is made to a liquibase changelog.xml.</td>
</tr>
<tr>
<td></td>
<td>AWS CloudFormation </td>
<td> Templating language for orchestrating all AWS resources </td>
<td> CloudFormation is used for creating a fully working Jenkins environment and Target environment. For instance for the Jenkins environment it creates the EC2 instance with CloudWatch monitoring alarms, associated IAM user, SNS notification topic, everything required for Jenkins to build. This along with Jenkins are the major pieces of the infrastructure.</td>
</tr>
<tr>
<td></td>
<td>AWS SimpleDB </td>
<td> Cloud-based NoSQL database </td>
<td> SimpleDB is used for storing dynamic property configuration and passing properties through the CD Pipeline. As part of the environment creation process, we store multiple values such as IP addresses that we need when deploying the application to the created environment.</td>
</tr>
<tr>
<td></td>
<td>Jenkins </td>
<td> We’re using Jenkins to implement a CD pipeline using the Build Pipeline plugin.</td>
<td> Jenkins runs the CD pipeline which does the building, testing, environment creation and deploying. Since the CD pipeline is also code (i.e. configuration code), we version our Jenkins configuration.</td>
</tr>
<tr>
<td></td>
<td> Capistrano </td>
<td> Deployment automation </td>
<td> Capistrano orchestrates and automates deployments. Capistrano is a Ruby-based deployment DSL that can be used to deploy to multiple platforms including Java, Ruby and PHP. It is called as part of the CD pipeline and deploys to the target environment.</td>
</tr>
<tr>
<td></td>
<td>Puppet </td>
<td> Infrastructure automation </td>
<td> Puppet takes care of the environment provisioning. CloudFormation requests the environment and then calls Puppet to do the dynamic configuration. We configured Puppet to install, configure, and manage the packages, files and services.</td>
</tr>
<tr>
<td></td>
<td> Git </td>
<td> Version control system </td>
<td> Git is the version control repository where every piece of the Manatee infrastructure is stored. This includes the environment scripts such as the Puppet modules, the CloudFormation templates, Capistrano deployment scripts, etc.</td>
</tr>
</tbody>
</table>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2014/03/11/continuous-delivery-patterns/">Continuous Delivery Patterns</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2014-03-11T00:00:00-04:00" pubdate data-updated="true"></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>ABOUT CONTINUOUS DELIVERY</p>

<p>With Continuous Delivery (CD), teams continuously deliver new  versions of software to production by decreasing the cycle time between an idea and usable software through the automation of the entire delivery system: build, deployment, test, and release. CD is enabled through the Deployment Pipeline, which encompasses a collection of patterns described in this Refcard.</p>

<p>CD is concerned with “…how all the moving parts fit together: configuration management, automated testing, continuous integration and deployment, data management, environment management, and release management.” (1)</p>

<p>THE DEPLOYMENT PIPELINE</p>

<p>The purpose of the deployment pipeline is threefold:</p>

<p> • Visibility: All aspects of the delivery system &ndash; building, deploying, testing, and releasing – are visible to all team members promoting collaboration.</p>

<p> • Feedback: Team members learn of problems as soon as they occur so that issues are fixed as soon as possible.</p>

<p> • Continually Deploy: Through a fully automated process, you can deploy and release any version of the software to any environment. (1)</p>

<p>In the Deployment Pipeline diagram above, all of the patterns are shown in context. There are some patterns that span multiple stages of the pipeline, so I chose the stage where it’s most predominately used.</p>

<p>BENEFITS
 • Empowering Teams: Because the deployment pipeline is a pull system, testers, developers, operations, and others can self service the application version into an environment of their choice.</p>

<p> • Reducing Errors: Ensuring the correct version, configuration, database schema, etc. are applied the same way every time through automation.</p>

<p> • Lowering Stress: Through push-button releases to production and Rehearsing Deployments, a release becomes commonplace without the typical stress.</p>

<p> • Deployment Flexibility: Instantiate a new environment or configuration by making a few changes to the automated delivery system.</p>

<p> • Practice makes Perfect: Through the deployment pipeline, the final deployment into production is being rehearsed every single time the software is deployed to any target environments. (1)</p>

<h1>CONFIGURATION MANAGEMENT</h1>

<p>Configuration Management is “the process by which all artifacts relevant to your project, and the relationships between them, are stored, retrieved, uniquely identified, and modified”. (1)</p>

<p>Note: Each pattern is cited with a number in parentheses that corresponds to the source in the References section.</p>

<h2>Configurable Third-Party Software (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Evaluate and use third-party software that can be easily configured, deployed, and automated. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td> Procuring software that cannot be externally configured. Software without an API or command-line interface that forces teams to use the GUI only. |</td>
</tr>
</tbody>
</table>


<h2>Configuration Catalog (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern  </th>
<th>  Maintain a catalog of all options for each application, how to change these options and storage locations for each application. Automatically create this catalog as part of the build process. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td> Configuration options are not documented. The catalog of applications and other assets is “tribal knowledge”. |</td>
</tr>
</tbody>
</table>


<h2>Mainline (3)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern  </th>
<th>  Minimize merging and keep the number of active code lines manageable by developing on a mainline. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td> Multiple branches per project.</td>
</tr>
</tbody>
</table>


<h2>Merge Daily (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Changes committed to the mainline are applied to each branch on at least a daily basis. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Merging every iteration once a week or less often than once a day. |</td>
</tr>
</tbody>
</table>


<h2>Protected Configuration (5) ,(1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Store configuration information in secure remotely accessible locations such as a database, directory, or registry.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Open text passwords and/or single machine or share.</td>
</tr>
</tbody>
</table>


<h2>Repository (3) , (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>All source files &ndash; executable code, configuration, host environment, and data &ndash; are committed to a version-control repository.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Some files are checked in, others, such as environment configuration or data changes, are not. Binaries – that can be recreated through the build and deployment process – are checked in.</td>
</tr>
</tbody>
</table>


<h2>Short-Lived Branches (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Branches must be short lived – ideally less than a few days and never more than an iteration.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Branches that last more than an iteration. Branches by product feature that live past a release.</td>
</tr>
</tbody>
</table>


<h2>Single Command Environment (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Check out the project’s version-control repository and run a single command to build and deploy the application to any accessible environment, including the local development.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Forcing the developer to define and configure environment variables. Making the developer install numerous tools in order for the build/deployment to work.</td>
</tr>
</tbody>
</table>


<h2>Single Path to Production (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Configuration management of the entire system &ndash; source, configuration, environment and data. Any change can be tied back to a single revision in the version-control system.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Parts of system are not versioned. Inability to get back to a previously configured software system.</td>
</tr>
</tbody>
</table>


<h1>CONTINUOUS INTEGRATION (CI)</h1>

<h2>Build Threshold (5)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Fail a build when a project rule is violated – such as architectural breaches, slow tests, and coding standard violations. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Manual code reviews. Learning of code quality issues later in the development cycle.</td>
</tr>
</tbody>
</table>


<h2>Commit Often (6)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Each team member checks in regularly to trunk &ndash; at least once a day but preferably after each task to trigger the CI system.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Source files are committed less frequently than daily due to the number of changes from the developer.</td>
</tr>
</tbody>
</table>


<h2>Continuous Feedback (6)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Send automated feedback from CI system to all Cross-Functional Team members.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Notifications are not sent; notifications are ignored; CI system spams everyone with information they cannot use.</td>
</tr>
</tbody>
</table>


<h2>Continuous Integration (6)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Building and testing software with every change committed to a project’s version control repository.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Scheduled builds, nightly builds, building periodically, building exclusively on developer’s machines, not building at all. </td>
</tr>
</tbody>
</table>


<h2>Stop the Line (5) , (1) , (4), (12)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Fix software delivery errors as soon as they occur; stop the line. No one checks in on a broken build as the fix becomes the highest priority.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Builds stay broken for long periods of time, thus preventing developers from checking out functioning code. </td>
</tr>
</tbody>
</table>


<h2>Independent Build (6)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Write build scripts that are decoupled from IDEs. These build scripts are executed by a CI system so that software is built at every change. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Automated build relies on IDE settings. Builds are unable to be run from the command line.</td>
</tr>
</tbody>
</table>


<h2>Visible Dashboards</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Provide large visible displays that aggregate information from your delivery system to provide high-quality feedback to the Cross-Functional Team in real time.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Email-only alerts or not publicizing the feedback to the entire team.</td>
</tr>
</tbody>
</table>


<h1>TESTING</h1>

<h2>Automate Tests</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Automate the verification and validation of software to include unit, component, capacity, functional, and deployment tests</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Manual testing of units, components, deployment, and other types of tests.</td>
</tr>
</tbody>
</table>


<blockquote><p>Unit- Automating tests without any dependencies.</p>

<p>Component- Automating tests with dependencies to other components and heavyweight dependencies such as the database or file system.</p>

<p>Deployment- Automating tests to verify the deployment and configuration were successful. Sometimes referred to as a “smoke tests”.</p>

<p>Functional- Automating tests to verify the behavior of the software from a user’s perspective.</p>

<p>Capacity- Automating load and performance testing in near- production conditions.</p></blockquote>

<h2>Isolate Test Data (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Use transactions for database-dependent tests (e.g., component tests) and roll back the transaction when done. Use a small subset of data to effectively test behavior </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td> Using a copy of production data for Commit Stage tests. Running tests against a shared database.</td>
</tr>
</tbody>
</table>


<h2>Parallel Tests (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Run multiple tests in parallel across hardware instances to decrease the time in running tests.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Running tests on one machine or instance. Running dependent tests that cannot be run in parallel.</td>
</tr>
</tbody>
</table>


<h2>Stub Systems (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Use stubs to simulate external systems to reduce deployment complexity.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Manually installing and configuring interdependent systems for Commit Stage build and deployment.</td>
</tr>
</tbody>
</table>


<h1>DEPLOYMENT PIPELINE</h1>

<h2>Deployment Pipeline (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> A deployment pipeline is an automated implementation of your application’s build, deploy, test, and release process. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Deployments require human intervention (other than approval or clicking a button). Deployments are not production ready. </td>
</tr>
</tbody>
</table>


<h2>Value-Stream Map (4) ##</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Create a map illustrating the process from check in to the version-control system to the software release to identify process bottlenecks.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Separately defined processes and views of the checkin to release process.</td>
</tr>
</tbody>
</table>


<p>BUILD AND DEPLOYMENT SCRIPTING</p>

<h2>Dependency Management (5)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Centralize all dependent libraries to reduce bloat, classpath problems, and repetition of the same dependent libraries and transitive dependencies from project to project. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Multiple copies of the same binary dependencies in each and every project. Redefining the same information for each project. Classpath hell!</td>
</tr>
</tbody>
</table>


<h2>Common Language (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> As a team, agree upon a common scripting language &ndash; such as Perl, Ruby, or Python &ndash; so that any team member can apply changes to the Single Delivery System </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Each team uses a different language making it difficult for anyone to modify the delivery system reducing cross-functional team effectiveness.</td>
</tr>
</tbody>
</table>


<h2>Externalize Configuration (5)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Changes between environments are captured as configuration information. All variable values are externalized from the application configuration into build/deployment-time properties </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Hardcoding values inside the source code or per target environment.</td>
</tr>
</tbody>
</table>


<h2>Fail Fast (6)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Fail the build as soon as possible. Design scripts so that processes that commonly fail run first. These processes should be run as part of the Commit Stage.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Common build mistakes are not uncovered until late in the deployment process.</td>
</tr>
</tbody>
</table>


<h2>Fast Builds (6)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> The Commit Build provides feedback on common build problems as quickly as possible &ndash; usually in under 10 minutes.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Throwing everything into the commit stage process, such as running every type of automated static analysis tool or running load tests such that feedback is delayed.</td>
</tr>
</tbody>
</table>


<h2>Scripted Deployment (5)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> All deployment processes are written in a script, checked in to the version-control system, and run as part of the Single Delivery System.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Deployment documentation is used instead of automation. Manual deployments or partially manual deployments. Using GUI to perform a deployment.</td>
</tr>
</tbody>
</table>


<h2>Unified Deployment (5)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> The same deployment script is used for each deployment. The Protected Configuration – per environment &ndash; is variable but managed.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td> Different deployment script for each target environment or even for a specific machine. Manual configuration after deployment for each target environment. </td>
</tr>
</tbody>
</table>


<h1>DEPLOYING AND RELEASING APPLICATIONS</h1>

<h2>Binary Integrity (5)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Build your binaries once, while deploying the binaries to multiple target environments, as necessary.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td> Software is built in every stage of the deployment pipeline. </td>
</tr>
</tbody>
</table>


<h2>Canary Release</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Release software to production for a small subset of users (e.g. , 10%) to get feedback prior to a complete rollout. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td> Software is released to all users at once.|</td>
</tr>
</tbody>
</table>


<h2>Blue-Green Deployments (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Deploy software to a non-production environment (call it blue) while production continues to run. Once it’s deployed and “warmed up”, switch production (green) to non-production and blue to green simultaneously. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Production is taken down while the new release is applied to production instance(s).</td>
</tr>
</tbody>
</table>


<h2>Dark Launching (11)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Launch a new application or features when it affects the least amount of users.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Software is deployed regardless of number of active users.</td>
</tr>
</tbody>
</table>


<h2>Rollback Release (5)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Provide an automated single command rollback of changes after an unsuccessful deployment.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Manually undoing changes applied in a recent deployment. Shutting down production instances while changes are undone. </td>
</tr>
</tbody>
</table>


<h2>Self-Service Deployment (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Any Cross-Functional Team member selects the version and environment to deploy the latest working software. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Deployments released to team are at specified intervals by the “Build Team”. Testing can only be performed in a shared state without isolation from others. </td>
</tr>
</tbody>
</table>


<h1>INFRASTRUCTURE AND ENVIRONMENTS</h1>

<h2>Automate Provisioning (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Automate the process of configuring your environment to include networks, external services, and infrastructure.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Configured instances are “works of art” requiring team members to perform partially or fully manual steps to provision them.</td>
</tr>
</tbody>
</table>


<h2>Behavior-Driven Monitoring (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Automate tests to verify the behavior of the infrastructure. Continually run these tests to provide near real-time alerting. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>No real-time alerting or monitoring. System configuration is written without tests.</td>
</tr>
</tbody>
</table>


<h2>Immune Systems</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Deploy software one instance at a time while conducting Behavior-Driven Monitoring. If an error is detected during the incremental deployment, a Rollback Release is initiated to revert changes.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Non-incremental deployments without monitoring.</td>
</tr>
</tbody>
</table>


<h2>Lockdown Environments (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Lock down shared environments from unauthorized external and internal usage, including operations staff. All changes are versioned and applied through automation.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>The “Wild West”: any authorized user can access shared environments and apply manual configuration changes, putting the environment in an unknown state leading to deployment errors.</td>
</tr>
</tbody>
</table>


<h2>Production-Like Environments (1) ##</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Target environments are as similar to production as possible.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Environments are “production like” only weeks or days before a release. Environments are manually configured and controlled.</td>
</tr>
</tbody>
</table>


<h2>Transient Environments</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Utilizing the Automate Provisioning, Scripted Deployment and Scripted Database patterns, any environment should be capable of terminating and launching at will.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Environments are fixed to “DEV, QA” or other pre-determined environments.</td>
</tr>
</tbody>
</table>


<h1>DATA</h1>

<h2>Database Sandbox (7)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Create a lightweight version of your database – using the Isolate Test Data pattern. Each developer uses this lightweight DML to populate his local database sandboxes to expedite test execution.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Shared database. Developers and testers are unable to make data changes without it potentially adversely affecting other team members immediately.</td>
</tr>
</tbody>
</table>


<h2>Decouple Database (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Ensure your application is backward and forward compatible with your database so you can deploy each independently</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Application code data are not capable of being deployed separately.</td>
</tr>
</tbody>
</table>


<h2>Database Upgrade (7)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Use scripts to apply incremental changes in each target environment to a database schema and data. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Manually applying database and data changes in each target environment.</td>
</tr>
</tbody>
</table>


<h2>Scripted Database (7)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Script all database actions as part of the build process. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Using data export/import to apply data changes. Manually applying schema and data changes to the database.</td>
</tr>
</tbody>
</table>


<h1>INCREMENTAL DEVELOPMENT</h1>

<h2>Branch by Abstraction (2)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Instead of using version-control branches, create an abstraction layer that handles both an old and new implementation. Remove the old implementation.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Branching using the version-control system leading to branch proliferation and difficult merging. Feature branching.</td>
</tr>
</tbody>
</table>


<h2>Toggle Features (10)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Deploy new features or services to production but limit access dynamically for testing purposes.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Waiting until a feature is fully complete before committing the source code.</td>
</tr>
</tbody>
</table>


<h1>COLLABORATION</h1>

<h2>Delivery Retrospective (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>For each iteration, hold a retrospective meeting where everybody on the Cross-Functional Team discusses how to improve the delivery process for the next iteration. </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Waiting until an error occurs during a deployment for Dev and Ops to collaborate. Having Dev and Ops work separately.</td>
</tr>
</tbody>
</table>


<h2>Cross-Functional Teams (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th>Everybody is responsible for the delivery process. Any person on the Cross-Functional Team can modify any part of the delivery system.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Siloed teams: Development, Testing, and Operations have their own scripts and processes and are not part of the same team. </td>
</tr>
</tbody>
</table>


<pre><code>Amazon.com has an interesting take on this approach. They call it “You build it, you run it”. Developers take the software they’ve written all the way to production. 
</code></pre>

<h2>Root-Cause Analysis (1)</h2>

<table>
<thead>
<tr>
<th></th>
<th> Pattern </th>
<th> Learn the root cause of a delivery problem by asking “why” of each answer and symptom until discovering the root cause.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Anti-Patterns </td>
<td>Accepting the symptom as the root cause of the problem.</td>
</tr>
</tbody>
</table>


<h1>TOOLS</h1>

<p>This is meant to be an illustrative list, not an exhaustive list, to give you an idea of the types of tools and some of the vendors that help to enable effective Continuous Delivery. The Java, .NET and Ruby platforms are represented. The tools that span categories have been assigned to the most appropriate category or duplicated when necessary.</p>

<table>
<thead>
<tr>
<th></th>
<th> Category </th>
<th> Example Software Tools </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Configuration Management </td>
<td> &lsquo;Subversion (SVN), git, Perforce, PassPack, PasswordSafe, ESCAPE, ConfigGen&rsquo; |</td>
</tr>
<tr>
<td></td>
<td> Continuous Integration </td>
<td> &lsquo;Bamboo, Jenkins, AntHill Pro, Go, TeamCity, TFS 2010, Electric Commander&rsquo; &lsquo;Supporting tools: Doxygen, Grand, GraphViz, JavaDoc, NDoc, SchemaSpy, UmlGraph, CheckStyle, Clover, Cobertura, FindBugs,FxCop, JavaNCSS, JDepend, PMD, Sonar, Simian&rsquo; |</td>
</tr>
<tr>
<td></td>
<td> Testing </td>
<td> Twist , AntUnit, Cucumber, DbUnit, webrat, easyb, Fitnesse, JMeter, JUnit, NBehave, SoapUI, Selenium, RSpec, SauceLabs |</td>
</tr>
<tr>
<td></td>
<td> Deployment Pipeline </td>
<td> Go, AntHill Pro |</td>
</tr>
<tr>
<td></td>
<td> Build and Deployment Scripting </td>
<td> Ant, AntContrib, NAnt, MSBuild, Buildr, Gant, Gradle, make, Maven, Rake, Java Secure Channel, ControlTier, Altiris, Capistrano, Fabric, Func |</td>
</tr>
<tr>
<td></td>
<td> Infrastructure and Environments </td>
<td> AWS EC2, AWS S3, Windows Azure, Google App Engine, AWS Elastic Beanstalk, Heroku, Capistrano, Cobbler, BMC Bladelogic, CFEngine, IBM Tivoli Provisioning Manager, Puppet, Chef, Bcfg2, AWS Cloud Formation, Windows Azure AppFabric, rPath, JEOS, BoxGrinder, CLIP, Eucalyptus, AppLogic, CloudKick, CloudWatch, Nagios, Zabbix, Zenoss |</td>
</tr>
<tr>
<td></td>
<td> Data </td>
<td> Hibernate, MySQL, Liquibase, Oracle, PostgreSQL, SQL Server, SimpleDB, SQL Azure, Ant, MongoDB, dbdeploy |</td>
</tr>
<tr>
<td></td>
<td> Components and Dependencies </td>
<td> Ivy, Archiva, Nexus, Artifactory, Bundler |</td>
</tr>
<tr>
<td></td>
<td> Collaboration </td>
<td> Mingle, Greenhopper, JIRA |</td>
</tr>
</tbody>
</table>


<h1>REFERENCES</h1>

<ol>
<li><p>Jez Humble and David Farley, “Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation”, Addison Wesley Professional, 2010</p></li>
<li><p>Paul Hammant and www.continuousdelivery.com</p></li>
<li><p>Stephen P. Berczuk and Brad Appleton, “Software Configuration Management Patterns.”, Addison Wesley Professional, 2003</p></li>
<li><p>Mary and Tom Poppendieck, “Leading Lean Software Development”, Addison Wesley, 2009</p></li>
<li><p>Paul M. Duvall, “Continuous integration. Patterns and Antipatterns”, DZone refcard #84, 2010 <a href="http://bit.ly/l8rfVS">http://bit.ly/l8rfVS</a></p></li>
<li><p>Paul M. Duvall, “Continuous integration. Improving Software Quality and Reducing Risk”, Addison Wesley, 2007</p></li>
<li><p>Scott W. Ambler and Pramodkumar J. Saladage, “Refactoring Databases. Evolutionary Database Design”, Addison Wesley, 2006.</p></li>
<li><p>Paul M. Duvall, IBM developerWorks series “Automation for the people” <a href="http://ibm.co/iwwvPX">http://ibm.co/iwwvPX</a></p></li>
<li><p>IMVU: <a href="http://bit.ly/jhqP5f">http://bit.ly/jhqP5f</a></p></li>
<li><p>Martin Fowler and Facebook: <a href="http://on.fb.me/miBrOM">http://on.fb.me/miBrOM</a></p></li>
<li><p>Facebook Engineering: <a href="http://on.fb.me/miBrOM">http://on.fb.me/miBrOM</a></p></li>
<li><p>Paul Julius, Enterprise Continuous Integration Maturity Model, <a href="http://bit.ly/m7h5vC">http://bit.ly/m7h5vC</a></p></li>
</ol>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2014/02/22/jenkins-job-builder-and-how-to-extned-it/">Jenkins Job Builder and How to Extned It</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2014-02-22T08:57:36-05:00" pubdate data-updated="true">Feb 22<span>nd</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h1>What is jenkins job builder</h1>

<p>Jenkins job builder is extreme good tool to manage your jenkins CI jobs, it takes simple description from YAML files, and use them to configure jenkins.</p>

<pre><code>#set free style job
#job-template.yml
- job:
    name: testjob
    project-type: freestyle
    defaults: global
    disabled: false
    display-name: 'Fancy job name'
    concurrent: true
    quiet-period: 5
    workspace: /srv/build-area/job-name
    block-downstream: false
    block-upstream: false
</code></pre>

<p>Then put your jenkins access into jenkins.ini file</p>

<pre><code>[jenkins]
user=USERNAME
password=USER_TOKEN
url=JENKINS_URL
ignore_cache=IGNORE_CACHE_FLAG
</code></pre>

<p>Based on the job configuration above, you just need to type command</p>

<pre><code>$ jenkins-jobs --conf jenkins.ini update job-template.yaml 
</code></pre>

<p>Then your job <em>testjob</em> is created in your jenkins server.</p>

<p>The project is created by <a href="https://wiki.openstack.org/wiki/InfraTeam">openstack-infrastructure team</a>, it is used to manage the openstack environment, fairly good.</p>

<h1>How it works</h1>

<p>There is no magic behind it, <em>jenkins-jobs</em> just convert the <em>job-template.yaml</em> to jenkins XML request file, and use jenkins remote API to send create request.</p>

<p>Try to do below to understand this.</p>

<pre><code>$ jenkins-jobs test job-template.yaml -o .
</code></pre>

<p>Then xml file <em>testjob</em> is created, see</p>

<pre><code>&lt;?xml version="1.0" ?&gt;
&lt;project&gt;
  &lt;actions/&gt;
  &lt;description&gt;

&amp;lt;!-- Managed by Jenkins Job Builder --&amp;gt;&lt;/description&gt;
  &lt;keepDependencies&gt;false&lt;/keepDependencies&gt;
  &lt;disabled&gt;false&lt;/disabled&gt;
  &lt;displayName&gt;Fancy job name&lt;/displayName&gt;
  &lt;blockBuildWhenDownstreamBuilding&gt;false&lt;/blockBuildWhenDownstreamBuilding&gt;
  &lt;blockBuildWhenUpstreamBuilding&gt;false&lt;/blockBuildWhenUpstreamBuilding&gt;
  &lt;concurrentBuild&gt;true&lt;/concurrentBuild&gt;
  &lt;customWorkspace&gt;/srv/build-area/job-name&lt;/customWorkspace&gt;
  &lt;quietPeriod&gt;5&lt;/quietPeriod&gt;
  &lt;canRoam&gt;true&lt;/canRoam&gt;
  &lt;properties/&gt;
  &lt;scm class="hudson.scm.NullSCM"/&gt;
  &lt;builders/&gt;
  &lt;publishers/&gt;
  &lt;buildWrappers/&gt;
&lt;/project&gt;
</code></pre>

<p>Now you can use curl command to send the request (testjob) directly !!</p>

<pre><code>$ curl --user USER:PASS -H "Content-Type: text/xml" -s --data "@testjob" "http://jenkins-server/createItem?name=testjob"
</code></pre>

<h2>How to recreate your jenkins job</h2>

<p>Looks great, finally you need think about how to re-create your jenkins job, it is also simple, just download the config.xml</p>

<pre><code>$ curl --user USER:PASS http://jenkins-server/testjob/config.xml
</code></pre>

<p>Or open the configuration page in broswer *<a href="http://jenkins-server/testjob/configure*">http://jenkins-server/testjob/configure*</a> and map from YAML file.</p>

<p>You need to read <a href="http://ci.openstack.org/jenkins-job-builder/configuration.html">jenkins job builder&rsquo;s guideline</a> to know the map, generate it had level Macro like <a href="https://wiki.openstack.org/wiki/InfraTeam">builders</a>, which is connected to the <a href="https://github.com/openstack-infra/jenkins-job-builder/blob/master/jenkins_jobs/modules/builders.py">real python builders module</a> to do transformation from YAML to XML.</p>

<p>What you stated in YAML file like</p>

<pre><code>-job:
  name: test_job
  builders:
- shell: "make test"
</code></pre>

<p>it will be converted to</p>

<pre><code>&lt;builders&gt;
&lt;hudson.tasks.Shell&gt;
  &lt;command&gt;make test&lt;/command&gt;&lt;/hudson.tasks.Shell&gt;
&lt;/builders&gt;
</code></pre>

<h2>How to extend</h2>

<p>Greatly to see jenkins job builder already had lots of default modules to support your normal jenkins jobs, but there is exceptions like some none popular jenkins plugins or your own plugins.</p>

<p>Then it is time to extend the module, the existing document: Extending is not clear enough, I will use example to show how it works, code is in <a href="https://github.com/bv2012/jenkins-buddy">github jenkins-buddy</a> project</p>

<p><a href="https://wiki.jenkins-ci.org/display/JENKINS/ArtifactDeployer+Plugin">ArtifactDeployer</a> Plugin is used as example, this plugin is the popular plugin to deploy the artifacts to other folder.</p>

<p>Artifact Deploy Plugin</p>

<p><img src="../downloads/code/artifactdeploy.png" alt="" /></p>

<p>And I want to have .YAML like below</p>

<pre><code>*#artifactdeploy.yaml*
- job:
name: test-job
publishers:
  - artifactdeployer: 
  includes: 'buddy-*.tar.gz'
  remote: '/project/buddy'
</code></pre>

<h2>write codes to transform</h2>

<p>Now I need to download the existing jobs to see how XML looks like, using curl above, I got it like</p>

<pre><code>&lt;publishers&gt;
   ...  
  &lt;org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerPublisher plugin="artifactdeployer@0.27"&gt;
&lt;entries&gt;
  &lt;org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerEntry&gt;
&lt;includes&gt;buddy-*.tar.gz&lt;/includes&gt;
&lt;basedir&gt;&lt;/basedir&gt;
&lt;excludes&gt;&lt;/excludes&gt;
&lt;remote&gt;/project/buddy&lt;/remote&gt;
&lt;flatten&gt;false&lt;/flatten&gt;
&lt;deleteRemote&gt;false&lt;/deleteRemote&gt;
&lt;deleteRemoteArtifacts&gt;false&lt;/deleteRemoteArtifacts&gt;
&lt;deleteRemoteArtifactsByScript&gt;false&lt;/deleteRemoteArtifactsByScript&gt;
&lt;failNoFilesDeploy&gt;false&lt;/failNoFilesDeploy&gt;
  &lt;/org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerEntry&gt;
&lt;/entries&gt;
&lt;deployEvenBuildFail&gt;false&lt;/deployEvenBuildFail&gt;
  &lt;/org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerPublisher&gt;
..
&lt;/publishers&gt; 
</code></pre>

<p>It belongs the section publishers So I write the jenkins_buddy/modules/publishers.py module to add one function artifactdeployer:</p>

<pre><code>def artifactdeployer(parser, xml_parent, data):
    logger = logging.getLogger("%s:artifactdeployer" % __name__)
    artifactdeployer = XML.SubElement(xml_parent, 'org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerPublisher')
    entries = XML.SubElement(artifactdeployer, 'entries')
    entry = XML.SubElement(entries, 'org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerEntry')
    print data
    XML.SubElement(entry, 'includes').text = data['includes']
    XML.SubElement(entry, 'remote').text = data['remote']
</code></pre>

<p>It is the core part handling convert.</p>

<h3>Hook into jenkins-job builder</h3>

<p>Now you need hook this script into jenkins-jobs builder, thank for the entry_points in python, it can be used for this.</p>

<p>Create the plugin related script and structure, add new entry_point in setup.py</p>

<pre><code>#setup.py in jenkins-buddy
entry_points={
    'jenkins_jobs.publishers': [
    'artifactdeployer=jenkins_buddy.modules.publishers:artifactdeployer',
    ],
}
</code></pre>

<p>it tells jenkins-jobs if you meet new keyword artifactdeployer in publishers, please let me jenkins_buddy.modules.publishers:artifactdeployer to handle.</p>

<h3>Verify it</h3>

<p>Build the pip package local and install it</p>

<pre><code>$ python setup.py sdist
$ pip install dist/jenkins-buddy-0.0.5.zip
</code></pre>

<p>And verify the new job, Bingo, it works.</p>

<pre><code>$ jenkins-jobs test artifactdeploy.yaml -o . 
</code></pre>

<h3>###Make it more complete by checking jenkins plugin java code</h3>

<p>Maybe you noticed, it is hack solution, since I skipped some parameter converting and guess what the XML will look like, if you want to make it more complete, we need to check the java codes directly.</p>

<p>src/main/java/org/jenkinsci/plugins/artifactdeployer/ArtifactDeployerPublisher.java is the class we need to take care.</p>

<pre><code>@DataBoundConstructor
public ArtifactDeployerPublisher(List&lt;ArtifactDeployerEntry&gt; deployedArtifact, boolean deployEvenBuildFail) {
    this.entries = deployedArtifact;
    this.deployEvenBuildFail = deployEvenBuildFail;
    if (this.entries == null)
    this.entries = Collections.emptyList();
}
</code></pre>

<p>It is directly mapping from XML into internal data, if you need know more, learn how to develop jenkins plugin.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2014/02/21/continuous-delivery-with-docker-and-jenkins-part-ii/">Continuous Delivery With Docker and Jenkins - Part II</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2014-02-21T14:41:15-05:00" pubdate data-updated="true">Feb 21<span>st</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>A few weeks ago I started talking about how we use <a href="2014-02-21-continuous-delivery-with-docker-and-jenkins-part-i">Docker and Jenkins for Continuous Delivery</a> in our staging environment. Today, we are open-sourcing a simple bash utility for managing inter-container dependencies, <a href="https://github.com/bv2012/dockerize">Dockerize</a>.</p>

<p>Before I go into specifics, I want to describe our workflow with Jenkins and Docker from a high-level perspective.</p>

<ul>
<li><p>let’s take the <a href="https://github.com/bv2012/hi_sinatra-docker">hi_sinatra</a> Ruby example app. It has its own GitHub repository and we have a simple, non-git Jenkins job for it.</p></li>
<li><p>every commit pushed to GitHub, regardless of the branch, triggers a Jenkins build (via Amazon SQS). All Jenkins builds will result in a Docker image. A successful build will produce a running Docker container. A failed build will produce a stopped container which can be investigated by either looking at the logs or starting it with a tty attached.</p></li>
<li><p>if Docker doesn’t have a <strong>hi_sinatra:master</strong> pre-built image, a new one will be created from the master branch. This master image gets re-built every time there’s a commit against the master branch. Having a master image speeds up image builds considerably (eg. installing Ruby gems, installing node modules, C extensions etc). The resulting image won’t use any caching and all intermediary images will be removed. Just to clarify, this image will not be shipped into production.</p></li>
<li><p>if a Docker image with that app’s name, branch name and git commit sha doesn’t exist, we want Docker to build it for us. At this point, we’re interested to have the eg. <strong>hi_sinatra:second-blog-post.a8e8e83 </strong>Docker image available.</p></li>
<li><p>before a new container can be started from the image that we’ve just built, all services that the app requires must be running in their own independent containers. Our <strong>hi_sinatra</strong> example app requires a running Redis server.</p></li>
<li><p>when all dependent services are running in their own containers, we start a container from the newly built app image (in our example, <strong>hi_sinatra:second-blog-post.a8e8e83</strong>). All dependent containers will have their IPs exposed via env options, eg. docker run -e REDIS_HOST=172.17.0.8 -d &hellip;</p></li>
<li><p>before our <strong>hi_sinatra app</strong> starts in its new Docker container, all tests must pass both unit, integration and acceptance. Full stack tests (also known as acceptance tests) use sandbox services, but they are setup via the same Docker containers that will be made available in production. Code portability is Docker’s strongest point, we’re making full use of it.</p></li>
<li><p>if everything worked as expected, including interactions with all external services, this Docker image will be tagged as production. The service responsible for bringing up new Docker containers from the latest production images will take it from here.</p></li>
</ul>


<p>Docker containers running on the CI are available only on our office network, anyone inside it can connect to them. All that it takes to get an instance for a specific app (and all its dependencies) is to push a new branch to GitHub.</p>

<h2>Dockerize</h2>

<p>Dockerize acts as a Docker proxy, meaning that all commands which it does not understand get forwarded to the docker binary. Dockerize has just 2 dependencies: bash &amp; git.</p>

<p>The previously described workflow as a single shell command:</p>

<pre><code>dockerize boot cambridge-healthcare/hi_sinatra-docker hi_sinatra
</code></pre>

<p>The hi_sinatra app comes with 2 files that Dockerize picks up on:</p>

<ul>
<li><p>dockerize.containers which defines dependencies on other containers (another service such as Redis server or another app)</p></li>
<li><p>dockerize.envs which will forward specific environment variables from the Docker host into the container</p></li>
</ul>


<p>The Vagrantfile that comes with hi_sinatra will get you up and running with Docker, Jenkins and now Dockerize. The quickest way to try the whole setup (<a href="2014-02-21-continuous-delivery-with-docker-and-jenkins-part-i">provided you have Vagrant installed</a>):</p>

<pre><code>git clone https://github.com/cambridge-healthcare/hi_sinatra-docker.git
cd hi_sinatra-docker
vagrant up
</code></pre>

<p>By the time the VM gets provisioned, there will be a running version of <strong>hi_sinatra</strong> inside a Docker container using a Redis server running in a separate container for tracking requests. Use the IP address and port displayed at the end of the Vagrant run to access the hi_sinatra app in your browser.</p>

<h2>Jenkins + Dockerize</h2>

<p>Dockerize makes Jenkins integration with Docker incredibly simple. In the Jenkins instance running on the Vagrant VM that we have just built, add the following job through the Jenkins web interface:</p>

<p>| Job name | hi_sinatra  |
| Job type | Build a free-style software project |
| Build| Execute shell   |</p>

<p>This is the shell command which you will need to use for the build execution:</p>

<pre><code>/bin/bash -c "source $HOME/.profile &amp;&amp; dockerize boot cambridge-healthcare/hi_sinatra-docker hi_sinatra"
</code></pre>

<p>Every successful Jenkins build will now result in a running Docker container.</p>

<p>CI setups are always opinionated. We have a few more additions such as Campfire notifications, Amazon SQS integration with GitHub and a few others which are specific to our infrastructure. The above Jenkins integration example with Docker is meant to be a most conservative starting point for your own setup.</p>

<p>Until next time!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2014/02/21/continuous-delivery-with-docker-and-jenkins-part-i/">Continuous Delivery With Docker and Jenkins - Part I</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2014-02-21T14:40:59-05:00" pubdate data-updated="true">Feb 21<span>st</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>We have been using Docker in our staging environment for nealrt several months now and are right now planning to make it part of our production setup once the first stable version gets released. We’ll be discussing the staging environment setup today with the promise of following up on the production environment at a later date.</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2014/02/21/continuous-delivery-with-docker-and-jenkins-part-i/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2014/02/17/using-docker-to-run-ruby-rspec-ci-in-jenkins/">Using Docker to Run Ruby Rspec CI in Jenkins</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2014-02-17T17:01:40-05:00" pubdate data-updated="true">Feb 17<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In this post, I am going to give a step-by-step introduction into how you can do continuous integration testing with Docker. I will be running the rspec test suite of the CloudFoundry project&rsquo;s Cloud Controller component, although the same process can be applied to any Ruby project. I will show how to build Docker images to easily run repeatable tests and how to set-up Jenkins to do it for you in an automated manner.</p>

<h1>Continuous Integration Using Docker</h1>

<p>The goal of this post is to show Jenkins running a project’s test-suite using Docker. This will occur following every code check-in or every N minutes or whenever it is needed.</p>

<p>Why use Docker to do this? Having a clean environment to run tests is one of the ten commandments of running tests. With Docker&rsquo;s Dockerfile, you can specify a series of steps to create the full stack of the test environment you need. Docker can follow the steps to pre-build the test environment, then stash that environment for disposable re-use. Since a running Docker image, or [LXC] &ldquo;container&rdquo;, is ephemeral, you can blow it away and re-create it very quickly. Perfect for continuous integration!</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2014/02/17/using-docker-to-run-ruby-rspec-ci-in-jenkins/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2014/02/04/nodejs-deployment-building-and-configuring-on-amazon-linux-ami/">Nodejs Deployment: Building and Configuring on Amazon Linux AMI</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2014-02-04T13:47:50-05:00" pubdate data-updated="true">Feb 4<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>Logging in and updating system to latest</h2>

<p>SSH your shiny new VM,</p>

<p>Now lets update the system to the latest:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo yum update</span></code></pre></td></tr></table></div></figure>


<h2>Install OS dependencies</h2>

<p>We’r going to build Node.js from sources, some dependencies (such as gcc) are required:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo yum install gcc-c++ make openssl-devel git</span></code></pre></td></tr></table></div></figure>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2014/02/04/nodejs-deployment-building-and-configuring-on-amazon-linux-ami/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/3/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>About Me</h1>
  <p>A little something about me.</p>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/04/04/a-case-study-on-using-100-percent-cloud-based-resources-with-automated-software-delivery/">A case study on using 100% cloud based Resources with Automated Software Delivery</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/03/25/continuous-delivery-implementation-laying-the-foundations-of-a-continuous-delivery-pipeline/">Continuous Delivery Implementation : Laying the Foundations of a Continuous Delivery Pipeline</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/03/25/continuous-delivery-implementation-getting-started-with-aws/">Continuous Delivery Implementation : Getting started with AWS</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/03/25/autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-4/">Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 4</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/03/25/autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-3/">Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 3</a>
      </li>
    
  </ul>
</section>



<section class="googleplus">
  <h1>
    <a href="https://plus.google.com/bvajjala@gmail.com?rel=author">
      <img src="http://www.google.com/images/icons/ui/gprofile_button-32.png" width="32" height="32">
      Google+
    </a>
  </h1>
</section>



  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  Balaji Vajjala <br/>
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> + <a href="https://github.com/ioveracker/mnml">mnml</a>.
	  
  </span>
</p>

</footer>
  



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
