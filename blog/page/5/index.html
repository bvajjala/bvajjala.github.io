
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Balaji Vajjala's Blog</title>
  <meta name="author" content="Balaji Vajjala">

  
  <meta name="description" content="I&rsquo;ve mentioned a few times that my team has been using Octopus Deploy in a few of my posts.
Now I&rsquo;ll describe in more detail some of the &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://bvajjala.github.io/blog/page/5">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Balaji Vajjala's Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Fjalla+One' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/assets/slides/css/for_blog.css">

  

</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner"><hgroup>
  <h1><a href="/">Balaji Vajjala's Blog</a></h1>
  
    <h2>A DevOps Blog from Trenches</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:bvajjala.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/about/home">Home</a></li> 
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/categories">Categories</a></li>
  <li><a href="/projects">Projects</a></li>
  <li><a href="/about/resume">About Me</a></li>
  <li><a href="/about/consulting">Consulting</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/09/06/how-Angies-Lists-uses-octopus-deploy/">How Angie&#8217;s List Uses Octopus Deploy</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-09-06T11:23:00-04:00" pubdate data-updated="true">Sep 6<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I&rsquo;ve mentioned a few times that my team has been using <a href="http://octopusdeploy.com/">Octopus Deploy</a> in a few of my posts.
Now I&rsquo;ll describe in more detail some of the ways we&rsquo;ve integrated with Octopus that may help others.</p>

<h2>The Before Times</h2>

<p>Octopus started a beta phase in late 2011, so what were we doing before? Since I started working in an asp.net shop
in 2006, I&rsquo;ve been surprised by the lack of robust deployment tools for the ecosystem.</p>

<p>The Web Deployment Projects system provided by Microsoft has always seemed like a non-starter for me. When you
have mature processes like version control, continuous integration, automated testing, and quality assurance,
how does it make sense to hand the keys over to a developer running Visual Studio to click &ldquo;Deploy&rdquo;
on their desktop? What guarantee do you have that the code has been checked in, that it builds, that
the tests pass, that QA signed off?</p>

<p>Even if you only give trusted users permission to deploy, I&rsquo;ve never understood how it makes sense that you
would need Visual Studio to deploy your projects. I mean, Visual Studio is for development. It isn&rsquo;t even
needed by a build server to compile your code. But now QA (or whoever is allowed to perform deployments) needs
to have Visual Studio installed somewhere and checkout the code and build it themselves? So much for automation.
So much for predictability. So much for repeatability.</p>

<p>So it was with a lack of viable alternatives that I started writing a deployment tool in 2007. The tool was
named Bazooka (because it &ldquo;shoots&rdquo; software onto servers), and we used it for 5 years. We never released it as
open source because it made several assumptions particular to us and we didn&rsquo;t take the time to clean it up.</p>

<p>Bazooka was a web application that watched a directory for release candidates to show up as they were
prepared by our build server. Each release candidate contained a deployment descriptor that had some
meta data and a list of components that could be deployed to servers that matched certain roles like
WebServer or AppServer.</p>

<p>Bazooka used xml configuration for everything. The deployment descriptor was xml, the server and environment
configuration was xml, the permissions were xml, and the log files that recorded what had been deployed where
were also xml. This made Bazooka pretty slow as deployment logs piled up, but it beat the heck out of
doing anything more heavy-weight with sql.</p>

<p>Bazooka was, in reality, just a web interface to MSBuild. When you went through the deployment wizard,
Bazooka simply executed MSBuild in the directory specified by each component with some properties saying
which server to deploy to, which target to execute, what the environment was, etc.</p>

<p>We could have easily kept improving Bazooka, but we had actual code to write for our business, so Bazooka
had some weaknesses that we never really addressed. For example, managing servers and environments was
done by hand-editing the xml files. We didn&rsquo;t build a UI for that. Also, it was pretty tough to answer
simple questions like, &ldquo;What was the most recent deployment of my project to the staging environment?&rdquo;</p>

<p>But it worked as a fine stand-in for 5 years until someone built something better.</p>

<h2>Octopus Trial</h2>

<p>It was fun playing with Octopus during the beta period and seeing another approach to a web based
deployment tool. In the beginning there were plenty of things Bazooka had that Octopus didn&rsquo;t (yet),
but development has been pretty rapid with Octopus delivering frequent releases.</p>

<p>The one time where we asked if using Octopus was right was when Paul Stovell, the creator/developer
of Octopus Deploy, took a few months to rewrite the persistence layer, converting it from Entity
Framework to RavenDB. It was the right decision to make and we&rsquo;re happy in the long run, but we
found ourselves in a lurch waiting for some important features and bugfixes. We came out fine on
the other side though.</p>

<p>We also found that using a local file-based NuGet feed with Octopus doesn&rsquo;t scale very well, and
switching to NuGet Server provided no benefit either. This inspired us to create a
<a href="https://github.com/themotleyfool/NuGet">custom fork</a> of NuGet Server that uses
<a href="http://incubator.apache.org/lucene.net/">Lucene.Net</a> and
<a href="https://github.com/themotleyfool/Lucene.Net.Linq">Lucene.Net.Linq</a> to provide a scalable, lightning fast internal feed for Octopus.</p>

<h2>The Switch</h2>

<p>As we integrated some pilot projects with Octopus, we slowly stopped using Bazooka and
eventually turned off Bazooka integration. Some quick stats of our Octopus configuration today:</p>

<ul>
<li>48 projects</li>
<li>18,080 release candidates</li>
<li>1,405 deployments</li>
</ul>


<p>Octopus has done a decent job managing our high demands. We have experienced some slow page
loads here and there, but Paul has been very responsive about troubleshooting and optimizing
these.</p>

<h2>Reusing Deployment Scripts</h2>

<p>Since we already had a highly automated deployment system, we wanted to preserve our exising
capabilities while finding ways to improve the system.</p>

<p>One problem that Octopus doesn&rsquo;t solve for you is how to share deployment scripts across
projects. By default, Octopus will execute deployment scripts contained in each project,
but there isn&rsquo;t a built-in or standard way to reuse common functionality.</p>

<p>One of the first projects we set up in Octopus we named OctopusScripts. This project
consists of a collection of PowerShell modules that we want to be available everywhere.
When we deploy the project, the deployment scripts install the modules into a
standard location where PowerShell will probe for them. Then from other projects,
we can simply start a script with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Import-Module SmokeTest</span></code></pre></td></tr></table></div></figure>


<h2>Reuse Moar</h2>

<p>Moving most of our scripts into PowerShell modules was working great, but we started
to notice that our Deploy.ps1 scripts still looked awfully repetitious.</p>

<p>All of our web projects follow the same basic deployment recipe:</p>

<ol>
<li>PreDeploy

<ol>
<li>Disable machine in load balancer</li>
<li>Create IIS site and app pool definitions if missing</li>
<li>Update site host-header bindings as needed</li>
</ol>
</li>
<li>Let Octopus update the document root to point to the new application</li>
<li>PostDeploy

<ol>
<li>Execute smoke tests against the server and abort if any URLs return a non-200 response</li>
<li>Enable machine in load balancer</li>
</ol>
</li>
</ol>


<p>Of course we have different configurations and different URLs to use for smoke tests
for each project. We ended up creating a configuration based, modular script collection
so that each project simply needs to include a stub:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Import-Module Fool-Octopus
</span><span class='line'>Invoke-OctopusDeploymentTasks</span></code></pre></td></tr></table></div></figure>


<p>The Invoke-OctopusDeploymentTasks function looks at whatever variable and configuration
are present and figure out which steps to execute. The same scripts are used for Windows
Service type projects and others too, and they figure out based on conventions if they
need to run web steps, create services, etc.</p>

<p>If the stub is missing from a project (because why duplicate the stub?) our build scripts
automatically insert a stub Deploy.ps1, PreDeploy.ps1 and PostDeploy.ps1.</p>

<p>We think we&rsquo;re about as DRY as we can get with regards to our deployment scripts.</p>

<h2>Ad-hoc PowerShell</h2>

<p>One thing we&rsquo;d like to see that hasn&rsquo;t made it into Octopus yet is the concept of ad-hoc
powershell scripts. Basically, we want to be able to run some arbitrary scripts during a
deployment, only once. It doesn&rsquo;t need to run once on each machine being deployed to, just
once and then done. There&rsquo;s a <a href="https://trello.com/card/new-deployment-step-ad-hoc-powershell/4e907de70880ba000079b75c/8">story card</a>
on Paul&rsquo;s Trello board that we&rsquo;re looking forward to. In the mean time, we&rsquo;ve been emulating
this behavior by deploying a small package to a dummy server and letting the script run there.</p>

<p>We mostly want this feature to simplify tasks such as sending email/other notifications when
deployments are beginning or completed. It might also be useful for a green/blue style
deployment model where the load balancer needs to toggle just once after the servers
have been updated.</p>

<p>Without the Ad-hoc feature, one of the stumbling points we run into is sometimes forgetting
to check the &ldquo;Force redeployment&rdquo; checkbox that Octopus leaves unchecked. When we forget,
some steps get skipped leading to confusing results.</p>

<h2>Looking Ahead</h2>

<p>Because of the level of automation we integrated with Octopus, our business is able to
deploy software more frequently and more reliably than ever before. In the coming
months and years, we look forward to seeing improvements in Octopus features that will
help us with cloud deployments to AWS or Azure. Octopus has definitely filled a gap
in our deployment capabilities, allowing us to deliver value to our business quickly,
iteratively and predictably.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/08/20/how-Angies-List-uses-nuget-part-2/">How Angie&#8217;s List Uses NuGet (Part 2)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-08-20T13:56:00-04:00" pubdate data-updated="true">Aug 20<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="/blog/2012/08/07/how-the-motley-fool-uses-nuget/">Last time</a>
I talked about how my development team progressed from having all
of our .net code in a single repository with a single solution to using a more
modular architecture complete with encapsulated domains.</p>

<p>When we started using this appraoch, we were still limited in a few ways:</p>

<ul>
<li>Everyone needs to integrate with the newest code</li>
<li>Difficult to patch an old version of a dependency</li>
<li>Cascading failures on the build server</li>
</ul>


<p>Even though we broke the ProjectReference rats nest, we still had an
implicit dependency on various shared code. It all had to be checked out
and built in the right order.</p>

<h2>Binary Package Management</h2>

<p>The next logical step was to further decouple our shared code by
packaging it up and publishing those packages. If we could do that,
we could decide when to upgrade dependencies on a product by product
basis.</p>

<p>There are two package managers in the .net ecosystem: OpenWrap and NuGet.</p>

<p>When we started shopping around, OpenWrap had been around longer and seemed
to be a better choice. There&rsquo;s a <a href="http://stackoverflow.com/questions/4256994/openwrap-vs-nuget">comparison of the products</a>
on stackoverflow.</p>

<p>We worked with OpenWrap for over 6 months and during that time started
to find some problems around integration with Visual Studio and ReSharper.
OpenWrap wants to manage dependencies per solution, and we have many cases
where we want to control dependencies at a per project level. We also
started to notice that NuGet was getting new versions released on a fairly
regular schedule, while OpenWrap 2.0 was in unstable beta limbo for over
a year.</p>

<p>Around the same time we started playing with <a href="http://octopusdeploy.com/">Octopus Deploy</a>
for deploying our code. Since Octopus uses NuGet packages for deployment, we
figured it would make sense to standardize on one package management system
for both deployments and dependency management. It&rsquo;s true that those are
separate problem spaces, but having less build scripts is always a good thing.</p>

<h2>Thoughts on NuGet</h2>

<h3>Conventions</h3>

<p>NuGet has several conventions that make it easy to create simple packages
that others can reference. You can share assemblies and content easily,
and when you want to customize anything there are some powershell extension
points you can hook into.</p>

<p>One problem we run into is that when building packages, sometimes there&rsquo;s
a NuGet convention we want to customize or suppress, and often we can&rsquo;t.</p>

<p>For example, if you create a <code>nuspec</code> and place it adjacent to a <code>csproj</code>
file, NuGet will look at the project and automatically inject metadata
and content into the package. For some things, you can override this
behavior with explicit specifications in the nuspec, but the behavior
can be surprising and confusing.</p>

<h3>Dependency Scoping</h3>

<p>NuGet supports the concept of transitive dependencies&hellip; sort of. If you
install package A, and A depends on package B, NuGet will go find a version
of B and install it while installing A. However, NuGet doesn&rsquo;t do any record
keeping to remember that B is a transitive dependency. To your project, A and
B appear simply as direct dependencies.</p>

<p>There may be cases where A depends on B at runtime, but consumers of A
shouldn&rsquo;t need to code against B at design time.</p>

<p>There may be other cases where B is an optional dependency for A, and
A can be used without it.</p>

<p>Since NuGet doesn&rsquo;t have a concept of scope, it only has one simplistic
approach to dealing with transitive dependencies: treat them just like
direct dependencies.</p>

<h3>Upgrade Behavior</h3>

<p>When you ask NuGet to update a specific package, it will first look for
updates to transitive dependencies that the package depends on. This may
seem obvious or desirable to some, but personally I find it confusing.
You can control this behavior with the <code>-IgnoreDependencies</code> flag in
the Package Management Console, but oddly you don&rsquo;t get that option
in the command line <code>nuget.exe</code> or from the Visual Studio GUI Package
Manager.</p>

<h3>Package Feed Performance</h3>

<p>We use continuous integration, and every successful build produces
&ldquo;release candidate&rdquo; versions of packages. We generate 50 to 100 packages
a day.</p>

<p>Using the simple NuGet UNC share quickly failed to scale, so next we tried
NuGet.Server and found that it doesn&rsquo;t perform well either.</p>

<p>NuGet Gallery seemed like overkill with its SQL Server requirement, so
I started optimizing NuGet.Server. This project ended up taking quite
a while, but the good news is that the fruits of the labor are now
open source on GitHub at <a href="https://github.com/themotleyfool/NuGet">https://github.com/themotleyfool/NuGet</a>.</p>

<p>For more information about that project, see my <a href="/blog/2012/07/03/Speeding-Up-NuGet-Server/">previous post</a>.</p>

<h3>Refactoring Applications and Shared Code</h3>

<p>We try to use <a href="http://semver.org/">Semantic Versioning</a> to communicate
breaking changes in the packages we publish, so sometimes when
we want to use a refactoring tool like Change Method Signature
or Use Base Class it would be nice to have application and shared
code loaded into a single instance of Visual Studio.</p>

<p>We created a tool called <a href="https://github.com/TheMotleyFool/SlimJim">SlimJim</a>
that generates these Solution files on the fly.</p>

<p>If you create a Solution with application code and shared library code,
ReSharper will be smart enough to apply refactoring tools across the projects
even though ProjectReference style references are not being used.</p>

<p>However, Visual Studio won&rsquo;t know the correct order to build projects in,
and won&rsquo;t automatically copy outputs from shared libraries over to applications.</p>

<p>We extended SlimJim to convert assembly references to project references and back
to address this limitation.</p>

<h2>Conclusion</h2>

<p>In terms of capability and maturity, we&rsquo;re in a much better place than
we were a few years ago. However, we still have a ways to go in terms of
productivity and workflow.</p>

<p>NuGet has helped us move in the right direction and we hope to see
further enhancements and even contribute some more of our own as we
develop them.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/08/07/how-Angies-List-uses-nuget/">How Angie&#8217;s List Uses NuGet (Part 1)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-08-07T14:25:00-04:00" pubdate data-updated="true">Aug 7<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>This post describes how we came to using binary package management. In the next part I&rsquo;ll get into NuGet.</p>

<p>In the beginning, there was one repository and it held all the projects for The Motley Fool, and it was good.
There were around a dozen asp.net web projects, a smattering of service and console apps, and a bunch of class libraries
to hold shared code. There was one Solution (sln) to rule them all.</p>

<p>As time went on, we found that there are downsides to the one-giant-solution approach to .net development:</p>

<ul>
<li><a href="http://www.laputan.org/mud/">Big Ball of Mud</a></li>
<li>Slow builds</li>
<li>Tight coupling</li>
<li>Configuration hell</li>
<li>Hard to release different applications on different schedules</li>
</ul>


<p>Typically our larger applications would be split into several projects following a typical N-tier layered architecture:</p>

<ul>
<li>Web</li>
<li>Service</li>
<li>Domain</li>
<li>Data Access</li>
</ul>


<p>Despite our attempts to encapsulate data access and domain logic behind the service project, code ended up leaking out
to the point where domain projects were using types and methods from unrelated domain projects. Cats and dogs were
sleeping together.</p>

<p>Around this time Steven Bohlen presented a <a href="http://unhandled-exceptions.com/blog/index.php/2010/11/27/dc-altnet-presentationthats-a-wrap/">talk</a>
to the Washington DC Alt.NET User Group titled &ldquo;Domain Driven Design Implementation Patterns in .NET&rdquo;. While some of us
were already familiar with concepts of DDD, this talk lit a spark for us to try fixing our big ball of mud.</p>

<p>In late 2010 we started to make some changes. Instead of having one giant repository, shared code would be split out
into separate repositories. We also took this opportunity to introduce a new project organization and architecture.</p>

<p>We established one repository to hold utility code, broken into specific class libraries:</p>

<ul>
<li><tt>Fool.Abstractions</tt> &ndash; similar in spirit to System.Web.Abstractions; adds interfaces and wrappers to various FCL types that lack them</li>
<li><tt>Fool.Lang</tt> &ndash; similar in spirit to <a href="http://commons.apache.org/lang/">Jakarta Commons Lang</a>; adds general utility classes and methods not found elsewhere</li>
<li>Other projects that extend 3rd party class libraries to make them easier for us to work with in standardized ways.</li>
</ul>


<p>Then we established another repository to hold Domain Driven, er, Domains. For example, many of our applications and web sites deal with
stock market data, so one of our business domains is Quotes. In the Quotes Domain we have these projects:</p>

<ul>
<li><tt>Fool.Quotes</tt> &ndash; contains service interfaces and value types; serves as an API to the domain</li>
<li><tt>Fool.Quotes.Core</tt> &ndash; contains domain logic, models, and entities; serves as a private implementation</li>
<li><tt>Fool.Quotes.Web.Api</tt> &ndash; exposes Fool.Quotes interfaces over a RESTful web API</li>
</ul>


<p>The key to keeping our domains distinct and decoupled is to keep Core projects private. While Core is required at runtime,
it should never be referenced at compile time. To bridge the gap, we use Dependency Injection to provide concrete implementations.</p>

<p>Domains may depend on other domains provided that they consume each other through the API project. That way entities and business logic
are kept focused on their own concerns and don&rsquo;t leak out to other problem areas where they don&rsquo;t fit.</p>

<h2>Gluing It Together</h2>

<p>Having projects split into different repositories and different solutions meant that we couldn&rsquo;t simply have one mega Solution
that includes everything. That&rsquo;s by design, so good on us. But this introduces a problem in that we still need to reference code
from our utility projects and DDD projects in our applications. The first solution we came up with to handle this problem was
to use the <a href="http://msdn.microsoft.com/en-us/library/wkze6zky.aspx">AssemblyFolders</a> registry to have our libraries
appear in the <tt>Add Reference</tt> dialog. Then to solve the runtime dependency on our private Core assemblies, we install
those to the GAC so they can be loaded using reflection by our IoC container.</p>

<p>This approach worked fine, mostly. But we encountered some downsides after using it for a while:</p>

<ul>
<li>Need to have all library code checked out and built on each development machine</li>
<li>No built-in way to manage different versions of the same dependency</li>
<li><a href="http://www.sellsbrothers.com/Posts/Details/12503">GAC considered harmful</a></li>
<li>Hard to debug build errors and runtime errors</li>
</ul>


<p>Using Continuous Integration means we&rsquo;re producing new builds dozens of times a day, so it isn&rsquo;t practical for us to
manage different assembly versions for each build. Like most shops, we leave our assembly versions at 1.0.0.0 despite
injecting actual version information into the AssemblyInformationalVersion attribute.</p>

<p>In order to support parallel development, we needed to find a more flexible way of managing dependencies, and
at this point we started to look at binary package management.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/07/03/Speeding-Up-NuGet-Server/">Speeding Up NuGet.Server</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-07-03T00:00:00-04:00" pubdate data-updated="true"></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><em>(Get the source code at <a href="https://github.com/themotleyfool/NuGet">https://github.com/themotleyfool/NuGet</a>)</em></p>

<p>Last time I wrote about creating a LINQ provider for Lucene.Net, and today I&rsquo;ll talk about integrating that provider
with NuGet. The existing server part of the NuGet codebase is a drop-in replacement for using local file-system based
feeds. I wanted to try to preserve that turnkey advantage but improve the performance of various queries.</p>

<p>In order to make sure that my improvements were up to snuff, I set up a private mirror of all packages on <a href="http://nuget.org/">nuget.org</a>,
which turned out to be 44,193 packages at the time, for a total size of over 20 gigs.</p>

<p>If you try hitting ~/api/v2/Packages on stock NuGet.Server, you&rsquo;ll find that your request just spins and spins. And spins. In fact
it took so long that I gave up waiting for the application to initialize. In the background, the server is finding all <code>*.nupkg</code>
files in ~/Packages and calculating a hash of the contents. Needless to say, it can take a while to run a checksum algorithm on 20gb
of data.</p>

<p>Switching over to my custom lucene branch, the first time the site is started, it scans the Packages folder and finds all packages
that haven&rsquo;t been indexed by Lucene. The site homepage helpfully tells you the current status, such as &ldquo;Indexing 2113 of 44193 new packages.&rdquo;
An ajax timer refreshes the info every few seconds so progress can be easily tracked.</p>

<p>The packages don&rsquo;t begin to appear in the feed until they&rsquo;ve all been indexed. So this isn&rsquo;t much better than stock NuGet.Server.</p>

<h2>Incremental Indexing</h2>

<p>The real improvements are appreciated after the initial index is built.</p>

<pre><code>[celdredge@localhost]$ appcmd recycle apppool nuget
"nuget" successfully recycled

[celdredge@localhost]$ time wget -O /dev/null http://localhost/api/v2/Packages

(snip)

real    0m3.230s
user    0m0.062s
sys     0m0.125s
</code></pre>

<p>This means that you don&rsquo;t have to worry much about IIS shutting down the application during idle times. The index gets loaded and ready to go
in a matter of seconds. Vast improvement over stock NuGet.Server.</p>

<p>While that happens, a background thread scans the Packages folder to see what might have changed while the application was stopped. New, modified
and deleted packages are synchronized with the Lucene index. The sycnhronization process takes about 25 seconds to scan 44,193 package files
split into 6,180 folders and calculate the differences with the Lucene index. That&rsquo;s pretty fast.</p>

<p>After the application finishes this initial scan, a FileSystemWatcher monitors the Packages folder to synchronize any changes in real time.
This allows the index to stay in sync when new packages appear, even if they are copied into the folder instead of using <code>nuget push</code>.</p>

<h2>Superfast Search</h2>

<p>All sorts of complex queries are possible, and they execute in very reasonable time. I used <a href="http://www.linqpad.net/">LINQPad</a> to
construct various test queries, like this one that finds packages whose id contain lucene but do not start with lucene:</p>

<pre><code>from p in Packages
where p.Id.Contains("Lucene")
where !p.Id.StartsWith("Lucene")
where p.IsLatestVersion
orderby p.Id descending
select p

Query successful (00:00.136)
</code></pre>

<p>136ms is pretty respectable, IMO.</p>

<p>Another advantage to using Lucene is how queries are analyzed. Term queries will match various word forms, so a query like <tt>build</tt> will
match packages that use any words like build, builds, building, built, etc. It is also possible to search for phrase queries, such as
<tt>&ldquo;glue them back together&rdquo;</tt>. That query matches only one package that contains the exact phrase, whereas on nuget.org you&rsquo;ll get
all kinds of results.</p>

<h2>Other Features</h2>

<p>The <a href="https://github.com/NuGet/NuGetGallery/wiki/Tab-Completion-API-Endpoints">Tab Completion API Endpoints</a> introduced in NuGet 2.0 have
been implemented, bringing fast results to users of the Package Manager Console.</p>

<h2>Conclusion</h2>

<p>It has taken a substantial amount of time and effort to implement Lucene.Net.Linq and integrate it with NuGet.Server, but the results
have proven to be worth the investment.</p>

<p>Lucene.Net.Linq has become a fairly mature, though still nascent, project now available on <a href="http://nuget.org/packages/Lucene.Net.Linq">nuget.org</a>. There are a few other
OSS projects that attempt to do what it does, but I think it is already one of the best.</p>

<p>Binaries of NuGet.Server + Lucene can be downloaded from <a href="https://github.com/themotleyfool/NuGet/downloads">https://github.com/themotleyfool/NuGet/downloads</a>.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/02/29/nuget-aside-for-octopress/">NuGet Aside for Octopress</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-02-29T15:37:00-05:00" pubdate data-updated="true">Feb 29<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I just finished as aside for <a href="http://www.octopress.org">Octopress</a> that list the
Top N downloaded packages where you are an author. It also adds a link to your
NuGet gallery profile if you have one. The style is basically the same as the style of the github aside.</p>

<p>Since there is no official way to publish 3rd party add-ons for Octopress yet, I
created a <a href="https://github.com/kmees/Octopress-NuGet-Aside">github repository</a> with the required files and setup instructions in the <a href="https://github.com/kmees/Octopress-NuGet-Aside/blob/master/README.md">ReadMe</a>.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/07/27/configuration-management-strategies/">Configuration Management Strategies</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-07-27T12:32:00-04:00" pubdate data-updated="true">Jul 27<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I just watched the “To Package or Not to Package” video from DevOps days Mountain View. The discussion was great, and there were some moments of hilarity. If you haven’t watched it yet, check it out <a href=http://goo.gl/KdDyf> here </a></p>

<p>Stephen Nelson Smith, I salute you, sir.</p>

<p>I’m quite firmly in the “Let your CM tool handle your config files” camp. To explain why, I think it’s worth briefly examining the evolution of configuration management strategies.</p>

<p>In order to keep this post as vague and heady as possible, no distinction between “system” and “application” configurations shall be made.</p>

<h2> What is a configuration file? </h2>


<p>Configuration files are text files that control the behavior of programs on a machine. That’s it. They are usually read once, when a program is started from a prompt or init script. A process restart or HUP is typically required for changes to take effect.</p>

<h2> What is configuration management, really? </h2>


<p>When thinking about configuration management, especially across multiple machines, it is easy to equate the task to file management. Configs do live in files, after all. Packages are remarkably good at file management, so it’s natural to want to use them.</p>

<p>However, the task goes well beyond that.</p>

<p>An important attribute of an effective management strategy, config or otherwise, is that it reduces the amount of complexity (aka work) that humans need to deal with. But what is the work that we’re trying to avoid?</p>

<h2> Dependency Analysis and Runtime Configuration </h2>


<p><img class="right" src="http://farm8.staticflickr.com/7239/7389252518_7c27eb1472_n.jpg">
Two tasks that systems administrators concern themselves with doing are dependency analysis and runtime configuration.</p>

<p>Within the context of a single machine, dependency analysis usually concerns software installation. Binaries depend on libraries and scripts depend on binaries. When building things from source, headers and compilers are needed. Keeping the details of all this straight is no small task. Packages capture these relationships in their metadata, the construction of which is painstaking and manual. Modern linux distributions can be described as collections of packages and the metadata that binds them. Go out and hug a package maintainer today.</p>

<p>Within the context of infrastructure architecture, dependency analysis involves stringing together layers of services and making individual software components act in concert. A typical web application might depend on database, caching, and email relay services being available on a network. A VPN or WiFi service might rely on PKI, Radius, LDAP and Kerberos services.</p>

<p>Runtime configuration is the process of taking all the details gathered from dependency analysis and encoding them into the system. Appropriate software needs to be installed, configuration files need to be populated, and kernels need to be tuned. Processes need to be started, and of course, it should all still work after a reboot.</p>

<h2> Manual Configuration </h2>


<p><img class="left" src="http://farm5.staticflickr.com/4139/4805330106_926dfc074f_m.jpg"></p>

<p>Once upon a time, all systems were configured manually. This strategy is the easiest to understand, but the hardest one to execute. It typically happens in development and small production environments where configuration details are small enough to fit into a wiki or spreadsheet. As a network’s size and scope increases, management efforts became massive, time consuming, and prone to human error. Details end up in the heads of a few key people and reproducibility is abysmal. This is obviously unsustainable.</p>

<h2> Scripting </h2>


<p><img class="right" src="http://farm4.staticflickr.com/3101/2428706650_d1fc862fdc_m.jpg">
The natural progression away from manual configuration was custom scripting. Scripting reduced management complexity by automating things using languages like Bash and Perl. Tutorials and documentation instruction like “add the following line to your /etc/sshd_config” were turned into automated scripts that grepped, sed’ed, appended, and clobbered. These scripts were typically very brittle and would only produce desired outcome after their first run.</p>

<h2> File Distribution </h2>


<p><img class="left" src="http://farm5.staticflickr.com/4068/4317655660_61a60f6576_m.jpg">
File distribution was the next logical tactic. In this scheme, master copies of important configuration files are kept in a centralized location and distributed to machines. Distribution is handled in various ways. RDIST, NFS mounts, scp-on-a-for-loop, and rsync pulls are all popular methods.</p>

<p>This is nice for a lot of reasons. Centralization enables version control and reduces the time it takes to make changes across large groups of hosts. Like scripting, file distribution lowers the chance of human error by automating repetitive tasks.</p>

<p>However, these methods have their drawbacks. NFS mounts introduce single points of failure and brittleness. Push based methods miss hosts that happen to be down for maintenance. Pulling via rsync on a cron is better, but lacks the ability to notify services when files change.</p>

<p>Managing configs with packages falls into this category, and is attractive for a number of reasons. Packages can be written to take actions in their post-install sections, creating a way to restart services. It’s also pretty handy to be able to query package managers to see installed versions. However, you still need a way to manage config content, as well as initiate their installation in the first place.</p>

<h2> Declarative Syntax </h2>


<p><img class="right" src="http://farm4.staticflickr.com/3144/2591838509_1fdbc46db7_m.jpg">
In this scheme, autonomous agents run on hosts under management. The word autonomous is important, because it stresses that the <strong>machines manage themselves</strong> by interpreting policy remotely set by administrators. The policy could state any number of things about installed software and configuration files.</p>

<p>Policy written as code is run through an agent, letting the manipulation of packages, configuration files, and services all be handled by the same process. Brittle scripts behaving badly are eliminated by exploiting the idempotent nature of a declarative interface.</p>

<p>When first encountered, this is often perceived as overly complex and confusing by some administrators. I believe this is because they have equated the task of configuration management to file management for such a long time. After the initial learning curve and picking up some tools, management is dramatically simplified by allowing administrators to spend time focusing on policy definition rather than implementation.</p>

<h2> Configuration File Content Management </h2>


<p>This is where things get interesting. We have programs under our command running on every node in an infrastructure, so what should we make them to do concerning configuration files?</p>

<p>“Copy this file from its distribution point” is very common, since it allows for versioning of configuration files. Packaging configs also accomplishes this, and lets you make declarations about dependency. But how are the contents of the files determined?</p>

<p>It’s actually possible to do this by hand. Information can be gathered from wikis, spreadsheets, grey matter, and stick-it notes. Configuration files can then be assembled by engineers, distributed, and manually modified as an infrastructure changes.</p>

<p>File generation is a much better idea. Information about the nodes in an infrastructure can be encoded into a database, then fed into templates by small utility programs that handle various aspects of dependency analysis. When a change is made, such as adding or removing a node from a cluster, configurations concerning themselves with that cluster can be updated with ease.</p>

<h2> Local Configuration Generation </h2>


<p><img class="left" src="http://farm5.staticflickr.com/4140/4753170413_f3488f6586_m.jpg">
The logic that generates configuration files has to be executed somewhere. This is often done on the machine responsible for hosting the file distribution. A better place is directly on the nodes that need the configurations. This eliminates the need for distribution entirely.</p>

<p>Modifications to the node database now end up in all the correct places during the next agent run. Packaging the configs is completely unnecessary, since they don’t need to be moved from anywhere. Management complexity is reduced by eliminating the task entirely. Instead of worrying about file versioning, all that needs to be ensured is code correctness and the accuracy of the database.</p>

<p>Don’t edit config files. Instead, edit the truth.</p>

<p>-s</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/07/19/Subversion-hot-backup-change-in/">Subversion Hot-backup Change in 1.6.11</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-07-19T09:11:00-04:00" pubdate data-updated="true">Jul 19<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><img class="left" src="/assets/images/subversion.jpg" title="'Subversion Logo'" >An important notice to users of the hot-backup.py utility which ships with subversion.</p>

<p>I found our nightly backup of subversion was failing with the following error:</p>

<pre><code>svnadmin: Can't open file '/pathtorepo/db/fsfs.conf': No such file or directory
</code></pre>

<p>What was troubling me was:</p>

<ol>
<li>How come a file is missing in my svn repository &ndash; nothing has changes (As far as I know &hellip; :)</li>
<li>hot-backup.py script hasen&rsquo;t changed much, how come my version has changed ?</li>
</ol>


<p>So I looked up the subversion&rsquo;s change log: <a href="http://svn.apache.org/repos/asf/subversion/tags/1.6.12/CHANGES">http://svn.apache.org/repos/asf/subversion/tags/1.6.12/CHANGES</a> which is the latest release (1.6.12) and took a look on my svn machine to find which version was installed and I found 1.6.12 was indeed in talled and in 1.6.11 release notes you will find:</p>

<pre><code>* make 'svnadmin hotcopy' copy the fsfs config file (r905303)
</code></pre>

<p>In addition I took a look at hot-bakup.py change log under: <a href="http://svn.apache.org/viewvc/subversion/trunk/tools/backup/hot-backup.py.in?view=log,">http://svn.apache.org/viewvc/subversion/trunk/tools/backup/hot-backup.py.in?view=log,</a> and found that indeed the fsfs file has be included in the hotbackup script since the 1.6.11 version of the file (see link speified above).</p>

<p>Googeling on the fsfs.conf subject led me to: <a href="http://comments.gmane.org/gmane.comp.version-control.subversion.user/97647">http://comments.gmane.org/gmane.comp.version-control.subversion.user/97647</a> which noted the same exact issue.</p>

<p><strong>How do we solve this issue</strong><strong>?</strong></p>

<ol>
<li>Create a test repository &ndash; svnadmin create /tmp/svntest which will create the fsfs.conf under /tmp/svntest/db/fsfs.conf</li>
<li>Copy the fsfs.conf to your svnroot/db directory and walla you have the fsfs.conf (what this file does is a different topic)</li>
</ol>


<p>Please note:</p>

<p>1. The svnadmin upgrade &ndash;<strong>Doesn&rsquo;t add this file so unless you are using an old veriosn of the hot-backup.py script your backups will fail!!!</strong> (beleive me I tried).</p>

<p>2. If you update subverison &ndash; don&rsquo;t forget to run <em>svnadmin upgrade /pathto yourrepo/</em> or you miss all the point of upgrading</p>

<p>So I&rsquo;ve learened that subversion was upgraded (again which doesn&rsquo;t say your repository was upgraded!!!) &ndash; but when? &ndash; considering the fact I am running CentOS &ndash; and I didn&rsquo;t have to compile SVN from source and start checking the subversion binaries Creation / Update time I used RPM to tell me when subverions was installed and there is was:</p>

<pre><code>[root@dev ~]# rpm -qi subversion
Name        : subversion                   Relocations: (not relocatable)
Version     : 1.6.12                       Vendor: Dag Apt Repository
Release     : 0.1.el5.rf                   Build Date: Tue 22 Jun 2010 12:55:11 PM IDT
Install Date: Mon 19 Jul 2010 12:36:54 AM IDT      Build Host: lisse.hasselt.wieers.com
Group       : Development/Tools             Source RPM: subversion-1.6.12-0.1.el5.rf.src.rpm
Size        : 21247326                         License: BSD
Signature   : DSA/SHA1, Tue 22 Jun 2010 04:46:18 PM IDT, Key ID a20e52146b8d79e6
Packager    : Dag Wieers 
</code></pre>

<p>I think the <a href="http://search.twitter.com/search?q=%231">#1</a> lesson learned here is before you upgrade read the release notes, see if it impacts you environment in any way &ndash; then you can upgrade.</p>

<p>Hope you find this useful.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/06/20/learning-programming/">Learning Programming</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-06-20T13:46:54-04:00" pubdate data-updated="true">Jun 20<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I was tutoring someone in web app development recently, and the monumental task in front of him really hit me. <strong>He was trying to learn and use nine new languages at the same time</strong>.</p>

<p>In our case it was:</p>

<ul>
<li><p>Ruby</p></li>
<li><p>Rails</p></li>
<li><p>MySQL</p></li>
<li><p>Bash (command line usage)</p></li>
<li><p>HTML</p></li>
<li><p>CSS</p></li>
<li><p>Javascript</p></li>
<li><p>JQuery</p></li>
<li><p>Git</p></li>
<li><p>(Capistrano, Yaml, nginx, ??)</p></li>
</ul>


<p>Even though some of these aren&rsquo;t true languages in the traditional sense, they appear this way to newcomers since they are each a new syntax to learn.</p>

<p>If you slowly built up these skills over 15 years, they are clearly separate concepts in your mind. But for a newcomer trying to use them, it&rsquo;s not even clear which one is which.</p>

<p>Is that a Ruby method or a Rails method?</p>

<p>Is &ldquo;script/server&rdquo; a shell command or is &ldquo;ls&rdquo; part of rails?</p>

<p>Is this file html, js, or css? (actually a mix of all three)</p>

<p>He made a comment along the lines of &ldquo;wouldn&rsquo;t it be great if you could build an entire web app in one language&rdquo;, and I started thinking about it.</p>

<p>GWT (Google Website Toolkit), ActiveRecord, CoffeeScript, and Heroku are all steps in this direction. You could classify them generally as trying to &ldquo;eliminate a language in the stack&rdquo; or allowing you to do a piece of the stack in a language you already know.</p>

<p>Obviously there is a trade off here between power and simplicity, but I&rsquo;m wondering &ndash; would it be possible or desirable to get an entire web app down to just one language? If not that how few could you use?</p>

<p>Btw, I think there are benefits to seasoned developers here as well. I remember Lars Rasmussen (creator of Google Maps and Wave) mentioned something to this effect at Google IO in 2009, that GWT allowed him to spend his mental CPU cycles at a higher level and be more creative (not having to worry about cross browser css or js). So the benefits of higher abstraction may not only be for newcomers.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/06/08/functional-groovy-switch-statement/">Functional Groovy Switch Statement</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-06-08T08:00:00-04:00" pubdate data-updated="true">Jun 8<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In the previous <a href="/2011/06/01/reversing-groovy-switch-statement">post</a> I showed how to replace chained if-else statements in Groovy with one concise switch. It was done for the special case of if-stement where every branch was evaluated using the same condition function. Today I want to make a generalization of that technique by allowing to use different conditionals.</p>

<p>Suppose your code looks like this:</p>

<pre><code>if (param % 2 == 0) {
    'even'
} else if (param % 3 == 0) {
    'threeven'
} else if (0 &lt; param) {
    'positive'
} else {
    'negative'
}
</code></pre>

<p>As long as every condition operates on the same parameter, you can replace the entire chain with a switch. In this scenario <code>param</code> becomes a switch parameter and conditions become <code>case</code> parameters of Closure type. The only thing we need to do is to override <code>Closure.isCase()</code> method as I described in the previous post. The safest way to do it is to create a category class:</p>

<pre><code>class CaseCategory {
    static boolean isCase(Closure casePredicate, Object switchParameter) {
        casePredicate.call switchParameter
    }
}
</code></pre>

<p>Now we can replace if-statement with the following switch:</p>

<pre><code>use (CaseCategory) {
    switch (param) {
        case { it % 2 == 0 } : return 'even'
        case { it % 3 == 0 } : return 'threeven'
        case { 0 &lt; it }      : return 'positive'
        default              : return 'negative'
    }
}
</code></pre>

<p>We can actually go further and extract in-line closures:</p>

<pre><code>def even = {
    it % 2 == 0
}
def threeven = {
    it % 3 == 0
}
def positive = {
    0 &lt; it
}
</code></pre>

<p>After which the code becomes even more readable:</p>

<pre><code>use (CaseCategory) {
    switch (param) {
        case even     : return 'even'
        case threeven : return 'threeven'
        case positive : return 'positive'
        default       : return 'negative'
    }
}
</code></pre>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/06/07/nothing-new-under-sun/">Nothing New Under the Sun</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-06-07T08:00:00-04:00" pubdate data-updated="true">Jun 7<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Every generation of software developers needs its own fad. For my generation it was Agile, for generation before it was OOP, and before that it was another big thing. Gerald Weinberg, one of the most influential people in our industry, blogged yesterday about this issue. With over 50 years of experience in software development he knows what he is talking about. Read his <a href="http://secretsofconsulting.blogspot.com/2011/06/beyond-agile-programming.html">blog post</a> — he has a very good point.</p>

<p>P.S. I&rsquo;m wondering what will be the next big thing. Will it be Cloud or <a href="/2013/01/15/embrace-big-data/">Big Data</a>?</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/6/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/4/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>About Me</h1>
  <p>A little something about me.</p>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/04/21/continuous-delivery-pipeline-stages/">Continuous Delivery Pipeline Stages</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/04/21/implementing-practical-continuous-deployment/">Implementing Practical Continuous Deployment</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/04/21/practical-continuous-deployment/">Practical Continuous Deployment</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/04/17/my-first-presentation/">my first presentation</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/02/22/jenkins-job-builder-and-how-to-extned-it/">Jenkins Job Builder and How to Extned it</a>
      </li>
    
  </ul>
</section>



<section class="googleplus">
  <h1>
    <a href="https://plus.google.com/bvajjala@gmail.com?rel=author">
      <img src="http://www.google.com/images/icons/ui/gprofile_button-32.png" width="32" height="32">
      Google+
    </a>
  </h1>
</section>



  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Balaji Vajjala -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> | Themed with <a href="https://github.com/lucaslew/whitespace">Whitespace</a></span>
</p>

</footer>
  






<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
