<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: CD | Balaji Vajjala's Blog]]></title>
  <link href="http://bvajjala.github.io/blog/categories/cd/atom.xml" rel="self"/>
  <link href="http://bvajjala.github.io/"/>
  <updated>2014-04-16T11:42:54-04:00</updated>
  <id>http://bvajjala.github.io/</id>
  <author>
    <name><![CDATA[Balaji Vajjala]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Continuous Delivery Implementation : Getting started with AWS]]></title>
    <link href="http://bvajjala.github.io/blog/2014/03/25/continuous-delivery-implementation-getting-started-with-aws/"/>
    <updated>2014-03-25T14:57:15-04:00</updated>
    <id>http://bvajjala.github.io/blog/2014/03/25/continuous-delivery-implementation-getting-started-with-aws</id>
    <content type="html"><![CDATA[<ul>
<li>Blog</li>
<li>This article is part of the Continuous Delivery Blueprints series.
It discusses how to go from no cloud infrastructure and no
continuous integration set up to having a functioning Continuous
Delivery Pipeline in Amazon Web Services. It discusses high level
topics while also providing a reference implementation so it’s easy
to follow along with.

<ul>
<li>Turn on CloudTrail</li>
<li>Turn on Programmatic Billing</li>
<li>If CloudTrail and Programmatic Billing are so important, why
aren’t they turned on by default?</li>
<li>Create IAM Users</li>
<li>Comments</li>
<li>Trackbacks</li>
<li>Post a comment</li>
<li>Categories</li>
</ul>
</li>
</ul>


<h1>This article is part of the Continuous Delivery Blueprints series. It discusses how to go from no cloud infrastructure and no continuous integration set up to having a functioning Continuous Delivery Pipeline in Amazon Web Services. It discusses high level topics while also providing a reference implementation so it’s easy to follow along with.</h1>

<p>Everyone is talking about migrating to the cloud these days, and getting
started with Amazon Web Services is super simple to do. However, most
people just rush in, creating headaches for themselves down the road.
There are some best practices you should take at the beginning of your
cloud migration that will make things easier, more secure, and allow you
to scale up and out better.</p>

<p>What we’re going to do today:</p>

<ul>
<li>• Create an AWS Account</li>
<li>• Turn On AWS CloudTrail</li>
<li>• Turn On Programmatic Billing</li>
<li>• Create IAM Users and Groups</li>
<li>• Add MFA for New Users</li>
</ul>


<p><strong>Create your AWS Account</strong></p>

<p>It all starts here: <a href="http://aws.amazon.com/">aws.amazon.com</a>. Find the
big sign up button and just follow the prompts. A couple of things to
note before getting started:</p>

<ol>
<li><ol>
<li>It’ll prompt you for your information (name, email, address, etc)
and credit card info, so you should get that figured out first.</li>
</ol>
</li>
<li><ol>
<li>You’ll need to verify your account via a phone call, so have your
phone handy.</li>
</ol>
</li>
<li><ol>
<li>You don’t need to sign up for support just yet.</li>
</ol>
</li>
</ol>


<p>Once you’re signed up, just login into the AWS console. The console
allows you to interact with most AWS services. Most people will start
building their servers in the sky right away, but there’s a bit of
information you should probably know up front, and some account set up
we recommend before getting started. Let’s go over that first.</p>

<p><strong>What You Need To Know About AWS Before Setting Stuff Up</strong></p>

<p>Amazon Web Services offers a lot of different services, from virtual
computing instances and storage to transcoding and streaming. Going over
each service would take a whole series of blog posts, but an
understanding of how AWS is laid out will be helpful when getting
started.</p>

<p>AWS has data centers all over the world, and has two ways of grouping
them. At global scale there are <strong>regions</strong>, representing parts of or
entire continents. Inside each region are <strong>availability zones</strong>.
Regions are completely distinct entities, and you can only work in one
at a time. Availability zones are designed to talk to each other, and
AWS will automatically spread your resources across availability zones.
Availability zones, however, can only speak to other zones within the
same region.</p>

<p>Choosing a region is important, though these directions are the same
more-or-less in every region. However, be aware that not all services
are available in all regions, and pricing does vary by region. In
addition to that, US-East-1 is the “default” zone when you start with
AWS, and has been around the longest. For that reason, it’s also the
most popular, and sometimes you won’t be able to allocate resources in
certain Availability Zones in the US-East-1 region due to those zones
being at capacity.</p>

<p>AWS provides <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html">lots of
documentation</a>
on how to choose a region, so definitely look through that to decide the
best place to host your infrastructure. If you’re just doing initial
investigation into AWS and aren’t sure what region to use, just pick one
close to you.</p>

<p><strong>Making a Name For Yourself</strong></p>

<p>We’ll be talking about several AWS services in this section, and many of
them make use of AWS Simple Storage Service, or <strong>S3</strong>. S3 allows you to
store objects in the cloud with a high degree of durability. Where S3
objects are stored are called “buckets”. S3 bucket names have to be
unique, not just across you account, but across the entire world. A
bucket name is globally unique. By the time we’re done, we’ll have
created a couple buckets, as well globally unique login URL. For that
reason, you should come up with a unique identifier now. For example,
when we tested this documentation, we used the identifier
“stelligent-cdblueprints.” Just note it down now and we’ll refer to it
as we go on.</p>

<h4>Turn on CloudTrail</h4>

<p>First thing is to turn on CloudTrail. CloudTrail is basically logging
for your AWS account. It will generate JSON files and store them in an
S3 bucket (Amazon’s cloud storage solution) every time an action is
performed on the account. While we won’t be doing a lot with CloudTrail
right away, we’re turning it on now because it’s not retroactive — you
can only see logs after you’ve turned it on. So let’s turn it on first.</p>

<p>(Quick note: CloudTrail is a relatively new service, and at the time of
this writing is only available in two regions: US-East-1 and US-West-2.
If you’re using a different region, you might not be able to turn
CloudTrail on. If that’s the case, just skip on to the next step.)</p>

<ol>
<li>Find CloudTrail panel from the main AWS Console,</li>
<li>Click Get Started and just punch in an S3 Bucket name. (As was
mentioned above, the S3 bucket name has to be globally unique. One
approach is to take the unique identifier you came up with before,
and just append -cloudtrail to it. We’ve named our bucket
“stelligent-cdblueprints-cloudtrail”.)</li>
<li>Click OK and you’re done.</li>
</ol>


<p>That was easy.</p>

<h4>Turn on Programmatic Billing</h4>

<p>Next, we’ll want to turn on Programmatic Billing. This will store your
AWS billing in JSON files in another S3 bucket, so that other services
can analyze your spending and plot trends over time. We’ll be visiting
those kind of tools later on, but we want to enable programmatic billing
now because (just like CloudTrail) it only generates data from the
present — there’s no way to go back and generate historical data. By
turning it on now, when we do start parsing that data for trends, you’ll
have a good amount of data to go back through.</p>

<p>Unlike CloudTrail, you’ll need to create and permission the bucket for
this yourself.</p>

<ol>
<li>Go to the S3 console so we can create a new bucket. (Taking your
previous unique identifier and just appending -billing to it isn’t a
bad idea. We’ve named ours “stelligent-cdblueprints-billing” to keep
with the theme.)</li>
<li>Click Create Bucket and punch that name in.</li>
<li>We’ll need to get a bucket permissions policy. Luckily, AWS will
generate that for us at this page (we’ll need to flip back to the S3
page in a second, so open this in a new
tab): <a href="https://portal.aws.amazon.com/gp/aws/developer/account?ie=UTF8&amp;action=billing-preferences">https://portal.aws.amazon.com/gp/aws/developer/account?ie=UTF8&amp;action=billing-preferences</a></li>
<li>Go down the list and turn everything on one and a time.</li>
<li>When you get to to Programmatic billing, punch in the name of
your bucket, and click “sample policy.” Just copy that policy, then
flip back to your S3 bucket.</li>
<li>Click on the bucket, then properties, then Permissions, and
you’ll see an option for setting an access policy.</li>
<li>Click into that, paste the policy you just copied, and save.</li>
<li>Now, flip back to the Billing Preferences page, click save there</li>
<li>Continue to enable everything else on this page.</li>
</ol>


<h4>If CloudTrail and Programmatic Billing are so important, why aren’t they turned on by default?</h4>

<p>One thing to be aware of with these two services is that they will put
data into your S3 buckets. S3 storage is very cheap, and while it is
pretty close, it is not free. You’ll be paying between nine and fifteen
cents a gig for storage, depending on region. For more details, <a href="https://aws.amazon.com/s3/pricing/">check
out the S3 pricing page</a>. The
services themselves don’t cost anything, though; you only pay for
storing the data they generate.</p>

<h4>Create IAM Users</h4>

<p>Now that the bookkeeping is taken care of, let’s set up some users. A
lot of new AWS users will start doing everything as the root account,
which besides being a bit of a security risk, also poses some issues
when you try to have multiple developers building solutions in your
cloud. That’s why we strongly recommend setting up IAM users and roles
from the beginning.</p>

<p>We’re going to use the AWS Identity and Access Management (IAM) console.
IAM allows you to create users, groups, and roles so that you can manage
users and access to your AWS account. For the first section, we’ll only
be creating one user (for you) and one group (admins) but as your usage
of the cloud increases and you need to add more users, you’ll be able to
control that from here.</p>

<p>To create a new admins group, head to the IAM console</p>

<ol>
<li>Click Create Group, and follow the prompts.</li>
<li>We’ll name the group “admins” and give it Administrator access.</li>
</ol>


<p>Now that we have an admins group, go to the Users panel and create a new
user for yourself to log in as. It’s pretty straightforward, and if you
hit any bumps in the road, <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/Using_SettingUpUser.html">AWS has some pretty good documentation about
it</a>.</p>

<p>After you create the user, add it to the admins group. Then, for each
user we want to set up two types of authentication. The first is a
simple password. Under each users’ Security Credentials tab, click the
“Manage Passwords” button and you’ll be able to assign a password.</p>

<p>After each user logs in, you’ll want to require them to add a
multi-factor authentication (MFA) device to their account. To add an MFA
device</p>

<ol>
<li>the user will need to login and go to the IAM console</li>
<li>find their username</li>
<li>under the security credentials tab, select “Manage MFA device.”</li>
<li>Then follow the steps to add your virtual MFA device to the account.</li>
</ol>


<p>Having MFAs set up for all accounts helps ensure that AWS accounts won’t
be compromised, keeping your data safe. Also, it helps ensure that your
account won’t be used for malicious purposes (DDOS attacks, spam emails,
etc) which would at best would increase your AWS bill and worst case
have your entire account disabled. We strongly recommend enabling MFAs
for all user accounts.</p>

<p>Now that users are able to log in, we’ll need to give them a URL to do
so. If you go to the main IAM console, you’ll find a IAM User Sign-In
URL section. Remember the unique identifier you came up with your
CloudTrail and Programatic Billing buckets? That’s probably a good
option for your sign in URL. Changing it is optional, though highly
recommended.</p>

<p><strong>Wrapping Up</strong></p>

<p>Using AWS is easy; using it well takes some thought. By setting up
logging of your usage and billing information, you’ll be able to
identify trends as time goes on. By setting up groups and users, your
account is prepared to scale as you bring on more developers. And by
giving those users multi-factor authentication, you’ve helped ensure the
security of the account. You’re in a great place to start using the
cloud. In our next post, we’ll lay the foundations for building a
continuous delivery pipeline.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Creating a Secure Deployment Pipeline in Amazon Web Services]]></title>
    <link href="http://bvajjala.github.io/blog/2014/03/23/creating-a-secure-deployment-pipeline-in-amazon-web-services/"/>
    <updated>2014-03-23T11:24:27-04:00</updated>
    <id>http://bvajjala.github.io/blog/2014/03/23/creating-a-secure-deployment-pipeline-in-amazon-web-services</id>
    <content type="html"><![CDATA[<p>Many organizations require a secure infrastructure. I’ve yet to meet a customer that says that security isn’t a concern. But, the decision on “how secure?” should be closely associated with a risk analysis for your organization.</p>

<p>Since Amazon Web Services (AWS) is often referred to as a “public cloud”, people sometimes infer that “public” must mean it’s “out in the public” for all to see. I’ve always seen “public/private clouds” as an
unfortunate use of terms. In this context, public means more like
“Public Utility”. People often interpret “private clouds” to be
inherently more secure. Assuming that “public cloud” = less secure and
“private cloud” = more secure couldn’t be further from the truth. Like
most things, it’s all about how you architect your infrastructure. While
you can define your infrastructure to have open access, AWS provides
many tools to create a truly secure infrastructure while eliminating
access to all but only authorized users.</p>

<p>I’ve created an initial list of many of the practices we use. We don’t
employ all these practices in all situations, as it often depends on our
customers’ particular security requirements. But, if someone asked me
“How do I create a secure AWS infrastructure using a Deployment
Pipeline?”, I’d offer some of these practices in the solution. I’ll be
expanding these over the next few weeks, but I want to start with some
of our practices.</p>

<p><strong>AWS Security</strong></p>

<p>* After initial AWS account creation and login, configure
<a href="https://aws.amazon.com/iam/" title="AWS IAM">IAM</a> so that there’s no need to
use the AWS root account\
 * Apply least privilege to all IAM accounts. Be very careful about who
gets Administrator access.\
 * Enable all IAM password rules\
 * Enable MFA for all users\
 * Secure all data at rest\
 * Secure all data in transit\
 * Put all AWS resources in a <a href="https://aws.amazon.com/vpc/" title="Virtual Private Cloud">Virtual Private
Cloud</a> (VPC).\
 * No EC2 Key Pairs should be shared with others. Same goes for Access
Keys.\
 * Only open required ports to the Internet. For example, with the
exception of, say, port 80, no security groups should have a CIDR Source
of 0.0.0.0/0). The bastion host might have access to port 22 (SSH), but
you should enable
<a href="https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing" title="CIDR">CIDR</a>
to limit access to specific subnets. Using a VPC is a part of a solution
to eliminate Internet access. No canonical environments should have
SSH/RDP access.\
 * Use IAM to limit access to specific AWS resources and/or
remove/limit AWS console access\
 * Apply a bastion host configuration to reduce your attack profile\
 * Use <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html" title="IAM Roles">IAM
Roles</a>
so that there’s no need to configure Access Keys on the instances\
 * Use resource-level permissions in EC2 and RDS\
 * Use SSE to secure objects in S3 buckets\
 * Share initial IAM credentials with others through a secure mechanism
(e.g. <a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard" title="AES">AES-256
encryption</a>)\
 * Use and monitor AWS
<a href="https://aws.amazon.com/cloudtrail/" title="AWS CloudTrail">CloudTrail</a> logs</p>

<p><strong>Deployment Pipeline</strong></p>

<p>A deployment pipeline is a staged process in which the complete software
system is built and tested with every change. Team members receive
feedback as it completes each stage. With most customers, we usually
construct between 4-7 deployment pipeline stages and the pipeline only
goes to the next stage if the previous stages were successful. If a
stage fails, the whole pipeline instance fails. The first stage (often
referred to as the “Commit Stage”) will usually take no more than 10
minutes to complete. Other stages may take longer than this. Most stages
require no human intervention as the software system goes through more
extensive testing on its way to production. With a deployment pipeline,
software systems can be released at any time the business chooses to do
so. Here are some of the security-based practices we employ in
constructing a deployment pipeline.</p>

<p>* Automate everything: Networking (VPC, Route 53) Compute (EC2),
Storage, etc. All <em>AWS</em> automation should be defined in
<a href="https://aws.amazon.com/cloudformation/" title="CloudFormation">CloudFormation</a>.
All environment configuration should be defined using infrastructure
automation scripts – such as Chef, Puppet, etc.\
 * Version Everything: Application Code, Configuration, Infrastructure
and Data\
 * Manage your binary dependencies. Be specific about binary version
numbers. Ensure you have control over these binaries.\
 * Lockdown pipeline environments. Do not allow SSH/RDP access to any
environment in the deployment pipeline\
 * For project that require it, use permissions on the CI server or
Deployment application to limit who can run deployments in certain
environments – such as QA, Pre-Production and Production. When you have
a policy in which all changes are applied through automation and
environments are locked down, this usually becomes less of a concern.
But, it can still be a requirements on some teams.\
 * Use the Disposable Environments pattern – instances are terminated
once every few days. This approach reduces the attack profile\
 * Log everything outside of the EC2 instances (so that they can be
access later). Ensure these log files are encrypted e.g. securely
through S3)\
 * All canonical changes are only applied through automation that are
part of the deployment pipeline. This includes application,
configuration, infrastructure and data change. Infrastructure patch
management would be a part of the pipeline just like any outer software
system change.\
 * No one has access to nor can make direct changes to pipeline
environments\
 * Create high-availability systems Multi-AZ, <a href="https://aws.amazon.com/autoscaling/" title="Auto Scaling">Auto
Scaling</a>, <a href="https://aws.amazon.com/elasticloadbalancing/" title="ELB">Elastic
Load Balancing</a> and
Route 53\
 * For non-Admin AWS users, only provide access to AWS through a secure
<a href="https://en.wikipedia.org/wiki/Continuous_integration" title="Continuous Integration">Continuous
Integration</a>
(CI) server or a self-service application\
 * Use Self-Service Deployments and give developers full SSH/RDP access
to their self-service deployment. Only their particular EC2 Key Pair can
access the instance(s) associated with the deployment. Self-Service
Deployments can be defined in the CI server or a lightweight
self-service application.\
 * Provide capability for any authorized user to perform a self-service
deployment with full SSH/RDP access to the environment they created
(while eliminating outside access)\
 * Run two active environments – We’ve yet to do this for customers,
but if you want to eliminate all access to the canonical production
environment, you might choose to run two active environments at once so
that engineers can access the non-production environment to troubleshoot
a problem in which the environment has the exact same configuration and
data so you’re troubleshooting accurately.\
 * Run automated infrastructure tests to test for security
vulnerabilities (e.g. cross-site scripting, SQL injections, etc.) with
every change committed to the version-control repository as part of the
deployment pipeline.</p>

<p><strong>FAQ</strong></p>

<p>* <strong>What is a canonical environment?</strong> It’s your system of record. You
want your canonical environment to be solely defined in source code and
versioned. If someone makes a change to the canonical system and it
affects everyone it should only be done through automation. While you
can use a self-service deployment to get a copy of the canonical system,
any direct change you make to the environment is isolated and never made
part of the canonical system unless code is committed to the
version-control repository.\
 * <strong>How can I troubleshoot if I cannot directly access canonical
environments?</strong> Using a self-service deployment, you can usually
determine the cause of the problem. If it’s a data-specific problem, you
might import a copy of the production database. If this isn’t possible
for time or security reasons, you might run multiple versions of the
application at once.\
 * <strong>Why should we dispose of environments regularly?</strong> Two primary
reasons. The first is to reduce your attack profile (i.e. if
environments always go up and down, it’s more difficult to hone in on
specific resources. The second reason is that it ensures that all team
members are used to applying all canonical changes through automation
and not relying on environments to always be up and running somewhere.\
 * <strong>Why should we lockdown environments?</strong> To prevent people from
making disruptive environment changes that don’t go through the
version-control repository.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Jenkins Job Builder and How to Extned it]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/22/jenkins-job-builder-and-how-to-extned-it/"/>
    <updated>2014-02-22T08:57:36-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/22/jenkins-job-builder-and-how-to-extned-it</id>
    <content type="html"><![CDATA[<h1>What is jenkins job builder</h1>

<p>Jenkins job builder is extreme good tool to manage your jenkins CI jobs, it takes simple description from YAML files, and use them to configure jenkins.</p>

<pre><code>#set free style job
#job-template.yml
- job:
    name: testjob
    project-type: freestyle
    defaults: global
    disabled: false
    display-name: 'Fancy job name'
    concurrent: true
    quiet-period: 5
    workspace: /srv/build-area/job-name
    block-downstream: false
    block-upstream: false
</code></pre>

<p>Then put your jenkins access into jenkins.ini file</p>

<pre><code>[jenkins]
user=USERNAME
password=USER_TOKEN
url=JENKINS_URL
ignore_cache=IGNORE_CACHE_FLAG
</code></pre>

<p>Based on the job configuration above, you just need to type command</p>

<pre><code>$ jenkins-jobs --conf jenkins.ini update job-template.yaml 
</code></pre>

<p>Then your job <em>testjob</em> is created in your jenkins server.</p>

<p>The project is created by <a href="https://wiki.openstack.org/wiki/InfraTeam">openstack-infrastructure team</a>, it is used to manage the openstack environment, fairly good.</p>

<h1>How it works</h1>

<p>There is no magic behind it, <em>jenkins-jobs</em> just convert the <em>job-template.yaml</em> to jenkins XML request file, and use jenkins remote API to send create request.</p>

<p>Try to do below to understand this.</p>

<pre><code>$ jenkins-jobs test job-template.yaml -o .
</code></pre>

<p>Then xml file <em>testjob</em> is created, see</p>

<pre><code>&lt;?xml version="1.0" ?&gt;
&lt;project&gt;
  &lt;actions/&gt;
  &lt;description&gt;

&amp;lt;!-- Managed by Jenkins Job Builder --&amp;gt;&lt;/description&gt;
  &lt;keepDependencies&gt;false&lt;/keepDependencies&gt;
  &lt;disabled&gt;false&lt;/disabled&gt;
  &lt;displayName&gt;Fancy job name&lt;/displayName&gt;
  &lt;blockBuildWhenDownstreamBuilding&gt;false&lt;/blockBuildWhenDownstreamBuilding&gt;
  &lt;blockBuildWhenUpstreamBuilding&gt;false&lt;/blockBuildWhenUpstreamBuilding&gt;
  &lt;concurrentBuild&gt;true&lt;/concurrentBuild&gt;
  &lt;customWorkspace&gt;/srv/build-area/job-name&lt;/customWorkspace&gt;
  &lt;quietPeriod&gt;5&lt;/quietPeriod&gt;
  &lt;canRoam&gt;true&lt;/canRoam&gt;
  &lt;properties/&gt;
  &lt;scm class="hudson.scm.NullSCM"/&gt;
  &lt;builders/&gt;
  &lt;publishers/&gt;
  &lt;buildWrappers/&gt;
&lt;/project&gt;
</code></pre>

<p>Now you can use curl command to send the request (testjob) directly !!</p>

<pre><code>$ curl --user USER:PASS -H "Content-Type: text/xml" -s --data "@testjob" "http://jenkins-server/createItem?name=testjob"
</code></pre>

<h2>How to recreate your jenkins job</h2>

<p>Looks great, finally you need think about how to re-create your jenkins job, it is also simple, just download the config.xml</p>

<pre><code>$ curl --user USER:PASS http://jenkins-server/testjob/config.xml
</code></pre>

<p>Or open the configuration page in broswer *<a href="http://jenkins-server/testjob/configure*">http://jenkins-server/testjob/configure*</a> and map from YAML file.</p>

<p>You need to read <a href="http://ci.openstack.org/jenkins-job-builder/configuration.html">jenkins job builder&rsquo;s guideline</a> to know the map, generate it had level Macro like <a href="https://wiki.openstack.org/wiki/InfraTeam">builders</a>, which is connected to the <a href="https://github.com/openstack-infra/jenkins-job-builder/blob/master/jenkins_jobs/modules/builders.py">real python builders module</a> to do transformation from YAML to XML.</p>

<p>What you stated in YAML file like</p>

<pre><code>-job:
  name: test_job
  builders:
- shell: "make test"
</code></pre>

<p>it will be converted to</p>

<pre><code>&lt;builders&gt;
&lt;hudson.tasks.Shell&gt;
  &lt;command&gt;make test&lt;/command&gt;&lt;/hudson.tasks.Shell&gt;
&lt;/builders&gt;
</code></pre>

<h2>How to extend</h2>

<p>Greatly to see jenkins job builder already had lots of default modules to support your normal jenkins jobs, but there is exceptions like some none popular jenkins plugins or your own plugins.</p>

<p>Then it is time to extend the module, the existing document: Extending is not clear enough, I will use example to show how it works, code is in <a href="https://github.com/bv2012/jenkins-buddy">github jenkins-buddy</a> project</p>

<p><a href="https://wiki.jenkins-ci.org/display/JENKINS/ArtifactDeployer+Plugin">ArtifactDeployer</a> Plugin is used as example, this plugin is the popular plugin to deploy the artifacts to other folder.</p>

<p>Artifact Deploy Plugin</p>

<p><img src="../downloads/code/artifactdeploy.png" alt="" /></p>

<p>And I want to have .YAML like below</p>

<pre><code>*#artifactdeploy.yaml*
- job:
name: test-job
publishers:
  - artifactdeployer: 
  includes: 'buddy-*.tar.gz'
  remote: '/project/buddy'
</code></pre>

<h2>write codes to transform</h2>

<p>Now I need to download the existing jobs to see how XML looks like, using curl above, I got it like</p>

<pre><code>&lt;publishers&gt;
   ...  
  &lt;org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerPublisher plugin="artifactdeployer@0.27"&gt;
&lt;entries&gt;
  &lt;org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerEntry&gt;
&lt;includes&gt;buddy-*.tar.gz&lt;/includes&gt;
&lt;basedir&gt;&lt;/basedir&gt;
&lt;excludes&gt;&lt;/excludes&gt;
&lt;remote&gt;/project/buddy&lt;/remote&gt;
&lt;flatten&gt;false&lt;/flatten&gt;
&lt;deleteRemote&gt;false&lt;/deleteRemote&gt;
&lt;deleteRemoteArtifacts&gt;false&lt;/deleteRemoteArtifacts&gt;
&lt;deleteRemoteArtifactsByScript&gt;false&lt;/deleteRemoteArtifactsByScript&gt;
&lt;failNoFilesDeploy&gt;false&lt;/failNoFilesDeploy&gt;
  &lt;/org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerEntry&gt;
&lt;/entries&gt;
&lt;deployEvenBuildFail&gt;false&lt;/deployEvenBuildFail&gt;
  &lt;/org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerPublisher&gt;
..
&lt;/publishers&gt; 
</code></pre>

<p>It belongs the section publishers So I write the jenkins_buddy/modules/publishers.py module to add one function artifactdeployer:</p>

<pre><code>def artifactdeployer(parser, xml_parent, data):
    logger = logging.getLogger("%s:artifactdeployer" % __name__)
    artifactdeployer = XML.SubElement(xml_parent, 'org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerPublisher')
    entries = XML.SubElement(artifactdeployer, 'entries')
    entry = XML.SubElement(entries, 'org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerEntry')
    print data
    XML.SubElement(entry, 'includes').text = data['includes']
    XML.SubElement(entry, 'remote').text = data['remote']
</code></pre>

<p>It is the core part handling convert.</p>

<h3>Hook into jenkins-job builder</h3>

<p>Now you need hook this script into jenkins-jobs builder, thank for the entry_points in python, it can be used for this.</p>

<p>Create the plugin related script and structure, add new entry_point in setup.py</p>

<pre><code>#setup.py in jenkins-buddy
entry_points={
    'jenkins_jobs.publishers': [
    'artifactdeployer=jenkins_buddy.modules.publishers:artifactdeployer',
    ],
}
</code></pre>

<p>it tells jenkins-jobs if you meet new keyword artifactdeployer in publishers, please let me jenkins_buddy.modules.publishers:artifactdeployer to handle.</p>

<h3>Verify it</h3>

<p>Build the pip package local and install it</p>

<pre><code>$ python setup.py sdist
$ pip install dist/jenkins-buddy-0.0.5.zip
</code></pre>

<p>And verify the new job, Bingo, it works.</p>

<pre><code>$ jenkins-jobs test artifactdeploy.yaml -o . 
</code></pre>

<h3>###Make it more complete by checking jenkins plugin java code</h3>

<p>Maybe you noticed, it is hack solution, since I skipped some parameter converting and guess what the XML will look like, if you want to make it more complete, we need to check the java codes directly.</p>

<p>src/main/java/org/jenkinsci/plugins/artifactdeployer/ArtifactDeployerPublisher.java is the class we need to take care.</p>

<pre><code>@DataBoundConstructor
public ArtifactDeployerPublisher(List&lt;ArtifactDeployerEntry&gt; deployedArtifact, boolean deployEvenBuildFail) {
    this.entries = deployedArtifact;
    this.deployEvenBuildFail = deployEvenBuildFail;
    if (this.entries == null)
    this.entries = Collections.emptyList();
}
</code></pre>

<p>It is directly mapping from XML into internal data, if you need know more, learn how to develop jenkins plugin.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploy/Release Workflow from GitHub]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/04/deploy-slash-release-workflow-from-github/"/>
    <updated>2014-02-04T09:50:50-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/04/deploy-slash-release-workflow-from-github</id>
    <content type="html"><![CDATA[<h1>## Workflow : Deploying/Release Apps from Development to Production  ##</h1>

<p>Deploying is a big part of the lives of most of our Engineering employees. We don&rsquo;t have a release manager and there are no set weekly deploys. Developers and designers are responsible for shipping new stuff themselves as soon as it&rsquo;s ready. This means that deploying needs to be as smooth and safe a process as possible.</p>

<p>The best system we&rsquo;ve found so far to provide this flexibility is to have people deploy branches. Changes never get merged to master until they have been verified to work in production from a branch. This means that master is always stable; a safe point that we can roll back to if there&rsquo;s a problem.</p>

<p>The basic workflow goes like this:</p>

<ul>
<li>Push changes to a branch in GitHub</li>
<li>Wait for the build to pass on our CI server (Jenkins)</li>
<li>Tell Hubot to deploy it</li>
<li>Verify that the changes work and fix any problems that come up</li>
<li>Merge the branch into master
Not too long ago, however, this system wasn&rsquo;t very smart. A branch could accidentally be deployed before the build finished, or even if the build failed. Employees could mistakenly deploy over each other. As the company has grown, we&rsquo;ve needed to add some checks and balances to help us prevent these kinds of mistakes.</li>
</ul>


<h2>Safety First</h2>

<p>The first thing we do now, when someone tries to deploy, is make a call to <a href="https://github.com/github/janky">Janky</a> to determine whether the current CI build is green. If it hasn&rsquo;t finished yet or has failed, we&rsquo;ll tell the deployer to fix the situation and try again.</p>

<p>Next we check whether the application is currently &ldquo;locked&rdquo;. The lock indicates that a particular branch is being deployed in production and that no other deploys of the application should proceed for the moment. Successful builds on the master branch would otherwise get deployed automatically, so we don&rsquo;t want those going out while a branch is being tested. We also don&rsquo;t want another developer to accidentally deploy something while the branch is out.</p>

<p>The last step is to make sure that the branch we&rsquo;re deploying contains the latest commit on master that has made it into production. Once a commit on master has been deployed to production, it should never be “removed” from production by deploying a branch that doesn’t have that commit in it yet.</p>

<p>We use the GitHub API to verify this requirement. An endpoint on the github.com application exposes the SHA1 that is currently running in production. We submit this to the GitHub compare API to obtain the &ldquo;merge base&rdquo;, or the common ancestor, of master and the production SHA1. We can then compare this to the branch that we&rsquo;re attempting to deploy to check that the branch is caught up. By using the common ancestor of master and production, code that only exists on a branch can be removed from production, and changes that have landed on master but haven&rsquo;t been deployed yet won&rsquo;t require branches to merge them in before deploying.</p>

<p>If it turns out the branch is behind, master gets merged into it automatically. We do this using the new :sparkles:Merging API:sparkles: that we&rsquo;re making available today. This merge starts a new CI build like any other push-style event, which starts a deploy when it passes.</p>

<p>At this point the code actually gets deployed to our servers. We usually deploy to all servers for consistency, but a subset of servers can be specified if necessary. This subset can be by functional role — front-end, file server, worker, search, etc. — or we can specify an individual machine by name, e.g, &lsquo;fe7&rsquo;.</p>

<h2>Watch it in action</h2>

<p>What now? It depends on the situation, but as a rule of thumb, small to moderate changes should be observed running correctly in production for at least 15 minutes before they can be considered reasonably stable. During this time we monitor exceptions, performance, tweets, and do any extra verification that might be required. If non-critical tweaks need to be made, changes can be pushed to the branch and will be deployed automatically. In the event that something bad happens, rolling back to master only takes 30 seconds.</p>

<h2>All done!</h2>

<p>If everything goes well, it&rsquo;s time to merge the changes. At GitHub, we use Pull Requests for almost all of our development, so merging typically happens through the pull request page. We detect when the branch gets merged into master and unlock the application. The next deployer can now step up and ship something awesome.</p>

<h1>How do we do it?</h1>

<p>Most of the magic is handled by an internal deployment service called Heaven. At its core, Heaven is a catalog of Capistrano recipes wrapped up in a Sinatra application with a JSON API. Many of our applications are deployed using generic recipes, but more complicated apps can define their own to specify additional deployment steps. Wiring it up to Janky, along with clever use of post-receive hooks and the GitHub API, lets us hack on the niceties over time. Hubot is the central interface to both Janky and Heaven, giving everyone in Campfire great visibility into what’s happening all of the time. As of this writing, 75 individual applications are deployed by Heaven.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenStack : Git Gerrit and Jenkins Workflow]]></title>
    <link href="http://bvajjala.github.io/blog/2014/02/03/openstack-git-gerrit-and-jenkins-workflow/"/>
    <updated>2014-02-03T14:54:23-05:00</updated>
    <id>http://bvajjala.github.io/blog/2014/02/03/openstack-git-gerrit-and-jenkins-workflow</id>
    <content type="html"><![CDATA[<h1>Gerrit Workflow</h1>

<p><img src="/downloads/code/GerritGitJenkinsWorkflow.png" title="Git Gerrit Jenkins Workflow" alt="Alt text in case picture load fails" /></p>

<h2>Git Account Setup</h2>

<p>You&rsquo;ll need a <a href="https://login.launchpad.net">Launchpad account</a>, since this is how the Web interface for the Gerrit Code Review system will identify you. This is also useful for automatically crediting bug fixes to you when you address them with your code commits.</p>

<p>If you haven&rsquo;t already, <a href="https://www.openstack.org/join/">join The OpenStack Foundation</a> (it&rsquo;s free and required for all code contributors). Among other privileges, this also allows you to vote in elections and run for elected positions within The OpenStack Project. When signing up for Foundation Membership, make sure to give the same E-mail address you&rsquo;ll use for code contributions, since this will need to match your preferred E-mail address in Gerrit.</p>

<p>Visit <a href="https://review.openstack.org/">https://review.openstack.org/</a> and click the Sign In link at the top-right corner of the page. Log in with your Launchpad ID.</p>

<p>Because Gerrit uses Launchpad OpenID single sign-on, you won&rsquo;t need a separate password for Gerrit, and once you log in to one of Launchpad, Gerrit, or Jenkins, you won&rsquo;t have to enter your password for the others.</p>

<p>You&rsquo;ll also want to upload an SSH key while you&rsquo;re at it, so that you&rsquo;ll be able to commit changes for review later.</p>

<p>Ensure that you have run these steps to let git know about your email address:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>Git Config </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git config &mdash;global user.name &ldquo;Firstname Lastname&rdquo;
</span><span class='line'>git config &mdash;global user.email &ldquo;&lt;a href="&#109;&#x61;&#x69;&#x6c;&#116;&#111;&#x3a;&#x79;&#x6f;&#117;&#x72;&#x5f;&#101;&#109;&#97;&#x69;&#108;&#64;&#121;&#x6f;&#x75;&#x72;&#x65;&#109;&#97;&#105;&#x6c;&#x2e;&#x63;&#x6f;&#109;">&#x79;&#111;&#x75;&#114;&#95;&#101;&#109;&#x61;&#x69;&#108;&#64;&#121;&#x6f;&#x75;&#114;&#101;&#x6d;&#x61;&#105;&#x6c;&#46;&#99;&#111;&#x6d;&lt;/a>&rdquo;</span></code></pre></td></tr></table></div></figure></notextile></div>
To check your git configuration:
<div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>Git Config </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git config &mdash;list</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<h2>Git Review Installation</h2>

<p>We recommend using the &ldquo;git-review&rdquo; tool which is a git subcommand that handles all the details of working with Gerrit, the code review system used in OpenStack development. Before you start work, make sure you have git-review installed on your system.</p>

<p>On Ubuntu, MacOSx, or most other Unix-like systems, it is as simple as:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>pip install git-review</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>On Ubuntu Precise (12.04) and later, git-review is included in the distribution, so install it as any other package:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>apt-get install git-review</span></code></pre></td></tr></table></div></figure></notextile></div>
On Fedora 16 and later, git-review is included into the distribution, so install it as any other package:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install git-review</span></code></pre></td></tr></table></div></figure></notextile></div>
On Fedora 15 and earlier you have to install pip (its package name is <code>python-pip</code>), then install git-review using pip in a conventional way.</p>

<p>On Red Hat Enterprise Linux, you must first enable the EPEL repository, then install it as any other package:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install git-review</span></code></pre></td></tr></table></div></figure></notextile></div>
On openSUSE 12.2 and later, git-review is included in the distribution under the name python-git-review, so install it as any other package:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>zypper in python-git-review</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>All of git-review&rsquo;s interactions with gerrit are sequences of normal git commands. If you want to know more about what it&rsquo;s doing, just add -v to the options and it will print out all of the commands it&rsquo;s running.</p>

<h2>Project Setup</h2>

<p>Clone a project in the usual way, for example:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone git://git.openstack.org/openstack/nova.git</span></code></pre></td></tr></table></div></figure></notextile></div>
You may want to ask git-review to configure your project to know about Gerrit at this point. If you don&rsquo;t, it will do so the first time you submit a change for review, but you probably want to do this ahead of time so the Gerrit Change-Id commit hook gets installed. To do so (again, using Nova as an example):
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd nova
</span><span class='line'>git review -s</span></code></pre></td></tr></table></div></figure></notextile></div>
Git-review checks that you can log in to gerrit with your ssh key. It assumes that your gerrit/launchpad user name is the same as the current running user. If that doesn&rsquo;t work, it asks for your gerrit/launchpad user name. If you don&rsquo;t remember the user name go to the settings page on gerrit to check it out (it&rsquo;s not your email address).</p>

<p>Note that you can verify the SSH host keys for review.openstack.org here: <a href="https://review.openstack.org/#/settings/ssh-keys">https://review.openstack.org/#/settings/ssh-keys</a></p>

<p>If you get the error &ldquo;We don&rsquo;t know where your gerrit is.&rdquo;, you will need to add a new git remote. The url should be in the error message. Copy that and create the new remote.
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git remote add gerrit ssh://&lt;username>@review.openstack.org:29418/openstack/nova.git</span></code></pre></td></tr></table></div></figure></notextile></div>
In the project directory, you have a <code>.git</code> hidden directory and a <code>.gitreview</code> hidden file. You can see them with:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ls -la</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<h2>1.4 Normal Workflow</h2>

<p>Once your local repository is set up as above, you must use the following workflow.</p>

<p>Make sure you have the latest upstream changes:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git remote update
</span><span class='line'>git checkout master
</span><span class='line'>git pull &mdash;ff-only origin master</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Create a topic branch to hold your work and switch to it. If you are working on a blueprint, name your topic branch bp/BLUEPRINT where BLUEPRINT is the name of a blueprint in launchpad (for example, &ldquo;bp/authentication&rdquo;). The general convention when working on bugs is to name the branch bug/BUG-NUMBER (for example, &ldquo;bug/1234567&rdquo;). Otherwise, give it a meaningful name because it will show up as the topic for your change in Gerrit.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git checkout -b TOPIC-BRANCH</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>To generate documentation artifacts, navigate to the directory where the pom.xml file is located for the project and run the following command:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mvn clean generate-sources</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<h3>1.4.1 Committing Changes</h3>

<p>Git commit messages should start with a short 50 character or less summary in a single paragraph. The following paragraph(s) should explain the change in more detail.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>If your changes addresses a blueprint or a bug, be sure to mention them in the commit message using the following syntax:&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>Implements: blueprint BLUEPRINT
</span><span class='line'>Closes-Bug: ####### (Partial-Bug or Related-Bug are options)</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>For example:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Adds keystone support&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>&hellip;Long multiline description of the change&hellip;&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>Implements: blueprint authentication
</span><span class='line'>Closes-Bug: #123456
</span><span class='line'>Change-Id: I4946a16d27f712ae2adf8441ce78e6c0bb0bb657</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Note that in most cases the Change-Id line should be automatically added by a Gerrit commit hook that you will want to install. See Project Setup for details on configuring your project for Gerrit. If you already made the commit and the Change-Id was not added, do the Gerrit setup step and run:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git commit &mdash;amend</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>The commit hook will automatically add the Change-Id when you finish amending the commit message, even if you don&rsquo;t actually make any changes.</p>

<p>Make your changes, commit them, and submit them for review:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git commit -a
</span><span class='line'>git review</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p><em>Caution: Do not check in changes on your master branch. Doing so will cause merge commits when you pull new upstream changes, and merge commits will not be accepted by Gerrit.</em></p>

<p>Prior to checking in make sure that you run &ldquo;<a href="http://testrun.org/tox/latest/">tox</a>&rdquo;.</p>

<h3>1.4.2 Review</h3>

<h3>1.4.3 Work in Progress</h3>

<h3>1.4.4 Long-lived Topic Branches</h3>

<h3>1.4.5 Updating a Change</h3>

<h3>1.4.6 Add dependency</h3>
]]></content>
  </entry>
  
</feed>
