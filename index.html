
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Balaji Vajjala's Blog</title>
  <meta name="author" content="Balaji Vajjala">

  
  <meta name="description" content="A Deep Dive into AWS Reserved Instances One of our primary goals at Flux7 Labs is to help our clients reduce
their AWS costs. In fact, our product &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://bvajjala.github.io">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Balaji Vajjala's Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Fjalla+One' rel='stylesheet' type='text/css'>
  

</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner"><hgroup>
  <h1><a href="/">Balaji Vajjala's Blog</a></h1>
  
    <h2>A DevOps Blog from Trenches</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:bvajjala.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/about/home">Home</a></li> 
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/categories">Categories</a></li>
  <li><a href="/projects">Projects</a></li>
  <li><a href="/about/resume">About Me</a></li>
  <li><a href="/about/consulting">Consulting</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/03/25/a-deep-dive-into-aws-reserved-instances/">A Deep Dive Into AWS Reserved Instances</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-03-25T09:35:27-04:00" pubdate data-updated="true">Mar 25<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>A Deep Dive into AWS Reserved Instances</h3>

<p>One of our primary goals at Flux7 Labs is to help our clients reduce
their AWS costs. In fact, our product VyScale is based entirely on cost
optimization using Spot instances. We inform our clients when it makes
economic sense for them to buy instance reservations because
reservations for periods of unexpected minimum usage can be beneficial.
Reserved instances function exactly like on-demand instances, except
that you pay an upfront fee to gain cheaper hourly rates.</p>

<h3>Reserved Instance Pricing</h3>

<p>There are several levels of reservations. Higher ones allow you to pay
more up front in order to achieve a lower hourly cost. The table below
shows the rates for various types of reserved and on-demand instances
for m1.large instances.</p>

<hr />

<pre><code>           Upfront   Hourly in cents
</code></pre>

<p>  On-demand    0         24
  Light Util   243       13.6
  Med Util     554       8.4
  Heavy Util   676       5.6</p>

<hr />

<p>At some levels of utilization it makes sense to purchase reservations,
rather than to rely solely upon on-demand instances. By factoring in
upfront costs, we can determine which levels warrant purchasing
reservations. What’s surprising is that those levels are fairly low,
especially in the case of light reservations. Even at 30% utilization,
light-reservation costs start to break even with those of on-demand
instances. The following graph shows equivalent hourly costs for
reservations at various utilization levels</p>

<p><a href="../../../wp-content/uploads/AWS-reserved-instances-1.png"><img src="../../../wp-content/uploads/AWS-reserved-instances-1.png" alt="Hourl rates of reserved
instances" /></a></p>

<p>To better understand these numbers in terms of total cost, as opposed to
incremental cost, the figure below shows total annual expenditures at
various levels. For a 100% utilization of an instance, one can reduce
costs to almost half of Amazon’s standard pricing for reserved
instances, and that’s without any of the bulk discounts made available
to high-volume AWS customers. At lower levels, reservations cost almost
twice as much as those at higher levels. However, keep in mind that, as
a fraction of overall cost, the amount is still not very high. So, in
cases where growth is expected, it can make sense to purchase
reservations early.</p>

<p><a href="../../../wp-content/uploads/AWS-reserved-instances-2.png"><img src="../../../wp-content/uploads/AWS-reserved-instances-2.png" alt="Annual rates of reserved
instances" /></a></p>

<h3>Do You Have A Reservation?</h3>

<p>Another thing to know about Amazon’s policy is that you receive a
guaranteed instance when you buy a reservation, whereas there is no such
guarantee for on-demand instances. When trying to acquire an on-demand
instance, AWS may return an error message stating that capacity is not
available for that instance type. On the other hand, once you purchase a
reservation Amazon guarantees that the instance will be made available
to you at the moment you request it. If you’re using an autoscaling
solution it can make sense to make light utilization reservations to
handle your excess capacity, even if it costs more to guarantee uptime
to your customers. With Amazon’s guarantee it’s no wonder that Netflix
runs almost exclusively on reservations. As a matter of policy,
Netflix’s use of on-demand instances indicates that more reservations
need to be purchased.</p>

<p>One thing to note is that Amazon always uses reservations first, so you
can’t keep unused light reservations as backups while using on-demand
instances for capacity.</p>

<h3>How Amazon Handles Unused Reservations</h3>

<p>Spot instances are one of the best uses of unutilized reserved capacity
with AWS. They come with no guarantee of availability, and can be taken
away from you at Amazon’s discretion at any time in order to fulfill
other customers’ needs. Spot prices hover around 15% of the on-demand
price, which allows Amazon to make a decent return on unused
reservations and while offering reservations at a relatively low price.</p>

<h3>Conclusion</h3>

<p>The most common reason to use reservations is price, but there are also
other considerations. The final decision should be based on capacity
analysis from a business perspective.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/03/25/6-reasons-why-large-enterprises-should-move-to-amazon-web-services/">6 Reasons Why Large Enterprises Should Move to Amazon Web Services</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-03-25T09:13:48-04:00" pubdate data-updated="true">Mar 25<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>6 reasons why large enterprises should move to Amazon Web Services</h2>

<h3>6 reasons why large enterprises should move to Amazon Web Services</h3>

<p>Amazon has changed the face of the world of startups with its cloud services. Now it’s possible for two men in a garage to set up large
computer clusters for zero capital cost.</p>

<p><a href="https://twitter.com/share?text=Amazon+has+its+sights+set+on+enterprise,+which+they+will+conquer+slowly+but+surely.+&amp;via=OurLabs&amp;related=Flux7Labs&amp;url=http://wp.me/p4sEOD-o6">Amazon has its sights set on enterprise, which they will conquer slowly
but
surely.</a></p>

<p>At Our Labs, one of our specialties is cloud migration for enterprise
clients, and we’ve received considerable feedback about both their
concerns and delights. In this post I’ll explain why I strongly believe
large enterprises should consider moving to the cloud.</p>

<h2><strong>Agility and Responsiveness</strong> </h2>

<p>Let’s face it, if a startup is a speedboat, then a large enterprise is
the Titanic. While startups can pivot any time there’s danger and large
enterprises turn more slowly, there are icebergs that can sink either.
Disasters happen to everyone, but a large enterprise has significantly
more skin in the game, and the old model of waiting a month before
buying a new rack just doesn’t cut it anymore. By shifting to the cloud,
large enterprises can bring up new servers in minutes, which means
shorter downtimes, rapid experimentation, more innovation, increased
global reach, improved demand-surge handling and more successful
short-lived-but-resource-intensive projects.</p>

<h2><strong>Innovation</strong> </h2>

<p>Business is a cutthroat world in which we must always strive to
out-innovate our competitors. This article is about the cloud in
general, but I want to explain why it’s also Amazon focused. Amazon has
been blazing the cloud-computing trail longer than anyone, which is
reflected by its 80% market share, and is clearly ahead of its
competitors. For example, check out Gartner’s analysis of different
cloud providers in the chart below and you’ll see Amazon in a corner of
their own. In fact, Amazon so dominated the competition that Gartner had
to artificially lower the scale. Amazon is innovating at such a rapid
pace in providing new services and broadening its customer base that
competitors are struggling to keep pace.</p>

<p>The effect of this innovation is obvious to developers. Do you want to
provide Active Directory integration with Amazon? You can do that. Do
you want to save on data costs by using the Bittorrent protocol for
content distribution? You can do that. Do you want to put heavy compute
behind your mobile games? You can do that. Do you want to ramp up your
clients quickly with remote workstations? You can do that, too. With
Amazon Web Services you can do all of that and more. I’m a
certified<a href="file:///C:/Users/Vishnu/Documents/Our/SMM/Blog/WhyEnterprisemustAmazon_(REVISED_CLEAN">[1]</a>.docx#_msocom_1) 
AWS instructor, yet I learn something new about AWS every day as Amazon
constantly releases new features. The advantage of moving your business
to the cloud is that you can let your developers focus on your company’s
area of expertise while leaving the boilerplate to Amazon.</p>

<p>\</p>

<p> <img src="https://lh3.googleusercontent.com/GmdBmLBmcGW9TIBqUCVkf403uIIf9arZ6-brIjBOt8tus6n_7YaKdOW3kcQqSIUbKnQd9vthqX1nyHHGouT8xbmDF3-xkpXUbJV0UJLpguSzg7EKB6QuaqR92uw_og" alt="" /></p>

<h2><strong>A Data Center That Hosts A Top Website</strong> </h2>

<p>One thing we mustn’t forget is what the AWS cloud really is—a product of
years of innovation by one of the largest e-commerce websites in the
world. Its data centers are distributed globally in numerous parts of
the world, with multiple, independently-operating data centers in each
region that provide different failure domains while being close enough
together to provide cheap communication. And that’s just the compute
side of the equation. Amazon also shares its global content distribution
network, its DNS servers. it compiles traffic data from across the globe
to find the best route for your network traffic. There are only a
handful of companies today with that kind of global reach, and they,
too, are developing cloud capabilities. But unless you’re in that elite
club, you won’t be able to create a data center that can compete with
AWS. So by using a cloud solution, rather than an in-house solution,
you’ll clearly benefit in terms of better performance, better fault
tolerance, and better disaster recovery.</p>

<h2><strong>Security</strong> </h2>

<p>Security is the biggest concern about cloud computing for most
enterprises, but Amazon holds many of the most important certifications,
including PCI, HIPAA, Sarbanes-Oxley, and ISO. Since it has so much at
stake, it maintains separation of logical and physical access to data in
order to limit the impact of disgruntled employees. While I certainly
understand someone hesitating to entrust one’s most valuable information
with a third party, it’s certainly debatable whether on-premise storage
is more secure than cloud storage.</p>

<h2><strong>In-house Expertise Not Required</strong> </h2>

<p>Anyone that’s had to hire people knows that good employees are worth
their weight in platinum. Moving to the cloud allows you to offload much
of your data-center maintenance onto Amazon. Let it do what it’s best at
while you focus on what you’re best at.</p>

<p>As we know, you often have to pay dearly to hire someone outside of your
area of expertise. For example, say you’re a director of IT at a company
like Schlumberger. Your company is great at what it does and has strong
brand value in its area of expertise. Do you think you can possibly
poach someone like <a href="http://mvdirona.com/jrh/work/">James Hamilton</a> for
your company? No, you can’t. With core expertise in data centers, Amazon
will likely offer a more intriguing challenge and a deeper sense of
mission to the kind of people you’ll want to hire for your team. And we
all know that employee engagement is not about the money, but rather
about being involved in a greater mission.</p>

<h2><strong>Lower Costs</strong> </h2>

<p>While I’m convinced that moving to the cloud will lower your costs, I
acknowledge that getting there requires a lot of expertise and hard
work. Additionally you may very easily find yourself comparing apples
and oranges. Yet cloud solutions are cheaper than on-premise solutions
when played right. First, when you pay for a machine on AWS you’re also
paying for Amazon’s years of expertise in setting up resilient data
centers, something that’s not true for in-house departments lacking core
IT expertise. Second, there’s great potential for saving money by
scaling to your variable demand needs instead of designing for max
capacity. That’s why Netflix, even though it comprises one-third of the
Internet in terms of data volume, has gone “all in” on AWS, or they’d
have to provision enough machines to handle 9PM Friday traffic.
Alternatively you can have an expected area of needing high demand, as
say for a chip company close to tapeout, a movie studio needing to
render for a year
(<a href="http://gigaom.com/2014/03/02/the-oscars-how-american-hustles-fx-team-made-2013-boston-look-like-1980s-new-york/">http://gigaom.com/2014/03/02/the-oscars-how-american-hustles-fx-team-made-2013-boston-look-like-1980s-new-york/</a>),
or a game company handling launch day demand
(<a href="http://www.respawn.com/news/lets-talk-about-the-xbox-live-cloud/">http://www.respawn.com/news/lets-talk-about-the-xbox-live-cloud/</a>).</p>

<h2><strong>Conclusion</strong> </h2>

<p>If you’re a large enterprise and on the fence about whether or not to
move to the cloud, we highly recommend trying it out. Try it on a
contained project, rather than one that’s on a critical path, something
that fits well within the cloud’s capabilities. After implementation, do
a post-mortem and analyze the results in terms of cost and
time-to-market. What is the expectation as you build in-house
expertise<strong>.</strong> The results may well surprise you.</p>

<h2><strong>Let’s Talk</strong> </h2>

<p>Our Research Labs has helped Fortune 100 and Fortune 500 companies move
successfully to AWS. We’d love to hear any questions, comments or
concerns you may have, so feel free to contact us anytime during weekly
<a href="http://ohours.org/aatersuleman">office hours</a> to discuss your specific
needs or situation. We offer this service free of charge with no
obligation or strings attached.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/03/14/ci-and-test-automation-utilizing-openstack/">CI and Test Automation Utilizing OpenStack</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-03-14T09:54:08-04:00" pubdate data-updated="true">Mar 14<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content">
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/02/22/jenkins-job-builder-and-how-to-extned-it/">Jenkins Job Builder and How to Extned It</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-02-22T08:57:36-05:00" pubdate data-updated="true">Feb 22<span>nd</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h1>What is jenkins job builder</h1>

<p>Jenkins job builder is extreme good tool to manage your jenkins CI jobs, it takes simple description from YAML files, and use them to configure jenkins.</p>

<pre><code>#set free style job
#job-template.yml
- job:
    name: testjob
    project-type: freestyle
    defaults: global
    disabled: false
    display-name: 'Fancy job name'
    concurrent: true
    quiet-period: 5
    workspace: /srv/build-area/job-name
    block-downstream: false
    block-upstream: false
</code></pre>

<p>Then put your jenkins access into jenkins.ini file</p>

<pre><code>[jenkins]
user=USERNAME
password=USER_TOKEN
url=JENKINS_URL
ignore_cache=IGNORE_CACHE_FLAG
</code></pre>

<p>Based on the job configuration above, you just need to type command</p>

<pre><code>$ jenkins-jobs --conf jenkins.ini update job-template.yaml 
</code></pre>

<p>Then your job <em>testjob</em> is created in your jenkins server.</p>

<p>The project is created by <a href="https://wiki.openstack.org/wiki/InfraTeam">openstack-infrastructure team</a>, it is used to manage the openstack environment, fairly good.</p>

<h1>How it works</h1>

<p>There is no magic behind it, <em>jenkins-jobs</em> just convert the <em>job-template.yaml</em> to jenkins XML request file, and use jenkins remote API to send create request.</p>

<p>Try to do below to understand this.</p>

<pre><code>$ jenkins-jobs test job-template.yaml -o .
</code></pre>

<p>Then xml file <em>testjob</em> is created, see</p>

<pre><code>&lt;?xml version="1.0" ?&gt;
&lt;project&gt;
  &lt;actions/&gt;
  &lt;description&gt;

&amp;lt;!-- Managed by Jenkins Job Builder --&amp;gt;&lt;/description&gt;
  &lt;keepDependencies&gt;false&lt;/keepDependencies&gt;
  &lt;disabled&gt;false&lt;/disabled&gt;
  &lt;displayName&gt;Fancy job name&lt;/displayName&gt;
  &lt;blockBuildWhenDownstreamBuilding&gt;false&lt;/blockBuildWhenDownstreamBuilding&gt;
  &lt;blockBuildWhenUpstreamBuilding&gt;false&lt;/blockBuildWhenUpstreamBuilding&gt;
  &lt;concurrentBuild&gt;true&lt;/concurrentBuild&gt;
  &lt;customWorkspace&gt;/srv/build-area/job-name&lt;/customWorkspace&gt;
  &lt;quietPeriod&gt;5&lt;/quietPeriod&gt;
  &lt;canRoam&gt;true&lt;/canRoam&gt;
  &lt;properties/&gt;
  &lt;scm class="hudson.scm.NullSCM"/&gt;
  &lt;builders/&gt;
  &lt;publishers/&gt;
  &lt;buildWrappers/&gt;
&lt;/project&gt;
</code></pre>

<p>Now you can use curl command to send the request (testjob) directly !!</p>

<pre><code>$ curl --user USER:PASS -H "Content-Type: text/xml" -s --data "@testjob" "http://jenkins-server/createItem?name=testjob"
</code></pre>

<h2>How to recreate your jenkins job</h2>

<p>Looks great, finally you need think about how to re-create your jenkins job, it is also simple, just download the config.xml</p>

<pre><code>$ curl --user USER:PASS http://jenkins-server/testjob/config.xml
</code></pre>

<p>Or open the configuration page in broswer *<a href="http://jenkins-server/testjob/configure*">http://jenkins-server/testjob/configure*</a> and map from YAML file.</p>

<p>You need to read <a href="http://ci.openstack.org/jenkins-job-builder/configuration.html">jenkins job builder&rsquo;s guideline</a> to know the map, generate it had level Macro like <a href="https://wiki.openstack.org/wiki/InfraTeam">builders</a>, which is connected to the <a href="https://github.com/openstack-infra/jenkins-job-builder/blob/master/jenkins_jobs/modules/builders.py">real python builders module</a> to do transformation from YAML to XML.</p>

<p>What you stated in YAML file like</p>

<pre><code>-job:
  name: test_job
  builders:
- shell: "make test"
</code></pre>

<p>it will be converted to</p>

<pre><code>&lt;builders&gt;
&lt;hudson.tasks.Shell&gt;
  &lt;command&gt;make test&lt;/command&gt;&lt;/hudson.tasks.Shell&gt;
&lt;/builders&gt;
</code></pre>

<h2>How to extend</h2>

<p>Greatly to see jenkins job builder already had lots of default modules to support your normal jenkins jobs, but there is exceptions like some none popular jenkins plugins or your own plugins.</p>

<p>Then it is time to extend the module, the existing document: Extending is not clear enough, I will use example to show how it works, code is in <a href="https://github.com/bv2012/jenkins-buddy">github jenkins-buddy</a> project</p>

<p><a href="https://wiki.jenkins-ci.org/display/JENKINS/ArtifactDeployer+Plugin">ArtifactDeployer</a> Plugin is used as example, this plugin is the popular plugin to deploy the artifacts to other folder.</p>

<p>Artifact Deploy Plugin</p>

<p><img src="../downloads/code/artifactdeploy.png" alt="" /></p>

<p>And I want to have .YAML like below</p>

<pre><code>*#artifactdeploy.yaml*
- job:
name: test-job
publishers:
  - artifactdeployer: 
  includes: 'buddy-*.tar.gz'
  remote: '/project/buddy'
</code></pre>

<h2>write codes to transform</h2>

<p>Now I need to download the existing jobs to see how XML looks like, using curl above, I got it like</p>

<pre><code>&lt;publishers&gt;
   ...  
  &lt;org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerPublisher plugin="artifactdeployer@0.27"&gt;
&lt;entries&gt;
  &lt;org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerEntry&gt;
&lt;includes&gt;buddy-*.tar.gz&lt;/includes&gt;
&lt;basedir&gt;&lt;/basedir&gt;
&lt;excludes&gt;&lt;/excludes&gt;
&lt;remote&gt;/project/buddy&lt;/remote&gt;
&lt;flatten&gt;false&lt;/flatten&gt;
&lt;deleteRemote&gt;false&lt;/deleteRemote&gt;
&lt;deleteRemoteArtifacts&gt;false&lt;/deleteRemoteArtifacts&gt;
&lt;deleteRemoteArtifactsByScript&gt;false&lt;/deleteRemoteArtifactsByScript&gt;
&lt;failNoFilesDeploy&gt;false&lt;/failNoFilesDeploy&gt;
  &lt;/org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerEntry&gt;
&lt;/entries&gt;
&lt;deployEvenBuildFail&gt;false&lt;/deployEvenBuildFail&gt;
  &lt;/org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerPublisher&gt;
..
&lt;/publishers&gt; 
</code></pre>

<p>It belongs the section publishers So I write the jenkins_buddy/modules/publishers.py module to add one function artifactdeployer:</p>

<pre><code>def artifactdeployer(parser, xml_parent, data):
    logger = logging.getLogger("%s:artifactdeployer" % __name__)
    artifactdeployer = XML.SubElement(xml_parent, 'org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerPublisher')
    entries = XML.SubElement(artifactdeployer, 'entries')
    entry = XML.SubElement(entries, 'org.jenkinsci.plugins.artifactdeployer.ArtifactDeployerEntry')
    print data
    XML.SubElement(entry, 'includes').text = data['includes']
    XML.SubElement(entry, 'remote').text = data['remote']
</code></pre>

<p>It is the core part handling convert.</p>

<h3>Hook into jenkins-job builder</h3>

<p>Now you need hook this script into jenkins-jobs builder, thank for the entry_points in python, it can be used for this.</p>

<p>Create the plugin related script and structure, add new entry_point in setup.py</p>

<pre><code>#setup.py in jenkins-buddy
entry_points={
    'jenkins_jobs.publishers': [
    'artifactdeployer=jenkins_buddy.modules.publishers:artifactdeployer',
    ],
}
</code></pre>

<p>it tells jenkins-jobs if you meet new keyword artifactdeployer in publishers, please let me jenkins_buddy.modules.publishers:artifactdeployer to handle.</p>

<h3>Verify it</h3>

<p>Build the pip package local and install it</p>

<pre><code>$ python setup.py sdist
$ pip install dist/jenkins-buddy-0.0.5.zip
</code></pre>

<p>And verify the new job, Bingo, it works.</p>

<pre><code>$ jenkins-jobs test artifactdeploy.yaml -o . 
</code></pre>

<h3>###Make it more complete by checking jenkins plugin java code</h3>

<p>Maybe you noticed, it is hack solution, since I skipped some parameter converting and guess what the XML will look like, if you want to make it more complete, we need to check the java codes directly.</p>

<p>src/main/java/org/jenkinsci/plugins/artifactdeployer/ArtifactDeployerPublisher.java is the class we need to take care.</p>

<pre><code>@DataBoundConstructor
public ArtifactDeployerPublisher(List&lt;ArtifactDeployerEntry&gt; deployedArtifact, boolean deployEvenBuildFail) {
    this.entries = deployedArtifact;
    this.deployEvenBuildFail = deployEvenBuildFail;
    if (this.entries == null)
    this.entries = Collections.emptyList();
}
</code></pre>

<p>It is directly mapping from XML into internal data, if you need know more, learn how to develop jenkins plugin.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/02/04/nodejs-deployment-building-and-configuring-on-amazon-linux-ami/">Nodejs Deployment: Building and Configuring on Amazon Linux AMI</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-02-04T13:47:50-05:00" pubdate data-updated="true">Feb 4<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>Logging in and updating system to latest</h2>

<p>SSH your shiny new VM,</p>

<p>Now lets update the system to the latest:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo yum update</span></code></pre></td></tr></table></div></figure>


<h2>Install OS dependencies</h2>

<p>We’r going to build Node.js from sources, some dependencies (such as gcc) are required:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo yum install gcc-c++ make openssl-devel git</span></code></pre></td></tr></table></div></figure>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2014/02/04/nodejs-deployment-building-and-configuring-on-amazon-linux-ami/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/02/04/deploy-slash-release-workflow-from-github/">Deploy/Release Workflow From GitHub</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-02-04T09:50:50-05:00" pubdate data-updated="true">Feb 4<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h1>## Workflow : Deploying/Release Apps from Development to Production  ##</h1>

<p>Deploying is a big part of the lives of most of our Engineering employees. We don&rsquo;t have a release manager and there are no set weekly deploys. Developers and designers are responsible for shipping new stuff themselves as soon as it&rsquo;s ready. This means that deploying needs to be as smooth and safe a process as possible.</p>

<p>The best system we&rsquo;ve found so far to provide this flexibility is to have people deploy branches. Changes never get merged to master until they have been verified to work in production from a branch. This means that master is always stable; a safe point that we can roll back to if there&rsquo;s a problem.</p>

<p>The basic workflow goes like this:</p>

<ul>
<li>Push changes to a branch in GitHub</li>
<li>Wait for the build to pass on our CI server (Jenkins)</li>
<li>Tell Hubot to deploy it</li>
<li>Verify that the changes work and fix any problems that come up</li>
<li>Merge the branch into master
Not too long ago, however, this system wasn&rsquo;t very smart. A branch could accidentally be deployed before the build finished, or even if the build failed. Employees could mistakenly deploy over each other. As the company has grown, we&rsquo;ve needed to add some checks and balances to help us prevent these kinds of mistakes.</li>
</ul>


<h2>Safety First</h2>

<p>The first thing we do now, when someone tries to deploy, is make a call to <a href="https://github.com/github/janky">Janky</a> to determine whether the current CI build is green. If it hasn&rsquo;t finished yet or has failed, we&rsquo;ll tell the deployer to fix the situation and try again.</p>

<p>Next we check whether the application is currently &ldquo;locked&rdquo;. The lock indicates that a particular branch is being deployed in production and that no other deploys of the application should proceed for the moment. Successful builds on the master branch would otherwise get deployed automatically, so we don&rsquo;t want those going out while a branch is being tested. We also don&rsquo;t want another developer to accidentally deploy something while the branch is out.</p>

<p>The last step is to make sure that the branch we&rsquo;re deploying contains the latest commit on master that has made it into production. Once a commit on master has been deployed to production, it should never be “removed” from production by deploying a branch that doesn’t have that commit in it yet.</p>

<p>We use the GitHub API to verify this requirement. An endpoint on the github.com application exposes the SHA1 that is currently running in production. We submit this to the GitHub compare API to obtain the &ldquo;merge base&rdquo;, or the common ancestor, of master and the production SHA1. We can then compare this to the branch that we&rsquo;re attempting to deploy to check that the branch is caught up. By using the common ancestor of master and production, code that only exists on a branch can be removed from production, and changes that have landed on master but haven&rsquo;t been deployed yet won&rsquo;t require branches to merge them in before deploying.</p>

<p>If it turns out the branch is behind, master gets merged into it automatically. We do this using the new :sparkles:Merging API:sparkles: that we&rsquo;re making available today. This merge starts a new CI build like any other push-style event, which starts a deploy when it passes.</p>

<p>At this point the code actually gets deployed to our servers. We usually deploy to all servers for consistency, but a subset of servers can be specified if necessary. This subset can be by functional role — front-end, file server, worker, search, etc. — or we can specify an individual machine by name, e.g, &lsquo;fe7&rsquo;.</p>

<h2>Watch it in action</h2>

<p>What now? It depends on the situation, but as a rule of thumb, small to moderate changes should be observed running correctly in production for at least 15 minutes before they can be considered reasonably stable. During this time we monitor exceptions, performance, tweets, and do any extra verification that might be required. If non-critical tweaks need to be made, changes can be pushed to the branch and will be deployed automatically. In the event that something bad happens, rolling back to master only takes 30 seconds.</p>

<h2>All done!</h2>

<p>If everything goes well, it&rsquo;s time to merge the changes. At GitHub, we use Pull Requests for almost all of our development, so merging typically happens through the pull request page. We detect when the branch gets merged into master and unlock the application. The next deployer can now step up and ship something awesome.</p>

<h1>How do we do it?</h1>

<p>Most of the magic is handled by an internal deployment service called Heaven. At its core, Heaven is a catalog of Capistrano recipes wrapped up in a Sinatra application with a JSON API. Many of our applications are deployed using generic recipes, but more complicated apps can define their own to specify additional deployment steps. Wiring it up to Janky, along with clever use of post-receive hooks and the GitHub API, lets us hack on the niceties over time. Hubot is the central interface to both Janky and Heaven, giving everyone in Campfire great visibility into what’s happening all of the time. As of this writing, 75 individual applications are deployed by Heaven.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/02/03/openstack-git-gerrit-and-jenkins-workflow/">OpenStack : Git Gerrit and Jenkins Workflow</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-02-03T14:54:23-05:00" pubdate data-updated="true">Feb 3<span>rd</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h1>Gerrit Workflow</h1>

<p><img src="/downloads/code/GerritGitJenkinsWorkflow.png" title="Git Gerrit Jenkins Workflow" alt="Alt text in case picture load fails" /></p>

<h2>Git Account Setup</h2>

<p>You&rsquo;ll need a <a href="https://login.launchpad.net">Launchpad account</a>, since this is how the Web interface for the Gerrit Code Review system will identify you. This is also useful for automatically crediting bug fixes to you when you address them with your code commits.</p>

<p>If you haven&rsquo;t already, <a href="https://www.openstack.org/join/">join The OpenStack Foundation</a> (it&rsquo;s free and required for all code contributors). Among other privileges, this also allows you to vote in elections and run for elected positions within The OpenStack Project. When signing up for Foundation Membership, make sure to give the same E-mail address you&rsquo;ll use for code contributions, since this will need to match your preferred E-mail address in Gerrit.</p>

<p>Visit <a href="https://review.openstack.org/">https://review.openstack.org/</a> and click the Sign In link at the top-right corner of the page. Log in with your Launchpad ID.</p>

<p>Because Gerrit uses Launchpad OpenID single sign-on, you won&rsquo;t need a separate password for Gerrit, and once you log in to one of Launchpad, Gerrit, or Jenkins, you won&rsquo;t have to enter your password for the others.</p>

<p>You&rsquo;ll also want to upload an SSH key while you&rsquo;re at it, so that you&rsquo;ll be able to commit changes for review later.</p>

<p>Ensure that you have run these steps to let git know about your email address:</p>

<figure class='code'><figcaption><span>Git Config </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git config --global user.name "Firstname Lastname"
</span><span class='line'>git config --global user.email "your_email@youremail.com"</span></code></pre></td></tr></table></div></figure>


<p>To check your git configuration:</p>

<figure class='code'><figcaption><span>Git Config </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git config --list</span></code></pre></td></tr></table></div></figure>


<h2>Git Review Installation</h2>

<p>We recommend using the &ldquo;git-review&rdquo; tool which is a git subcommand that handles all the details of working with Gerrit, the code review system used in OpenStack development. Before you start work, make sure you have git-review installed on your system.</p>

<p>On Ubuntu, MacOSx, or most other Unix-like systems, it is as simple as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>pip install git-review</span></code></pre></td></tr></table></div></figure>


<p>On Ubuntu Precise (12.04) and later, git-review is included in the distribution, so install it as any other package:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>apt-get install git-review</span></code></pre></td></tr></table></div></figure>


<p>On Fedora 16 and later, git-review is included into the distribution, so install it as any other package:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install git-review</span></code></pre></td></tr></table></div></figure>


<p>On Fedora 15 and earlier you have to install pip (its package name is <code>python-pip</code>), then install git-review using pip in a conventional way.</p>

<p>On Red Hat Enterprise Linux, you must first enable the EPEL repository, then install it as any other package:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install git-review</span></code></pre></td></tr></table></div></figure>


<p>On openSUSE 12.2 and later, git-review is included in the distribution under the name python-git-review, so install it as any other package:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>zypper in python-git-review</span></code></pre></td></tr></table></div></figure>


<p>All of git-review&rsquo;s interactions with gerrit are sequences of normal git commands. If you want to know more about what it&rsquo;s doing, just add -v to the options and it will print out all of the commands it&rsquo;s running.</p>

<h2>Project Setup</h2>

<p>Clone a project in the usual way, for example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone git://git.openstack.org/openstack/nova.git</span></code></pre></td></tr></table></div></figure>


<p>You may want to ask git-review to configure your project to know about Gerrit at this point. If you don&rsquo;t, it will do so the first time you submit a change for review, but you probably want to do this ahead of time so the Gerrit Change-Id commit hook gets installed. To do so (again, using Nova as an example):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cd nova
</span><span class='line'>git review -s</span></code></pre></td></tr></table></div></figure>


<p>Git-review checks that you can log in to gerrit with your ssh key. It assumes that your gerrit/launchpad user name is the same as the current running user. If that doesn&rsquo;t work, it asks for your gerrit/launchpad user name. If you don&rsquo;t remember the user name go to the settings page on gerrit to check it out (it&rsquo;s not your email address).</p>

<p>Note that you can verify the SSH host keys for review.openstack.org here: <a href="https://review.openstack.org/#/settings/ssh-keys">https://review.openstack.org/#/settings/ssh-keys</a></p>

<p>If you get the error &ldquo;We don&rsquo;t know where your gerrit is.&rdquo;, you will need to add a new git remote. The url should be in the error message. Copy that and create the new remote.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git remote add gerrit ssh://&lt;username>@review.openstack.org:29418/openstack/nova.git</span></code></pre></td></tr></table></div></figure>


<p>In the project directory, you have a <code>.git</code> hidden directory and a <code>.gitreview</code> hidden file. You can see them with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ls -la</span></code></pre></td></tr></table></div></figure>


<h2>1.4 Normal Workflow</h2>

<p>Once your local repository is set up as above, you must use the following workflow.</p>

<p>Make sure you have the latest upstream changes:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git remote update
</span><span class='line'>git checkout master
</span><span class='line'>git pull --ff-only origin master</span></code></pre></td></tr></table></div></figure>


<p>Create a topic branch to hold your work and switch to it. If you are working on a blueprint, name your topic branch bp/BLUEPRINT where BLUEPRINT is the name of a blueprint in launchpad (for example, &ldquo;bp/authentication&rdquo;). The general convention when working on bugs is to name the branch bug/BUG-NUMBER (for example, &ldquo;bug/1234567&rdquo;). Otherwise, give it a meaningful name because it will show up as the topic for your change in Gerrit.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git checkout -b TOPIC-BRANCH</span></code></pre></td></tr></table></div></figure>


<p>To generate documentation artifacts, navigate to the directory where the pom.xml file is located for the project and run the following command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mvn clean generate-sources</span></code></pre></td></tr></table></div></figure>


<h3>1.4.1 Committing Changes</h3>

<p>Git commit messages should start with a short 50 character or less summary in a single paragraph. The following paragraph(s) should explain the change in more detail.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>If your changes addresses a blueprint or a bug, be sure to mention them in the commit message using the following syntax:
</span><span class='line'>
</span><span class='line'>Implements: blueprint BLUEPRINT
</span><span class='line'>Closes-Bug: ####### (Partial-Bug or Related-Bug are options)</span></code></pre></td></tr></table></div></figure>


<p>For example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Adds keystone support
</span><span class='line'>
</span><span class='line'>...Long multiline description of the change...
</span><span class='line'>
</span><span class='line'>Implements: blueprint authentication
</span><span class='line'>Closes-Bug: #123456
</span><span class='line'>Change-Id: I4946a16d27f712ae2adf8441ce78e6c0bb0bb657</span></code></pre></td></tr></table></div></figure>


<p>Note that in most cases the Change-Id line should be automatically added by a Gerrit commit hook that you will want to install. See Project Setup for details on configuring your project for Gerrit. If you already made the commit and the Change-Id was not added, do the Gerrit setup step and run:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git commit --amend</span></code></pre></td></tr></table></div></figure>


<p>The commit hook will automatically add the Change-Id when you finish amending the commit message, even if you don&rsquo;t actually make any changes.</p>

<p>Make your changes, commit them, and submit them for review:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git commit -a
</span><span class='line'>git review</span></code></pre></td></tr></table></div></figure>


<p><em>Caution: Do not check in changes on your master branch. Doing so will cause merge commits when you pull new upstream changes, and merge commits will not be accepted by Gerrit.</em></p>

<p>Prior to checking in make sure that you run &ldquo;<a href="http://testrun.org/tox/latest/">tox</a>&rdquo;.</p>

<h3>1.4.2 Review</h3>

<h3>1.4.3 Work in Progress</h3>

<h3>1.4.4 Long-lived Topic Branches</h3>

<h3>1.4.5 Updating a Change</h3>

<h3>1.4.6 Add dependency</h3>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/02/03/designing-a-restful-api-that-doesnt-suck/">Designing a RESTful API That Doesn&#8217;t Suck</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-02-03T14:23:30-05:00" pubdate data-updated="true">Feb 3<span>rd</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2><a href="/blog/2013/03/22/designing-a-restful-api-that-doesn-t-suck.html">Designing A RESTful API That Doesn&rsquo;t Suck</a></h2>

<p>As we&rsquo;re getting closer to shipping the first version of <a href="http://devo.ps">devo.ps</a> and we are joined by a few new team members, the team took the time to review the few principles we followed when designing our RESTful JSON API. A lot of these can be found on <a href="https://blog.apigee.com/taglist/rest_api_design">apigee&rsquo;s blog</a> (a recommended read). Let me give you the gist of it:</p>

<ul>
<li><p><strong>Design your API for developers first</strong>, they are the main users. In that respect, simplicity and intuitivity matter.</p></li>
<li><p><strong>Use <a href="http://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods">HTTP verbs</a></strong> instead of relying on parameters (e.g. <code>?action=create</code>). HTTP verbs map nicely with <a href="http://en.wikipedia.org/wiki/Create,_read,_update_and_delete">CRUD</a>:</p>

<ul>
<li><code>POST</code> for <em>create</em>,</li>
<li><code>GET</code> for <em>read</em>,</li>
<li><code>DELETE</code> for <em>remove</em>,</li>
<li><code>PUT</code> for <em>update</em> (and <code>PATCH</code> too).</li>
</ul>
</li>
<li><p><strong>Use <a href="http://en.wikipedia.org/wiki/List_of_HTTP_status_codes">HTTP status codes</a></strong>, especially for errors (authentication required, error on the server side, incorrect parameters)… There are plenty to choose from, here are a few:</p>

<ul>
<li><code>200</code>: <em>OK</em></li>
<li><code>201</code>: <em>Created</em></li>
<li><code>304</code>: <em>Not Modified</em></li>
<li><code>400</code>: <em>Bad Request</em></li>
<li><code>401</code>: <em>Unauthorized</em></li>
<li><code>403</code>: <em>Forbidden</em></li>
<li><code>404</code>: <em>Not Found</em></li>
<li><code>500</code>: <em>Internal Server Error</em></li>
</ul>
</li>
<li><p><strong>Simple URLs for resources: first a noun for the collection, then the item</strong>. For example <code>/emails</code> and <code>/emails/1234</code>; the former gives you the collection of emails, the second one a specific one identified by its internal id.</p></li>
<li><p><strong>Use verbs for special actions</strong>. For example, <code>/search?q=my+keywords</code>.</p></li>
<li><p><strong>Keep errors simple but verbose (and use HTTP codes)</strong>. We only send something like <code>{ message: "Something terribly wrong happened" }</code> with the proper status code (e.g. <code>401</code> if the call requires authentication) and log more verbose information (origin, error code…) in the backend for debugging and monitoring.</p></li>
</ul>


<p>Relying on HTTP status codes and verbs should already help you keep your API calls and responses lean enough. Less crucial, but still useful:</p>

<ul>
<li><strong>JSON first</strong>, then extend to other formats if needed and if time permits.</li>
<li><strong><a href="http://en.wikipedia.org/wiki/Unix_time">Unix time</a></strong>, or you&rsquo;ll have a bad time.</li>
<li><strong>Prepend your URLs with the API version</strong>, like <code>/v1/emails/1234</code>.</li>
<li><strong>Lowercase everywhere in URLs</strong>.</li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/02/03/i-can-haz-init-script/">I Can Haz Init Script</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-02-03T14:21:03-05:00" pubdate data-updated="true">Feb 3<span>rd</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2><a href="/blog/2013/06/19/I-can-haz-init-script.html">I Can Haz Init Script</a></h2>

<p>Something went awfully wrong, and a rogue process is eating up all of the resources on one of your servers. You have no other choice but to restart it. No big deal, really; this is the age of disposable infrastructure after all. Except when it comes back up, everything starts going awry. Half the stuff supposed to be running is down and it&rsquo;s screwing with the rest of your setup.</p>

<p><img src="/images/posts/y-u-no-guy.png" alt="INIT SCRIPTS, Y U NO LIKE?" /></p>

<p>You don&rsquo;t get to think about them very often, but init scripts are a key piece of a sound, scalable strategy for your infrastructure. It&rsquo;s a <a href="">mandatory best practice</a>. Period. And there are quite a few things in the way of getting them to work properly at scale in production environments. It&rsquo;s a tough world out there.</p>

<h3>What we&rsquo;re dealing with…</h3>

<h4>Packages</h4>

<p>Often enough, you&rsquo;re gonna end up installing a service using the package manager of your distro: <code>yum</code>, <code>apt-get</code>, you name it. These packages usually come with an init script that should get you started.</p>

<p>Sadly, as your architecture grows in complexity, you&rsquo;ll probably run into some walls. Wanna have multiple memcache buckets, or several instances of redis running on the same box? You&rsquo;re out of luck buddy. Time to hack your way
through:</p>

<ul>
<li>Redefine your start logic,</li>
<li>Load one or multiple config files from <code>/etc/defaults</code> or <code>/etc/sysconfig</code>,</li>
<li>Deal with the PIDs, log and lock files,</li>
<li>Implement conditional logic to start/stop/restart one or more of the services,</li>
<li>Realize you&rsquo;ve messed something up,</li>
<li>Same player shoot again.</li>
</ul>


<p>Honestly: PITA.</p>

<h4>Built from source</h4>

<p>First things first: <strong>you shouldn&rsquo;t be building from source</strong> (unless you really, really need to).</p>

<p>Now if you do, you&rsquo;ll have to be thorough: there may be samples of init scripts in there, but you&rsquo;ll have to dig them out. <code>/contrib</code>, <code>/addons</code>, …it&rsquo;s never in the same place.</p>

<p>And that makes things &ldquo;fun&rdquo; when you&rsquo;re <a href="http://devo.ps/blog/2013/03/06/troubleshooting-5minutes-on-a-yet-unknown-box.html">trying to unscrew things on a box</a>:</p>

<ul>
<li>You figured out that MySQL is running from <code>/home/user/src/mysql</code>,</li>
<li>You check if there&rsquo;s an init script: no luck this time…</li>
<li>You try to understand what exactly launched <code>mysqld_safe</code>,</li>
<li>You spend a while digging into the bash history smiling at typos,</li>
<li>You stumble on a <code>run.sh</code> script (uncommented, of course) in the home directory. Funny enough, it seems to be starting everything from MySQL, NGINX and php-fpm to the coffee maker.</li>
<li>You make a mental note to try and track down the &ldquo;genius&rdquo; who did that mess of a job, and get busy with converting everything to a proper init script.</li>
</ul>


<p>Great.</p>

<h3>Why existing solutions suck</h3>

<p>Well, based on what we&rsquo;ve just seen, you really only have two options:</p>

<ol>
<li> <strong>DIY</strong>; but if you&rsquo;re good at what you do, you&rsquo;re probably also lazy. You may do it the first couple times, but that&rsquo;s not gonna scale, especially when dealing with the various flavors of init daemons (upstart, systemd…),</li>
<li> <strong>Use that thing called &ldquo;the Internet&rdquo;</strong>; you read through forum pages, issue queues, gists and if you&rsquo;re lucky you&rsquo;ll find a perfect one (or more likely 10 sucky ones). Kudos to all those of whom shared their work, but you&rsquo;ll probably be back to option 1.</li>
</ol>


<h3>We can do better than this</h3>

<p>You&rsquo;ll find a gazillion websites for pictures of kittens, but as far as I know, there is no authoritative source for init scripts. That&rsquo;s just not right: we have to fix it. A few things I&rsquo;m aiming for:</p>

<ul>
<li><strong>Scalable</strong>; allow for multiple instances of a service to be started at once from different config files (see the memcache/redis example),</li>
<li><strong>Secure</strong>; ensure <code>configtest</code> is run before a restart/reload (because, you know, a faulty config file preventing the service to restart is kind of a bummer),</li>
<li><strong>Smart</strong>; ensuring for example that the cache is aggressively flushed before restarting your database (so that you don&rsquo;t end-up waiting 50 min for the DB to cleanly shutdown).</li>
</ul>


<p><a href="https://github.com/devo-ps/init-scripts">I&rsquo;ve just created a repo</a> where I&rsquo;ll be dumping various init scripts that will hopefully be helpful to others. I&rsquo;d love to get suggestions or help.</p>

<p>And by the way, things are not much better with applications, though we&rsquo;re trying our best to improve things there too with things like <a href="https://github.com/Unitech/pm2">pm2</a> (fresh and shinny, more about it in a later post).</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/02/03/goodbye-node-forever/">Goodbye Node-forever</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-02-03T14:15:10-05:00" pubdate data-updated="true">Feb 3<span>rd</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2><a href="/blog/2013/06/26/goodbye-node-forever-hello-pm2.html">Goodbye node-forever, hello PM2</a></h2>

<p><img src="http://apps.hemca.com/pm2/pres/pm22.png" alt="pm2 logo" /></p>

<p>It&rsquo;s no secret that the devo.ps team has a crush on Javascript; node.js in the backend, AngularJS for our clients, there isn&rsquo;t much of our stack that isn&rsquo;t at least in part built with it. Our approach of building <a href="http://devo.ps/blog/2013/01/31/farewell-to-regular-web-development-approaches.html">static clients and RESTful JSON APIs</a> means that we run a lot of node.js and I must admit that, despite all of it awesomeness, node.js still is a bit of a
headache when it comes to running in production. Tooling and best practices (think monitoring, logging, error traces…) are still lacking when compared to some of the more established languages.</p>

<p>So far, we had been relying on the pretty nifty <a href="https://github.com/nodejitsu/forever">node-forever</a>. Great tool, but a few things were missing:</p>

<ul>
<li>Limited monitoring and logging abilities,</li>
<li>Poor support for process management configuration,</li>
<li>No support for clusterization,</li>
<li>Aging codebase (which meant frequent failures when upgrading Node).</li>
</ul>


<p>This is what led us to write <a href="https://github.com/Unitech/pm2">PM2</a> in the past couple months. We thought we&rsquo;d give you a quick look at it while we&rsquo;re nearing a production ready release.</p>

<h3>So what&rsquo;s in the box?</h3>

<p>First things first, you can install it with <code>npm</code>:</p>

<pre><code>npm install -g pm2
</code></pre>

<p>Let&rsquo;s open things up with the usual comparison table:</p>

<p>FeatureForeverPM2</p>

<p>Keep Alive</p>

<p>✔</p>

<p>✔</p>

<p>Coffeescript</p>

<p>✔</p>

<p>Log aggregation</p>

<p>✔</p>

<p>API</p>

<p>✔</p>

<p>Terminal monitoring</p>

<p>✔</p>

<p>Clustering</p>

<p>✔</p>

<p>JSON configuration</p>

<p>✔</p>

<p>And now let me geek a tad more about the main features…</p>

<h3>Native clusterization</h3>

<p>Node v0.6 introduced the cluster feature, allowing you to share a socket across multiple networked Node applications. Problem is, it doesn&rsquo;t work out of the box and requires some tweaking to handle master and children processes.</p>

<p>PM2 handles this natively, without any extra code: PM2 itself will act as the master process and wrap your code into a special clustered process, as Nodejs does, to add some global variables to your files.</p>

<p>To start a clustered app using all the CPUs you just need to type something like that:</p>

<pre><code>$ pm2 start app.js -i max
</code></pre>

<p>Then;</p>

<pre><code>$ pm2 list
</code></pre>

<p>Which should display something like (ASCII UI FTW);</p>

<p><img src="http://apps.hemca.com/pm2/pres/pm2-list.png" alt="pm2 list" /></p>

<p>As you can see, your app is now forked into multiple processes depending on the number of CPUs available.</p>

<h3>Monitoring <em>a la</em> termcaps-HTOP</h3>

<p>It&rsquo;s nice enough to have an overview of the running processes and their status with the <code>pm2 list</code> command. But what about tracking their resources consumption? Fear not:</p>

<pre><code>$ pm2 monit
</code></pre>

<p>You should get the CPU usage and memory consumption by process (and cluster).</p>

<p><img src="http://apps.hemca.com/pm2/pres/pm2-monit.png" alt="pm2 monit" /></p>

<p><strong>Disclaimer</strong>: <a href="https://github.com/arunoda/node-usage">node-usage</a> doesn&rsquo;t support MacOS for now (feel free to PR). It works just fine on Linux though.</p>

<p>Now, what about checking on our clusters and GC cleaning of the memory stack?
Let&rsquo;s consider you already have an HTTP benchmark tool (if not, you should definitely check <a href="https://github.com/wg/wrk">WRK</a>):</p>

<pre><code>$ express bufallo     // Create an express app
$ cd bufallo
$ npm install
$ pm2 start app.js -i max
$ wrk -c 100 -d 100 http://localhost:3000/
</code></pre>

<p>In another terminal, launch the monitoring option:</p>

<pre><code>$ pm2 monit
</code></pre>

<p>W00t!</p>

<h3>Realtime log aggregation</h3>

<p>Now you have to manage multiple clustered processes: one who&rsquo;s crawling data, another who is processing stuff, and so on so forth. That means logs, lots of it. You can still handle it the old fashioned way:</p>

<pre><code>$ tail -f /path/to/log1 /path/to/log2 ...
</code></pre>

<p>But we&rsquo;re nice, so we wrote the <code>logs</code> feature:</p>

<pre><code>$ pm2 logs
</code></pre>

<p><img src="http://apps.hemca.com/pm2/pres/pm2-logs.png" alt="pm2 monit" /></p>

<h3>Resurrection</h3>

<p>So things are nice and dandy, your processes are humming and you need to do a hard restart. What now? Well, first, dump things:</p>

<pre><code>$ pm2 dump
</code></pre>

<p>From there, you should be able to resurrect things from file:</p>

<pre><code>$ pm2 kill     // let's simulate a pm2 stop
$ pm2 resurect // All my processes are now up and running 
</code></pre>

<h3>API Health point</h3>

<p>Let&rsquo;s say you want to monitor all the processes managed by PM2, as well as the status of the machine they run on (and maybe even build a nice Angular app to consume this API…):</p>

<pre><code>$ pm2 web
</code></pre>

<p>Point your browser at <code>http://localhost:9615</code>, aaaaand… done!</p>

<h3>And there&rsquo;s more…</h3>

<ul>
<li>Full tests,</li>
<li>Generation of <code>update-rc.d</code> (<code>pm2 startup</code>), though still very alpha,</li>
<li>Development mode with auto restart on file change (<code>pm2 dev</code>), still very drafty too,</li>
<li>Log flushing,</li>
<li>Management of your applications fleet via JSON file,</li>
<li>Log uncaught exceptions in error logs,</li>
<li>Log of restart count and time,</li>
<li>Automated killing of processes exiting too fast.</li>
</ul>


<h3>What&rsquo;s next?</h3>

<p>Well first, you could show your love on Github (we love stars):
<a href="https://github.com/Unitech/pm2">https://github.com/Unitech/pm2</a>.</p>

<p>We developed PM2 to offer an advanced and complete solution for Node process management. We&rsquo;re looking forward to getting more people helping us getting there: pull requests are more than welcome. A few things already on the
roadmap that we&rsquo;ll get right at once we have a stable core:</p>

<ul>
<li>Remote administration/status checking,</li>
<li>Built-in inter-processes communication channel (message bus),</li>
<li>V8 GC memory leak detection,</li>
<li>Web interface,</li>
<li>Persistent storage for monitoring data,</li>
<li>Email notifications.</li>
</ul>


<p>Special thanks to <a href="https://github.com/makara">Makara Wang</a> for concepts/tools and <a href="https://github.com/rlidwka">Alex Kocharin</a> for advices and pull requests.</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>About Me</h1>
  <p>A little something about me.</p>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/03/25/a-deep-dive-into-aws-reserved-instances/">A Deep Dive into AWS Reserved Instances</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/03/25/6-reasons-why-large-enterprises-should-move-to-amazon-web-services/">6 reasons why large enterprises should move to Amazon Web Services</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/03/14/ci-and-test-automation-utilizing-openstack/">CI and Test Automation utilizing OpenStack</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/02/22/jenkins-job-builder-and-how-to-extned-it/">Jenkins Job Builder and How to Extned it</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/02/04/nodejs-deployment-building-and-configuring-on-amazon-linux-ami/">Nodejs Deployment: Building and Configuring on Amazon Linux AMI</a>
      </li>
    
  </ul>
</section>



<section class="googleplus">
  <h1>
    <a href="https://plus.google.com/bvajjala@gmail.com?rel=author">
      <img src="http://www.google.com/images/icons/ui/gprofile_button-32.png" width="32" height="32">
      Google+
    </a>
  </h1>
</section>



  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Balaji Vajjala -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> | Themed with <a href="https://github.com/lucaslew/whitespace">Whitespace</a></span>
</p>

</footer>
  






<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
