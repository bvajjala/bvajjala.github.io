
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Balaji Vajjala's Blog</title>
  <meta name="author" content="Balaji Vajjala">

  
  <meta name="description" content="A Case Study on using 100% Cloud-based Resources with Automated Software Delivery We
help – typically large – organizations create one-click software &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://bvajjala.github.io">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Balaji Vajjala's Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Fjalla+One' rel='stylesheet' type='text/css'>
  

</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner"><hgroup>
  <h1><a href="/">Balaji Vajjala's Blog</a></h1>
  
    <h2>A DevOps Blog from Trenches</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:bvajjala.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/04/04/a-case-study-on-using-100-percent-cloud-based-resources-with-automated-software-delivery/">A Case Study on Using 100% Cloud Based Resources With Automated Software Delivery</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-04-04T14:08:01-04:00" pubdate data-updated="true">Apr 4<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h1>A Case Study on using 100% Cloud-based Resources with Automated Software Delivery</h1>

<p><a href="http://stelligent.com" title="Stelligent Continuous Delivery in the Cloud">We</a>
help – typically large – organizations create one-click software
delivery systems so that they can deliver software in a more rapid,
reliable and repeatable manner (AKA <a href="http://www.amazon.com/Continuous-Delivery-Deployment-Automation-Addison-Wesley/dp/0321601912" title="Continuous Delivery book">Continuous
Delivery</a>).
The only way this works is when Development works with Operations. As
has been written elsewhere in this series, this means changing the
hearts and minds of people because most organizations are used to
working in ‘siloed’ environments. In this entry, I focus on
implementation, by describing a real-world case study in which we have
brought Continuous Delivery Operations to the Cloud consisting of a team
of Systems and Software Engineers.  </p>

<p>For years, we’ve helped customers in <a href="http://www.amazon.com/gp/product/0321336380/?tag=integratecom-20" title="Continuous Integration">Continuous
Integration</a>
and Testing so more of our work was with Developers and Testers. Several
years ago, we hired a Sys Admin/Engineer/DBA who was passionate about
automation. As a result of this, we began assembling multiple two-person
“<a href="http://en.wikipedia.org/wiki/DevOps" title="DevOps on Wikipedia">DevOps</a>”
teams consisting of a Software Engineer and a Systems Engineer both of
whom being big-picture thinkers and not just “Developers” or “Sys
Admins”. These days, we put together these targeted teams of Continuous
Delivery and Cloud experts with hands-on experience as Software
Engineers and Systems Engineers so that organizations can deliver
software as quickly and as often as the business requires.</p>

<p>A couple of years ago we already had a few people in the company who
were experimenting with using Cloud infrastructures so we thought this
would be a great opportunity in providing cloud-based delivery
solutions. In this case study, I cover a project we are currently
working on for a large organization. It is a new Java-based web services
project so we’ve been able to implement solutions using our recommended
software delivery patterns rather than being constrained by legacy tools
or decisions. However, as I note, we aren’t without constraints on this
project. If I were you, I’d call “BS!” on any “case study” in which
everything went flawlessly and assume it was an extremely small or a
theoretical project in the author’s mind. This is the real deal. Enough
said, on to the case study.      </p>

<p><img src="https://s3.amazonaws.com/stelligent_img/aws_tools.jpg" alt="AWS Tools" /></p>

<p><strong>Fast Facts</strong></p>

<p><strong>Industry</strong>: Healthcare, Public Sector\
<strong>Profile</strong>: The customer is making available to all, free of charge, a
series of software specifications and open source software modules that
together make up an oncology-extended Electronic Health Record
capability.\
<strong>Key Business Issues</strong>: The customer was seeking that all team members
are provided “unencumbered” access to infrastructure resources without
the usual “request and wait” queued-based procedures present in most
organizations \
<strong>Stakeholders</strong>: Over 100 people consisting of Developers, Testers,
Analysts, Architects, and Project Management.\
<strong>Solution:</strong> Continuous Delivery Operations in the Cloud\
<strong>Key Tools/Technologies</strong>: Amazon Web Services  - AWS (Elastic Computer
Cloud (EC2), (Simple Storage Service (S3), Elastic Block Storage (EBS),
etc.), Jenkins, JIRA Studio, Ant, Ivy, Tomcat and PostgreSQL</p>

<p><strong>The Business Problem</strong>\
The customer was used to dealing with long drawn-out processes with
Operations teams that lacked agility. They were accustomed to submitting
Word documents via email to an Operations teams, attending multiple
meetings and getting their environments setup weeks or months later. We
were compelled to develop a solution that reduced or eliminated these
problems that are all too common in many large organizations (Note: each
problem is identified as a letter and number, for example: P1, and
referred to later):</p>

<ol>
<li>Unable to deliver software to users on demand (P1)</li>
<li>Queued requests for provisioned instances (P2)</li>
<li>Unable to reprovision precise target environment configuration on
demand (P3)</li>
<li>Unable to provision instances on demand (P4)</li>
<li>Configuration errors in target environments presenting deployment
bottlenecks while Operations and Development teams troubleshoot
errors (P5)</li>
<li>Underutilized instances (P6)</li>
<li>No visibility into purpose of instance (P7)</li>
<li>No visibility into the costs of instance (P8)</li>
<li>Users cannot terminate instances (P9)</li>
<li>Increased Systems Operations personnel costs (P10)</li>
</ol>


<p><strong>Our Team</strong>\
We put together a four-person team to create a solution for delivering
software and managing the internal Systems Operations for this 100+
person project. We also hired a part-time Security expert. The team
consists of two Systems Engineers and two Software Engineers focused on
Continuous Delivery and the Cloud. One of the Software Engineers is the
Solutions Architect/PM for our team.</p>

<p><strong>Our Solution</strong>\
We began with the end in mind based on the customer’s desire for
unencumbered access to resources. To us, “unencumbered” did not mean
without controls; it meant providing automated services over queued
“request and wait for the Ops guy to fulfill the request” processes. Our
approach is that every resource is in the cloud: Software as a Service
(SaaS), Platform as a Service (PaaS) or Infrastructure as a Service
(IaaS) to reduce operations costs (P10) and increase efficiency. In
doing this, effectively all project resources are available on demand in
the cloud. We have also automated the software delivery process to
Development and Test environments and working on the process of
one-click delivery to production. I’ve identified the problem we’re
solving – from above – in parentheses (P1, P8, etc.). The solution
includes:</p>

<ul>
<li><strong>On-Demand Provisioning</strong> – All hardware is provided via EC2’s
virtual instances in the cloud, on demand (P2). We’ve developed a
“Provisioner” (PaaS) that provides any authorized team member the
capability to click a button and get their project-specific target
environment (P3) in the AWS’ cloud – thus, providing unencumbered
access to hardware resources. (P4) The Provisioner provides all
authorized team members the capability to monitor instance usage
(P6) and adjust accordingly. Users can terminate their own virtual
instances (P9).</li>
<li><strong>Continuous Delivery</strong> Solution so that the team can deliver
software to users on demand (P1):

<ul>
<li>Automated build script using Ant – used to drive most of the
other automation tools</li>
<li>Dependency Management using Ivy. We will be adding Sonatype
Nexus</li>
<li>Database Integration/Change using Ant and Liquibase</li>
<li>Automated Static Analysis using Sonar (with CheckStyle,
FindBugs, JDepend, and Cobertura)</li>
<li>Test framework hooks for running JUnit, etc.</li>
<li>Reusing remote Deployment custom Ant scripts that use Java
Secure Channel and Web container configuration. However, we will
be starting a process of using a more robust tool such as
ControlTier to perform deployment</li>
<li>Automated document generation using Grand, SchemaSpy (ERDs) and
UMLGraph</li>
<li>Continuous Integration server using Hudson</li>
<li>Continuous Delivery pipeline system – we are customizing Hudson
to emulate a Deployment Pipeline</li>
</ul>
</li>
<li><strong>Issue Tracking</strong> – We’re using the JIRA Studio SaaS product from
Atlassian (P10), which provides issue tracking, version-control
repository, online code review and a Wiki. We also manage the
relationship with the vendor and perform the user administration
including workflow management and reporting.</li>
<li><strong>Development Infrastructure</strong>&ndash; There were numerous tools selected
by the customer for Requirements Management and Test Management and
Execution including HP QC, LoadRunner, SoapUI, Jama Contour. Many of
these tools were installed and managed by our team onto the EC2
instances</li>
<li><strong>Instance Management</strong>&ndash; Any authorized team member is able to
monitor virtual instance usage by viewing a web-based dashboard (P6,
P7, P8) we developed. This helps to determine instances that should
no longer be in use or may be eating up too much money. There is a
policy that test instances (e.g. Sprint Testing) are terminated no
less than every two weeks. This promotes ephemeral environments and
test automation.</li>
<li><strong>Deployment to Production</strong> – Much of the pre-production
infrastructure is in place, but we will be adding some additional
automation features to make it available to users in production
(P1). The deployment sites are unique in that we aren’t hosting a
single instance used by all users and it’s likely the software will
be installed at each site. One plan is to deploy separate instances
to the cloud or to virtual instances that are shipped to the user
centers</li>
<li><p><strong>System Monitoring and Disaster Recovery</strong> – Using
<a href="https://www.cloudkick.com/" title="CloudKick AWS Monitoring">CloudKick</a>
to notify us of instance errors or anomalies. EC2 provides us with
some monitoring as well. We will be implementing a more robust
monitoring solution using Nagios or something similar in the coming
months. Through automation and supporting process, we’ve implemented
a disaster recovery solution.</p></li>
</ul>


<p><strong>Benefits</strong>\
The benefits are primarily around removing the common bottlenecks from
processes so that software can be delivered to users and team members
more often. Also, we think our approach to providing on-demand services
over queued-based requests increases agility and significantly reduces
costs. Here are some of the benefits:</p>

<ul>
<li><strong>Deliver software more often</strong> – to users and internally (testers,
managers, demos)</li>
<li><strong>Deliver software more quickly</strong> – since the software delivery
process is automated, we identify the SVN tag and click a button to
deliver the software to any environment</li>
<li><strong>Software delivery is rapid, reliable and repeatable</strong>. All
resources can be reproduced with a single click – source code,
configuration, environment configuration, database and network
configuration is all checked in and versioned and part of a single
delivery system.</li>
<li><strong>Increased visibility</strong> to environments and other resources – All
preconfigured virtual hardware instances are available for any
project member to provision without needing to submit forms or
attend countless meetings</li>
</ul>


<p><strong>Tools</strong>\
Here are some of the tools we are using to deliver this solution. Some
of the tools were chosen by our team exclusively and some by other
stakeholders on the project.</p>

<ul>
<li><a href="http://aws.amazon.com/ec2/" title="AWS EC2"><strong>AWS EC2</strong></a>&ndash; Cloud-based
virtual hardware instances</li>
<li><a href="http://aws.amazon.com/s3/" title="AWS S3"><strong>AWS S3</strong></a> – Cloud-based
storage. We use S3 to store temporary software binaries and backups</li>
<li><a href="http://aws.amazon.com/ebs/" title="AWS EBS"><strong>AWS EBS</strong></a> – Elastic Block
Storage. We use EBS to attach PostgreSQL data volumes</li>
<li><a href="http://ant.apache.org/" title="Ant"><strong>Ant</strong></a> – Build Automation</li>
<li><a href="https://www.cloudkick.com/" title="CloudKick"><strong>CloudKick</strong></a> – Real-time
Cloud instance monitoring</li>
<li><a href="http://controltier.com/" title="ControlTier"><strong>ControlTier</strong></a> –
Deployment Automation. Not implemented yet.</li>
<li><strong>HP LoadRunner</strong> – Load Testing</li>
<li><strong>HP Quality Center (QC)</strong> – Test Management and Orchestration</li>
<li><strong>Ivy</strong> – Dependency Management</li>
<li><strong>Jama Contor</strong>&ndash; Requirements Management</li>
<li><a href="http://jenkins-ci.org/" title="Jenkins"><strong>Jenkins</strong></a> – Continuous
Integration Server</li>
<li><a href="http://www.atlassian.com/hosted/studio/" title="JIRA Studio"><strong>JIRA
Studio</strong></a>&ndash;
Issue Tracking, Code Review, Version-Control, Wiki</li>
<li><strong>JUnit</strong> – Unit and Component Testing</li>
<li><a href="http://www.liquibase.org/" title="Liquibase"><strong>Liquibase</strong></a> – Automated
database change management</li>
<li><strong>Nagios</strong> – or Zenoss. Not implemented yet</li>
<li><strong>Nexus</strong> – Dependency Management Repository Manager (not
implemented yet)</li>
<li><strong>PostgreSQL</strong> – Database used by Development team. We’ve written
script that automate database change management</li>
<li><strong>Provisioner</strong> (Custom Web-based) – Target Environment Provisioning
and Virtual Instance Monitoring</li>
<li><a href="http://www.puppetlabs.com/" title="Puppet"><strong>Puppet</strong></a> – Systems
Configuration Management</li>
<li><strong>QTP</strong> – Test Automation</li>
<li><strong>SoapUI</strong> – Web Services Test Automation</li>
<li><a href="http://www.sonarsource.org/" title="Sonar"><strong>Sonar</strong></a> – code quality
analysis (Includes CheckStyle, PMD, Cobertura, etc.)</li>
<li><strong>Tomcat/JBoss</strong> – Web container used by Development. We’ve written
script to automate the deployment and container configuration</li>
</ul>


<p><strong>Solutions we’re in the process of Implementing</strong>\
We’re less than a year into the project and have much more work to do.
Here are a few projects we’re in the process or will be starting to
implement soon:</p>

<ul>
<li>System Configuration Management – We’ve started using Puppet, but we
are expanding how it’s being used in the future</li>
<li>Deployment Automation – The move to a more robust Deployment
automation tool such as ControlTier</li>
<li>Development Infrastructure Automation – Automating the provisioning
and configuration of tools such as HP QC in a cloud environment.
etc.</li>
</ul>


<p><strong>What we would do Differently</strong>\
Typically, if we were start a Java-based project and recommend tools
around testing, we might choose the following tools for testing,
requirements and test management based on the particular need:</p>

<ul>
<li>Selenium with
<a href="http://saucelabs.com/" title="SauceLabs Selenium">SauceLabs</a></li>
<li>JIRA Studio for Test Management</li>
<li>JIRA Studio for Requirements Management</li>
<li>JMeter – or other open source tool – for Load Testing</li>
</ul>


<p>However, like most projects there are many stakeholders who have their
preferred approach and tools they are familiar in using, the same way
our team does. Overall, we are pleased with how things are going so far
and the customer is happy with the infrastructure and approach that is
in place at this time. I could probably do another case study on dealing
with multiple SaaS vendors, but I will leave that for another post.</p>

<p><strong>Summary</strong>\
There’s much more I could have written about what we’re doing, but I
hope this gives you a decent perspective of how we’ve implemented a
DevOps philosophy with Continuous Delivery and the Cloud and how this
has led our customer to more a service-based, unencumbered and agile
environment. </p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/03/25/continuous-delivery-implementation-laying-the-foundations-of-a-continuous-delivery-pipeline/">Continuous Delivery Implementation : Laying the Foundations of a Continuous Delivery Pipeline</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-03-25T15:02:54-04:00" pubdate data-updated="true">Mar 25<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>This article is part of the Continuous Delivery Blueprints series. It
discusses how to go from no cloud infrastructure and no continuous
integration set up to having a functioning Continuous Delivery Pipeline
in Amazon Web Services. It discusses high level topics while also
providing a reference implementation so it’s easy to follow along with.
You can read part one
<a href="../getting-started-with-aws-the-right-way/index.html" title="here">here</a>.</p>

<p>What we’re going to do today:</p>

<ul>
<li>• Set up Version Control System</li>
<li>• Migrate code into Version Control System</li>
<li>• Launch Jenkins Server</li>
<li>• Explore Jenkins Server</li>
</ul>


<p>With your AWS account set up and ready to go, it’s time to start setting
up your pipeline. At the end of this article, you’ll have a functioning
Continuous Integration server, and can start building out your
pipeline.\</p>

<hr />

<p><strong>Set up a Version Control System</strong></p>

<p>****To do any kind of continuous integration, delivery, or deployment,
you need your source code in source control. It’s the foundation upon
which everything else is built. If your source is already under a
version control system, you can just skip this step.</p>

<p>Otherwise, it’s time to set up a source control system there are two
options when it comes to source control: either you can build and run
the server which your source control service lives on, or you can use a
hosted source control provider.</p>

<p>Choosing a hosted provider gives you a lot of things — you can instantly
start using it without having to configure any sort of server; you don’t
have to worry about backups and disaster recovery; you’ll always be able
to take advantage of the latest features. On the downside, you are
giving your source code to another entity, which may not be an
acceptable trade off at your organization. You’ll need to figure out
which solution works best for you. If you’re not using source control
already, this is probably the easiest option.</p>

<p>For the purposes of these blueprints, we’ll be using Github for our
source hosting. If you’d like to set up your own git server, <a href="http://git-scm.com/book/en/Git-on-the-Server-Setting-Up-the-Server" title="there are directions for that online">there are
directions for that
online</a>.</p>

<p>There are other version control options (like Mercurial) and other
hosted source options (like bitbucket). All major version control
solutions will work with the Continuous Delivery Pipeline. That said,
we’d advise against using Subversion. Subversion is easy to pick up, but
once you really get going with it you can lose a lot of time dealing
with merge headaches. Distributed version control systems like Git and
Mercurial and better choices in the long run. (If you already have your
source code in Subversion, don’t worry — you’ll still be able to do
everything we talk about.)</p>

<p>Throughout the blueprints, we’ll be referring to our reference
implementation project, CanaryBoard. It’s an open source project
Stelligent built. It is currently hosted in Github, and if you’re just
getting started, we recommend you use Github as well. Setting up an
account with Github is pretty easy, here’s the account sign up form if
you don’t already have an account. Note that if you use the free github
account, you do not get any private repositories, and your code is open
up to the world. If you need a private repository, Github offers paid
plans with private repositories.</p>

<p>With an account set up, it’s time to import your code. If your
organization doesn’t use source control, it’s a snap to get it added,
just follow these instructions to set up a repo, copy your code in, and
then commit the code and push it to github. Boom. Done.</p>

<p>If you’re not ready to import your code, you can fork the CanaryBoard
repository and follow along using that as a reference implementation. To
do that, just go to the CanaryBoard repo page, and look for the “Fork”
button at the top right.</p>

<p><strong>Set up your AWS Access Credentials</strong></p>

<p>****To interact with AWS programmatically, you need to provide it with
access credentials. These are created via the AWS console and stored in
environment variables for the scripts to use. Since we’re going to be
running scripts to set up the Jenkins server, we need to create those
keys now. To do this, log into the AWS Console:</p>

<ol>
<li>Navigate to the IAM panel</li>
<li>Select “Users” from the left side</li>
<li>Select the user you want to create credentials for</li>
<li>In the lower pane, click the “Security Credentials tab”</li>
<li>Click “Manage Access Keys”</li>
<li>Click “Create Access Key”</li>
<li>Click “Download Credentials”</li>
</ol>


<p>Now we have the credentials created, but we need to store them in
environment variables. On Linux or OSX, use these commands (replacing
the values with your actual key values)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export AWS_ACCESS_KEY_ID=ASDF1234567890ASDF
</span><span class='line'>export AWS_SECRET_ACCESS_KEY=qwerty1234567890qwerty1234567890qwerty</span></code></pre></td></tr></table></div></figure>


<p>On Windows:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>set AWS_ACCESS_KEY_ID=ASDF1234567890ASDF
</span><span class='line'>set AWS_SECRET_ACCESS_KEY=qwerty1234567890qwerty1234567890qwerty</span></code></pre></td></tr></table></div></figure>


<p>Boom. Now we’re ready to programmatically create some computing
instances.</p>

<p><strong>Set up a Continuous Integration Server</strong></p>

<p>So, with your source in source control, we’re ready to set up your
Continuous Integration server. For the purpose of these blueprints,
we’ll be using Jenkins. Jenkins is a free, open source Continuous
Integration server, and it’s widely used. In fact, you might already be
using Jenkins somewhere in your organization.\
 Jenkins is pretty easy to set up, but during these lessons we’ll be
taking advantage of several Jenkins plugins and using a bunch of job
configuration that is pretty static. Because of that, we’re able to
provide you with a script to launch a Jenkins server. It creates an
Opsworks stack, and then pulls in some Chef cookbooks we collected to
set up a Jenkins server that’s ready for you to work on.</p>

<p>The first thing you’ll want to do is clone the Stelligent CDRI repo and
run the Jenkins server script:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone https://github.com/stelligent/cdri.git
</span><span class='line'>cd cdri
</span><span class='line'>bundle install
</span><span class='line'>ruby bin/create_jenkins_server_stack.rb</span></code></pre></td></tr></table></div></figure>


<p>(Don’t have Git and/or Ruby installed? Well then, you’ll probably want
to follow these directions for <a href="http://git-scm.com/book/en/Getting-Started-Installing-Git" title="installing Git">installing
Git</a>
or <a href="https://www.ruby-lang.org/en/downloads/" title="installing Ruby">installing
Ruby</a>.) The
script will take a couple of minutes to lay down everything it needs to
set up a Jenkins server. After the script completes, though, it’ll still
take a bit of time for the Jenkins server to build itself and be ready
to go. The script actually has a couple options you may want to take
advantage of:</p>

<pre><code>$ ruby bin/create_jenkins_server_stack.rb --help
Options:
 --region, -r &lt;s&gt;: The AWS region to use (default: us-west-2)
 --zone, -z &lt;s&gt;: The AWS availability zone to use (default: us-west-2a)
 --source, -s &lt;s&gt;: The github repo where the source to build resides (will not work with anything but github!)
 (default: https://github.com/stelligent/canaryboard.git)
 --size, -i &lt;s&gt;: The instance size to use (default: m1.large)
 --help, -h: Show this message
</code></pre>

<p>The default region is us-west-2. If you’re in North America, this is
probably fine; otherwise you may want to switch it to a region closer to
you. (A list of AWS regions is <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html" title="available here">available
here</a>.)
The default zone us us-west-2a, so if you change the region, you will
need to change this as well.</p>

<p>The source repo is configurable, with a couple of caveats. The
repository must be a github repository, and it must be a public repo. It
is possible to have Jenkins connect to really any other repository type,
and it is possible to set up username / password information or SSH keys
to authenticate against a private repo. Both increase complexity, so to
keep things simple, we’ve restricted the script to only run against
public Github repositories.</p>

<p>Finally, you can configure the size of the EC2 instance that the Jenkins
server will run on. A list of instance sizes and prices is available
here. Note that anything smaller that a c3.large instance will take
considerably longer to start up, but shouldn’t impact your ability to
run builds at all. (However, Jenkins is a bit too heavy to run on a
t1.micro instance, so don’t try going that low; the Chef scripts won’t
even successfully run.)</p>

<p><strong>How the Jenkins Server is Set Up</strong></p>

<p>To set up the Jenkins server, we take advantage of two AWS services:
CloudFormation and OpsWorks. These services handle a lot of the tedious
bits about setting up AWS resources, and handle coordinating all the
Chef calls necessary. AWS has a bunch of CloudFormation and OpsWorks
documentation you can refer to, and any questions you have about it
should be in there. We’ll just give you the highlights in the blueprints
to get around the tools we provide; if you need more information, please
refer to the documentation.</p>

<p>The first thing the script does is create a CloudFormation stack to
create everything that the subsequent OpsWorks stack will need (security
groups, roles, and policies). Once those are in place, it then sets up a
new OpsWorks stack with a single custom layer and instance for the
Jenkins server. If you go to the OpsWorks console, you’ll see a Jenkins
stack with an instance starting. Once that instance turns green, your
Jenkins server will be up and running. You can access it by clicking on
the IP address in the instance details.</p>

<p>Since you’re just getting started, it’s probably a good idea to note
that the server you just launched will cost you money for each hour it’s
up. If you’re not going to be doing anything with it right away, you may
want to stop it when you’re not using it.</p>

<p>Note: when you stop the instance through OpsWorks you’ll lose any
configuration changes you’ve made to the server!</p>

<p><strong>Exploring the Pipeline</strong></p>

<p>The Jenkins server comes prepopulated with the beginnings of a
continuous delivery pipeline. If you go to the “Continuous Delivery
Pipeline” view, you’ll see all the different stages laid out. As you
work through this series, you’ll build out each of these stages for your
application. For now, let’s take a look at the first two stages, the
trigger stage and the commit stage.</p>

<p>The trigger stage is a simple monitoring job. Its purpose is to look for
changes in the repositories of your project, and if it detects a change,
it kicks off a new run of the pipeline.</p>

<p>The commit stage handles building, packaging, and unit testing the
application. It is supposed to run quickly (around five minutes) and
give fast feedback to the developers. We will cover the commit stage in
depth in the next part of this series.</p>

<p><strong>Wrapping Up</strong></p>

<p>If you didn’t already have source control setup, you do now. That’s
huge! Source control is probably the most important part of a continuous
delivery pipeline. We also set up a Jenkins server, to automate the
different parts of our pipeline. With your code in source control, and
your continuous integration server running, you’re ready to start
building out the different stages of the pipeline to work with your
application. In the next article, we’ll talk about how to set up your
commit stage to build and test your application.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/03/25/continuous-delivery-implementation-getting-started-with-aws/">Continuous Delivery Implementation : Getting Started With AWS</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-03-25T14:57:15-04:00" pubdate data-updated="true">Mar 25<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><ul>
<li>Blog</li>
<li>This article is part of the Continuous Delivery Blueprints series.
It discusses how to go from no cloud infrastructure and no
continuous integration set up to having a functioning Continuous
Delivery Pipeline in Amazon Web Services. It discusses high level
topics while also providing a reference implementation so it’s easy
to follow along with.

<ul>
<li>Turn on CloudTrail</li>
<li>Turn on Programmatic Billing</li>
<li>If CloudTrail and Programmatic Billing are so important, why
aren’t they turned on by default?</li>
<li>Create IAM Users</li>
<li>Comments</li>
<li>Trackbacks</li>
<li>Post a comment</li>
<li>Categories</li>
</ul>
</li>
</ul>


<h1>This article is part of the Continuous Delivery Blueprints series. It discusses how to go from no cloud infrastructure and no continuous integration set up to having a functioning Continuous Delivery Pipeline in Amazon Web Services. It discusses high level topics while also providing a reference implementation so it’s easy to follow along with.</h1>

<p>Everyone is talking about migrating to the cloud these days, and getting
started with Amazon Web Services is super simple to do. However, most
people just rush in, creating headaches for themselves down the road.
There are some best practices you should take at the beginning of your
cloud migration that will make things easier, more secure, and allow you
to scale up and out better.</p>

<p>What we’re going to do today:</p>

<ul>
<li>• Create an AWS Account</li>
<li>• Turn On AWS CloudTrail</li>
<li>• Turn On Programmatic Billing</li>
<li>• Create IAM Users and Groups</li>
<li>• Add MFA for New Users</li>
</ul>


<p><strong>Create your AWS Account</strong></p>

<p>It all starts here: <a href="http://aws.amazon.com/">aws.amazon.com</a>. Find the
big sign up button and just follow the prompts. A couple of things to
note before getting started:</p>

<ol>
<li><ol>
<li>It’ll prompt you for your information (name, email, address, etc)
and credit card info, so you should get that figured out first.</li>
</ol>
</li>
<li><ol>
<li>You’ll need to verify your account via a phone call, so have your
phone handy.</li>
</ol>
</li>
<li><ol>
<li>You don’t need to sign up for support just yet.</li>
</ol>
</li>
</ol>


<p>Once you’re signed up, just login into the AWS console. The console
allows you to interact with most AWS services. Most people will start
building their servers in the sky right away, but there’s a bit of
information you should probably know up front, and some account set up
we recommend before getting started. Let’s go over that first.</p>

<p><strong>What You Need To Know About AWS Before Setting Stuff Up</strong></p>

<p>Amazon Web Services offers a lot of different services, from virtual
computing instances and storage to transcoding and streaming. Going over
each service would take a whole series of blog posts, but an
understanding of how AWS is laid out will be helpful when getting
started.</p>

<p>AWS has data centers all over the world, and has two ways of grouping
them. At global scale there are <strong>regions</strong>, representing parts of or
entire continents. Inside each region are <strong>availability zones</strong>.
Regions are completely distinct entities, and you can only work in one
at a time. Availability zones are designed to talk to each other, and
AWS will automatically spread your resources across availability zones.
Availability zones, however, can only speak to other zones within the
same region.</p>

<p>Choosing a region is important, though these directions are the same
more-or-less in every region. However, be aware that not all services
are available in all regions, and pricing does vary by region. In
addition to that, US-East-1 is the “default” zone when you start with
AWS, and has been around the longest. For that reason, it’s also the
most popular, and sometimes you won’t be able to allocate resources in
certain Availability Zones in the US-East-1 region due to those zones
being at capacity.</p>

<p>AWS provides <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html">lots of
documentation</a>
on how to choose a region, so definitely look through that to decide the
best place to host your infrastructure. If you’re just doing initial
investigation into AWS and aren’t sure what region to use, just pick one
close to you.</p>

<p><strong>Making a Name For Yourself</strong></p>

<p>We’ll be talking about several AWS services in this section, and many of
them make use of AWS Simple Storage Service, or <strong>S3</strong>. S3 allows you to
store objects in the cloud with a high degree of durability. Where S3
objects are stored are called “buckets”. S3 bucket names have to be
unique, not just across you account, but across the entire world. A
bucket name is globally unique. By the time we’re done, we’ll have
created a couple buckets, as well globally unique login URL. For that
reason, you should come up with a unique identifier now. For example,
when we tested this documentation, we used the identifier
“stelligent-cdblueprints.” Just note it down now and we’ll refer to it
as we go on.</p>

<h4>Turn on CloudTrail</h4>

<p>First thing is to turn on CloudTrail. CloudTrail is basically logging
for your AWS account. It will generate JSON files and store them in an
S3 bucket (Amazon’s cloud storage solution) every time an action is
performed on the account. While we won’t be doing a lot with CloudTrail
right away, we’re turning it on now because it’s not retroactive — you
can only see logs after you’ve turned it on. So let’s turn it on first.</p>

<p>(Quick note: CloudTrail is a relatively new service, and at the time of
this writing is only available in two regions: US-East-1 and US-West-2.
If you’re using a different region, you might not be able to turn
CloudTrail on. If that’s the case, just skip on to the next step.)</p>

<ol>
<li>Find CloudTrail panel from the main AWS Console,</li>
<li>Click Get Started and just punch in an S3 Bucket name. (As was
mentioned above, the S3 bucket name has to be globally unique. One
approach is to take the unique identifier you came up with before,
and just append -cloudtrail to it. We’ve named our bucket
“stelligent-cdblueprints-cloudtrail”.)</li>
<li>Click OK and you’re done.</li>
</ol>


<p>That was easy.</p>

<h4>Turn on Programmatic Billing</h4>

<p>Next, we’ll want to turn on Programmatic Billing. This will store your
AWS billing in JSON files in another S3 bucket, so that other services
can analyze your spending and plot trends over time. We’ll be visiting
those kind of tools later on, but we want to enable programmatic billing
now because (just like CloudTrail) it only generates data from the
present — there’s no way to go back and generate historical data. By
turning it on now, when we do start parsing that data for trends, you’ll
have a good amount of data to go back through.</p>

<p>Unlike CloudTrail, you’ll need to create and permission the bucket for
this yourself.</p>

<ol>
<li>Go to the S3 console so we can create a new bucket. (Taking your
previous unique identifier and just appending -billing to it isn’t a
bad idea. We’ve named ours “stelligent-cdblueprints-billing” to keep
with the theme.)</li>
<li>Click Create Bucket and punch that name in.</li>
<li>We’ll need to get a bucket permissions policy. Luckily, AWS will
generate that for us at this page (we’ll need to flip back to the S3
page in a second, so open this in a new
tab): <a href="https://portal.aws.amazon.com/gp/aws/developer/account?ie=UTF8&amp;action=billing-preferences">https://portal.aws.amazon.com/gp/aws/developer/account?ie=UTF8&amp;action=billing-preferences</a></li>
<li>Go down the list and turn everything on one and a time.</li>
<li>When you get to to Programmatic billing, punch in the name of
your bucket, and click “sample policy.” Just copy that policy, then
flip back to your S3 bucket.</li>
<li>Click on the bucket, then properties, then Permissions, and
you’ll see an option for setting an access policy.</li>
<li>Click into that, paste the policy you just copied, and save.</li>
<li>Now, flip back to the Billing Preferences page, click save there</li>
<li>Continue to enable everything else on this page.</li>
</ol>


<h4>If CloudTrail and Programmatic Billing are so important, why aren’t they turned on by default?</h4>

<p>One thing to be aware of with these two services is that they will put
data into your S3 buckets. S3 storage is very cheap, and while it is
pretty close, it is not free. You’ll be paying between nine and fifteen
cents a gig for storage, depending on region. For more details, <a href="https://aws.amazon.com/s3/pricing/">check
out the S3 pricing page</a>. The
services themselves don’t cost anything, though; you only pay for
storing the data they generate.</p>

<h4>Create IAM Users</h4>

<p>Now that the bookkeeping is taken care of, let’s set up some users. A
lot of new AWS users will start doing everything as the root account,
which besides being a bit of a security risk, also poses some issues
when you try to have multiple developers building solutions in your
cloud. That’s why we strongly recommend setting up IAM users and roles
from the beginning.</p>

<p>We’re going to use the AWS Identity and Access Management (IAM) console.
IAM allows you to create users, groups, and roles so that you can manage
users and access to your AWS account. For the first section, we’ll only
be creating one user (for you) and one group (admins) but as your usage
of the cloud increases and you need to add more users, you’ll be able to
control that from here.</p>

<p>To create a new admins group, head to the IAM console</p>

<ol>
<li>Click Create Group, and follow the prompts.</li>
<li>We’ll name the group “admins” and give it Administrator access.</li>
</ol>


<p>Now that we have an admins group, go to the Users panel and create a new
user for yourself to log in as. It’s pretty straightforward, and if you
hit any bumps in the road, <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/Using_SettingUpUser.html">AWS has some pretty good documentation about
it</a>.</p>

<p>After you create the user, add it to the admins group. Then, for each
user we want to set up two types of authentication. The first is a
simple password. Under each users’ Security Credentials tab, click the
“Manage Passwords” button and you’ll be able to assign a password.</p>

<p>After each user logs in, you’ll want to require them to add a
multi-factor authentication (MFA) device to their account. To add an MFA
device</p>

<ol>
<li>the user will need to login and go to the IAM console</li>
<li>find their username</li>
<li>under the security credentials tab, select “Manage MFA device.”</li>
<li>Then follow the steps to add your virtual MFA device to the account.</li>
</ol>


<p>Having MFAs set up for all accounts helps ensure that AWS accounts won’t
be compromised, keeping your data safe. Also, it helps ensure that your
account won’t be used for malicious purposes (DDOS attacks, spam emails,
etc) which would at best would increase your AWS bill and worst case
have your entire account disabled. We strongly recommend enabling MFAs
for all user accounts.</p>

<p>Now that users are able to log in, we’ll need to give them a URL to do
so. If you go to the main IAM console, you’ll find a IAM User Sign-In
URL section. Remember the unique identifier you came up with your
CloudTrail and Programatic Billing buckets? That’s probably a good
option for your sign in URL. Changing it is optional, though highly
recommended.</p>

<p><strong>Wrapping Up</strong></p>

<p>Using AWS is easy; using it well takes some thought. By setting up
logging of your usage and billing information, you’ll be able to
identify trends as time goes on. By setting up groups and users, your
account is prepared to scale as you bring on more developers. And by
giving those users multi-factor authentication, you’ve helped ensure the
security of the account. You’re in a great place to start using the
cloud. In our next post, we’ll lay the foundations for building a
continuous delivery pipeline.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/03/25/autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-4/">Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 4</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-03-25T11:23:17-04:00" pubdate data-updated="true">Mar 25<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>Autoscaling for LAMP on AWS | Choosing an instance type: Part 4</h3>

<p>As mentioned in <a href="../autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-1/index.html" title="Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 1">part 1 of this
series</a> (Creating
a LAMP Stack AMI), a common concern among most customers is to choose
the right instance type.</p>

<p>It is important to do the capacity planning. Before you choose instance
type ask yourself the following three questions:</p>

<ol>
<li><p>Is the application Memory intensive CPU intensive or Network
intensive? For this question to be meaningful put a monitoring
system in place and collect the data for a few weeks of real usage.</p></li>
<li><p>What is the expected request count at peak hours?</p></li>
<li><p>What is the minimum number of instances you want to run in
non-business hours?</p></li>
</ol>


<p><a href="https://docs.google.com/a/flux7.com/spreadsheet/ccc?key=0AkmUqlScRGp8dGJTNndjY0VjTURid3FTV2dNMEljOUE#gid=0">https://docs.google.com/a/flux7.com/spreadsheet/ccc?key=0AkmUqlScRGp8dGJTNndjY0VjTURid3FTV2dNMEljOUE#gid=0</a></p>

<p>Make sure to run at least 2 servers for high availability.  During times
of fewer loads, choose two m1.small instances for non-business hours
like weekends. This would optimize cost instead of using 2 large
instances for the same request.</p>

<p>Different instance types have different capacity levels. If the
application is memory intensive choose and use m1. class. If the
application is CPU intensive then choose and use c1. class.</p>

<p>Network bandwidth varies between different instance types. If the
application requires more bandwidth (ex: video streaming applications)
choose higher instance types irrespective of Memory or CPU to get better
bandwidth.</p>

<p>To know more about each instance types visit the following link.</p>

<p><a href="http://aws.amazon.com/ec2/instance-types/">http://aws.amazon.com/ec2/instance-types/</a></p>

<p>Start with micro/small instance for any workload. Do the Load run
starting with the minimum number of users and increase the load
gradually. Also scale the servers vertically or horizontally gradually
until it reaches the maximum capacity. This would give a clearer picture
of the number of instances required to serve the maximum or minimum
load. Closely watch the CloudWatch graphs to understand usage statistics
better.</p>

<p>An interesting question at this juncture could be as follows: Can 4
m1.medium instances be preferred to 2 large instances? Yes of course.
However, network bandwidth varies from one instance type to other. Given
that, it is wise to choose 2 large instances instead of 4 m1.medium
instances as it is easier to handle lesser number of instances.</p>

<p>(Note: DB server health needs to be checked when a load test is run.
Increasing the app servers count may not improve the performance all the
time. If DB queries are the bottleneck, chances are high for a bad
performance. Consider scaling up the DB server capacity as well.)</p>

<p>Based on the instance type identified from the load run, tune the PHP
memory settings and Apache prefork MPM client connections.</p>

<p>Check out the following links to know more about fine tuning Apache and
PHP</p>

<p><a href="http://www.hosting.com/support/linux/tuning-the-apache-prefork-mpm/">http://www.hosting.com/support/linux/tuning-the-apache-prefork-mpm/</a></p>

<p><a href="http://icreatestuff.co.uk/blog/article/apache-performance-tuning">http://icreatestuff.co.uk/blog/article/apache-performance-tuning</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/03/25/autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-3/">Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 3</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-03-25T11:23:14-04:00" pubdate data-updated="true">Mar 25<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>Autoscaling for LAMP on AWS | Load Test the Environment: Part 3</h3>

<p>In part 3 of the Autoscaling for LAMP on AWS series,</p>

<p>After setting up your application autoscaling, it’s important to load
run the application in order to understand the minimum and maximum
number of instances required for each application.</p>

<p><a href="../autoscaling-for-lamp-on-aws-setting-up-austoscaling-groups-part-2/index.html" title="Autoscaling for LAMP on AWS | Setting Up Austoscaling Groups : Part 2">Read Part 2 to learn more about setting up autoscaling
groups.</a></p>

<p>First, fire the load run. For this article we’ve used HP LoadRunner
because it provides more detailed results than others, but there are
also other load runner tools to choose from. Be sure to run the load
from multiple IP addresses or else the ELB may not distribute it equally
across all instances. HP LoadRunner helps you use multiple nodes in
order to generate the load.</p>

<p>Update the Auto Scaling Group’s minimum and maximum instance counts.\</p>

<p><img src="https://lh5.googleusercontent.com/nXhTs4kEqjzxfAy8aGdgqLvC_yKninSiyM3pClGWhemLXPWFjr_6Y5NU8xnf2j7twuJgVx2TO4tvs8fjm2K1c8y6C23NN8lacgef-JEH4621AXZITbYAAKRF4w" alt="" /></p>

<p>For the this article, we’ve set the minimum and maximum instance counts
at 2 and 8 respectively.</p>

<p>Now change your Auto Scaling Policies. A scaling policy specifies
whether to scale the auto scaling group up or down, and by how much.
Here we’ve chosen 2 instances for Scale Up and Scale Down policies.
These policies automatically increase or decrease the instances
according to your defined conditions in order to maintain performance
and minimize cost.</p>

<p><img src="https://lh3.googleusercontent.com/3G9JdQa1GNjF_2fWOrZ10y-cpc3m8_6gtTH2Y046OK3LoiaMYGx0Nm_N4LsBcBhhRhA3cxr7D1sMlCv90ewW9rZ8QCr8amOHqivVcZZmML7Bx-XMIeM7wdN1Nw" alt="" /></p>

<p>Next, <strong>increase the load</strong> gradually with LoadRunner. Once the CPU
reaches 75% of the instances, Autoscaling will trigger the scale up
policy to launch 2 more instances.</p>

<p>Now <strong>decrease the load</strong> gradually and the CPU instances load should
come down to less than 45%. Autoscaling will then trigger the scale down
policy and delete the additional instances that were launched.</p>

<p>Consider a scenario in which a 2,000-concurrent-request count is reached
during peak hours and 500 users are reached during non-peak hours. For
best load run results, try the following.</p>

<p>Capture load test results and CloudWatch metrics for each of the
following scenarios:</p>

<p>​1. Start the load test with 500 users. Set the minimum instance count
to 2 if you choose large or medium types, and set the maximum instance
count to 4.</p>

<p>​2. Increase the load to 1,000 users. Set the minimum and maximum
instance counts to 2 and 6 respectively.</p>

<p>​3. Increase the load to 2,000 users. Set the minimum and maximum
instance counts to 4 and 10 respectively.</p>

<p>​4. Increase the load to 3,000 users. Set the minimum and maximum
instance counts to 6 and 12 respectively.</p>

<p>Analyze the results for each scenario and note which gives better
throughput and response times. Also check the DB metrics.</p>

<p>Now it will be easy to choose the best instance type and max/min
instance counts..</p>

<p><strong>Shutdown Autoscaling</strong></p>

<p>Update the autoscale group to resize the instance count to 0. This will
terminate all instances launched within the lamp-asg autoscale group.</p>

<p><em>Watch out for the final part of this series tomorrow on how to choose
an instance type. </em></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/03/25/autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-2/">Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 2</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-03-25T11:23:10-04:00" pubdate data-updated="true">Mar 25<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>Autoscaling for LAMP on AWS | Setting Up Austoscaling Groups : Part 2</h3>

<p>In part 2 of the Autoscaling LAMP in AWS series, let’s discuss how to
create autoscaling launch configuration, autoscaling groups and how to
verify the setup autoscaling.</p>

<h2><strong>Autoscale Implementation</strong> </h2>

<p>Autoscale configuration is now available in console. AWS command lines
are no longer needed for implementation.</p>

<p>Complete the following steps as detailed in <a href="../autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-1/index.html" title="Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 1">Part
1</a>of
this series in order to set up autoscaling :</p>

<ul>
<li><p>Configure AMI to launch the Instances.</p></li>
<li><p>Configure Instance type to launch the instances. (Example:
m1.small,m1.large).</p></li>
<li><p>Configure KeyPair Name to access the machines.</p></li>
<li><p>Configure Security Group to allow the Instances to communicate with
other components.</p></li>
<li><p>Keep the ELB name readily available.</p></li>
<li><p>Keep your availability zones ready. (Example: us-east-1a,
us-east-1b).</p></li>
<li><p>Set the minimum number of instances for Maximum and Desired
Capacity. (Start with zero).</p></li>
<li><p>Set Health Check Type. (ELB).</p></li>
<li><p>Set Region.</p></li>
<li><p>Change Capacity Cooldown time.</p></li>
<li><p>Adjust for scale up and scale down.</p></li>
</ul>


<h2><strong>1. Create the Autoscaling Launch Config</strong> </h2>

<p>Log in to the AWS console and navigate to Services-> EC2-> Launch
Configuration.</p>

<p><img src="https://lh5.googleusercontent.com/-E7LhgP_GnMeKXQHtHPieeiqZBbrD3TauY5FHvA90y2hvnWMoncO5gEJASGmYd_v1HjDIzIr949-gjX7N2DlCtzCyO5iwZnUd481qEyiHKVkxhH40xDL3xiTzQ" alt="" /></p>

<p>Click on “Create Autoscaling Group”.\</p>

<p><img src="https://lh6.googleusercontent.com/8MJM4YJXpOnamUnIZdfPkHZDJJ_iyVXg1qj373NTmPKZbihu4C2nqYcm2zvCacxycBN1OjzxbpANVSI7W4t7a8LYo15FbooKxx0UlXXkpJwuWxSNX7ZfSYm-PA" alt="" /></p>

<p>On the next screen, click on “Create Launch Configuration”.\</p>

<p><img src="https://lh6.googleusercontent.com/8h6Tn5ncvwmpG6qqI5vqulA_qoG6ijs4wtOOWA6nDwuCUCF1_OpD1VSWhlJDimgNNyhp_dmEsVx4DNJkls043s_CbBJ3HK0VUsSUphYjNeM3Qnlx1clgXZ5jAA" alt="" /></p>

<p>Create a new launch configuration. The name of the launch configuration
must be unique within the scope of the client’s AWS account.</p>

<p><strong>Choose AMI:</strong> Go to My AMIs and select the LAMP AMI created.\</p>

<p><img src="https://lh4.googleusercontent.com/SJXQcn4aWSm83eKHuKKZHJomaUvLwJShg8w5M1Q6wEXXNiUv61JNoiaKMYlaacP4TYT8kTvJju2vDKK23RReWFLgLxRGb68xaSuMbHzjQ0H74evkNtINH_T_xQ" alt="" /></p>

<p><strong>Choose Instance Type:</strong> We’ve selected a micro instance for our
example.\</p>

<p><img src="https://lh3.googleusercontent.com/e7lh5P4VR8i9qJ5QgxUK1PBN6in9HQNpU0cjznY6Giemq8RZrF2wBEfmlL_cCS_cDUBKKUHkmU__YC6YPiNncXXhbXMq9BRE3tLSbDewJR8MZ8upoWi0OwIp2w" alt="" /></p>

<p><strong>Configure Details:</strong> Give a name for the Launch Configuration\</p>

<p><img src="https://lh4.googleusercontent.com/kP646CGpnJLDO-44macZIU6A4FMSeLpu8kSq6hpFIstb_heGHWMCRwtn5EpOw8YZdvCANbd-5Pur5SNw_OaWg7432_WdQ1Rbi9QUsR6FMUIo71Y6REwKA5wJ2w" alt="" /></p>

<p><strong>Add Storage:</strong> Keep the values on default.</p>

<p><strong>Configure the Security Group:</strong>Select the Security group to launch the
autoscaling instances. Review the details and create the Launch
Configuration.\</p>

<p><img src="https://lh5.googleusercontent.com/rd0tjYvJyMzG0irkmsj9LZSXrxigf93tfOhOLUtcrQ-c1Pk2XXsOF7ksRbu4BY_6PvPfMWqC9bF4NZcy1aNLBOfUd91vjV9shjfA9IRAA5MNSJXb6sPuVa8gHA" alt="" /></p>

<p>In the next window, select “KeyPair” to access the instances.</p>

<h2><strong>2. Create the Auto Scaling Group</strong> </h2>

<p>Create a new Auto Scaling group with a specified name and other
attributes. When you make the creation request, the Auto Scaling group
is ready for use in other calls.</p>

<p>Navigate to EC2-> Auto Scaling Groups-> Create Autoscaling Group.</p>

<p>Select the existing Launch Configuration (lamp-launch-1) and go to “Next
Step”.\</p>

<p><img src="https://lh4.googleusercontent.com/N9-2P0wrXtd1IKWO58H1i7s6MRx0w_toDG5wjmtvsw0v5wUamXz6oIC88g-YYqnP5H9OTobjDc_X9Hf1shLrn_5tTO4HLuHHUomu8qE8OYv3oLEDp02urv0U4g" alt="" /></p>

<p><strong>Configure Autoscaling Group Details:</strong></p>

<p>Give a name to the Auto Scaling Group.</p>

<p>Group Size: Start with zero instances in order to avoid the immediate
creation of instances. The exact number of required instances can be set
after these steps are completed, but just set cost optimization values
to 0 for now.</p>

<p><strong>Availability Zones:</strong></p>

<p>Choose 2 availability zones in order to maintain high availability.</p>

<p>In “Advanced Details” choose the values as shown below.\</p>

<p><img src="https://lh3.googleusercontent.com/YL14QA7srKzFlnQLXuXtIxo0wrB51aDQuOj8fR5YXDYErg2usKCEaKJ57fKN6UahjFP0KFOTkr_y8odlPVWqDh2z3OMwy_iUCRJ5ubiQ4EsOTTNSLKbz8IU2UQ" alt="" /></p>

<p><strong>Configure Scaling Policies:</strong> Keep minimum and maximum instances count
to zero.</p>

<p><strong>Increase Group Size:</strong> Keep the name at default.</p>

<p><strong>Execute Policy:</strong> Click on “Add new alarm”.\</p>

<p><img src="https://lh3.googleusercontent.com/ysaxf1OR2wazrUt1lvdkYI-YRoVdijKoJD4m6DDlGOHld-LO_mtz24-kOmvyTHybJPIEK_VOKV_UC5ybIwwt9jKtJhZ_3N3J1B_4e5P1iAyIsynEpODx6S4gKQ" alt="" /></p>

<p>A popup window will appear for creating a “Cloud Watch Alarm”.\</p>

<p><img src="https://lh3.googleusercontent.com/eqvhUKlpyNKv7vWQiAI95g5oUQxp9KpqQJ0WCEJjFNfCfn-PIBtaYEPf7k8T2BrlCxN8EoF6utdR5c67tMMk-K0CW-upVZIKR1jOTumOoiwic5fEvZndn8_aEA" alt="" /></p>

<p>For this example, we chose CPU Load average as the autoscaling trigger.
Whenever the average CPU load of app servers goes beyond 75% for 5
minutes, autoscaling will trigger the auto-scale-up-policy to launch the
1 instance and attach to load balancer. The application here is CPU
intensive and requires more computing power, so we chose the CPU Load
Average as the autoscale triggering event.</p>

<p>You can choose any Cloudwatch metric to trigger the autoscaling policy.
For example, Disk read/writes, Network In/Out, ELB request count, ELB
latency, etc.</p>

<p><strong>Decrease Group size:</strong>\</p>

<p><img src="https://lh3.googleusercontent.com/wxwMuiWm-72K4zGTx-oT56LdXvjhQXNQZPLg_HXPMb89ey5CEtmOV76euYKqLwbYKm88kOAlmSrW6xfRImjIefcQ7LVFOcUIRoyZ5t3xF_JffVy7R96Ypk8lMg" alt="" /></p>

<p><strong>Configure Notifications:</strong> If you’ve already created notifications,
select one to receive the notifications of Autoscaling events, else skip
this step for now.</p>

<p>Review the details and click on “Create Autoscaling Group”.</p>

<p>Auto Scaling evaluates the health of each Amazon EC2 instance in the
Auto Scaling Group and automatically replaces unhealthy instances in
order to keep the Auto Scaling Group size fixed. That ensures that the
application is getting the expected compute capacity.</p>

<p>In this example, we’ve chosen to scale down the environment when the
average CPU load lowers to 40% for 5 minutes.</p>

<h2><strong>Verify Autoscaling</strong> </h2>

<p>After completing the previous steps, it’s time to test autoscaling.</p>

<p>​1. Update the Autoscaling group minimum and maximum instances count to
2 and 4. You can change these later to fit your specific requirements.</p>

<p>Navigate to EC2-> Auto Scaling Group- > lamp-asg-1.</p>

<p>Right click on “lamp-asg-1” and select “Edit”.</p>

<p>In the “Details” tab, update Desired Min and Max values to 1, and then
save.\</p>

<p><img src="https://lh5.googleusercontent.com/hxxgnuVeJGgoMHPJvsr5oTeghIkuhJv4_hX0U1YtdjpO57N84IN4KmuSk4cO9cYeTH7JiSD9EMVAXCwS8z5v0-lZ3Gf4jW67oGNflBhgr10tiUVHbn9lpjE8oQ" alt="" /></p>

<p>Now, navigate to Services->EC2->Instances.</p>

<p>You can see that a new instance has been launched and attached to ELB.\</p>

<p><img src="https://lh3.googleusercontent.com/GPJHyEzUrO4LtbxCSzT6rHiz_RLPIIkmeIS--__XDuzFiW2dLSZwZhSwkvHG2KvW5CJ6Kx8DpvjEXo8Xgbw7Q8HBa6cfNUu2Mf6XAM6jpx-8LVKxEUMNeXCVHA" alt="" /></p>

<p>\</p>

<p><img src="https://lh3.googleusercontent.com/9_cNWARusMs6T9rypbPZWakid_pmJpMwBWLXuPhwaRlpiXLU5XFjQDGzBAYbPTtjSiupY8tELI5s8ROwgZtKCvfKhxVsagLXYoKCl_MnWqvnqrOGAoqj-wHGOg" alt="" /></p>

<p>If your health check is configured properly, the instance status will
turn into “In Service” rather quickly. Always have a minimum of 2
instances running in order to maintain high availability.</p>

<p>​2. How to Update Launch Configuration: It’s not possible to update the
existing Launch configuration, so you’ll have to create a new Launch
configuration and edit the Autoscaling Group to use that new
configuration.</p>

<p><img src="https://lh5.googleusercontent.com/_bDZv08DtcohHQkG5s0e2_0rd3VnkzImL-87T5wfku5cmvL69tgMhMwKc71Vo-zgVbVImT-HaD0f_fo7dcv5F0JmkbXZGpllnhldKILcPVdgYR7nKZ0qaRniwQ" alt="" /></p>

<p>Now, start the application load run and find out the minimum and maximum
instances required for your application to handle the load. Then update
the AutoScaling Group to meet your needs.</p>

<p>Watch out for  Part 3 of this series to understand <a href="../autoscaling-for-lamp-on-aws-load-test-the-environment-part-3/index.html" title="Autoscaling for LAMP on AWS | Load Test the Environment: Part 3">how to load run an
application</a>.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/03/25/autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-1/">Autoscaling for LAMP on AWS : Creating a LAMP Stack AMI : Part 1</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-03-25T11:23:04-04:00" pubdate data-updated="true">Mar 25<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 1</h3>

<p>This is part 1 of the Autoscaling for LAMP in AWS series. The
step-by-step guide would walk you through</p>

<p><a href="index.html" title="Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 1">Part 1: Creating a LAMP Stack
AMI</a></p>

<p><a href="../autoscaling-for-lamp-on-aws-setting-up-austoscaling-groups-part-2/index.html" title="Autoscaling for LAMP on AWS | Setting Up Austoscaling Groups : Part 2">Part 2: Setting up Autoscaling
Groups. </a></p>

<p><a href="../autoscaling-for-lamp-on-aws-load-test-the-environment-part-3/index.html" title="Autoscaling for LAMP on AWS | Load Test the Environment: Part 3">Part 3: Load Test the
environment</a></p>

<p><a href="../autoscaling-for-lamp-on-aws-choosing-an-instance-type-part-4/index.html" title="Autoscaling for LAMP on AWS | Choosing an instance type: Part 4">Part 4: Choose instance
type</a></p>

<p>First, let’s discuss how to prepare the AMI, create an ELB and RDS,
verify the application and terminate the instance.</p>

<h2><strong>Prepare AMI</strong> </h2>

<p>In Autoscaling, create an AMI with all required packages installed. This
AMI will be used as a template to launch instances in autoscaling. For
LAMP Stack, you should install and fine tune the latest versions of
Apache and PHP.</p>

<p>​1. Log in to AWS console.</p>

<p>​2. Navigate to EC2 services. Make sure to switch to the desired region
before launching instances.</p>

<p>​3. Click on “Launch instance”.</p>

<p><img src="https://lh4.googleusercontent.com/hRoQeC1-aimLHiAt0ptacAxdMqlX8w0N19bEmMo8vm31e0n3-PRzWJLd6gg5TUn2FgokMj6n9y72S6gE_XKS0jNx9tHGv5kqD9TKz5qM0vYhu9TIF5FBAG6C-Q" alt="" /></p>

<p>​4. Select your favorite <strong>Linux AMI</strong> in Classic Wizard.</p>

<p><img src="https://lh6.googleusercontent.com/aWfL4g9PfAqaFTg3uI0Ntv52hSxUyPCNKN5Sf8Y4R5zYyjGOmHeA2zi6pGk59F9NiNJa16n27rSk111xlUJUt24oCLPdQfBZMr0k5yhCZpQ6B_ibxzU9hQRKeA" alt="" /></p>

<p>For this article we used CentOS 6.3 from the community AMIs, but
OpenSuse and Ubuntu are also good choices.</p>

<p>​5. Launch a <strong>micro</strong> instance from the selected AMI with the desired
<strong>Security group</strong> and <strong>Key pair</strong>. Remember, this instance is only for
creating an AMI and will be terminated once <strong>LAMP Stack</strong> is installed
on it.</p>

<p><img src="https://lh3.googleusercontent.com/_U5YNCpt3CelshRRvlBG8CLKp8tYd64u1lwFWqqZcN48v3UjL9GBOfHoxL3EXJ9pXOk3tNz0Ig049xbwJa4gIsNvr4Z869PFqwkSfBDkJ8GK1PU9olC7ktBn7w" alt="" /></p>

<p>​6. Now log in to the instance and perform the <strong>PHP</strong> and Apache
installation steps. For this article, we used <strong>Cygwin</strong> to connect the
<strong>Linux</strong> servers. Make sure you open port 22 to allow <strong>SSH</strong> access
using security groups.</p>

<p><img src="https://lh4.googleusercontent.com/Al62wHOtPAxqNRUh_kr47FmQqMSycjLKMJ9zSz47OfIWUVDENcSi6ypdoBaiABQ9Rlr33kid0Be8a-1KuyLDYhFbea4Fw4rdJKxGSoDfbQZP6iClUbDTnUmFGA" alt="" /></p>

<p>We modified the “default” security group to open the SSH port. We
recommend that you not completely open port 22 for SSH, but rather allow
incoming requests to SSH from your specific IP address. In our case the
IP address was 49.206.166.12.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ssh –I LAMP_AMI.pem root@ec2-54-254-37-17.ap-southeast-1.compute.amazonaws.commailto:root@ec2-54-254-37-17.ap-southeast-1.compute.amazonaws.com
</span><span class='line'>mailto:root@ec2-54-254-37-17.ap-southeast-1.compute.amazonaws.com
</span><span class='line'>$ yum install httpd.x86_64 httpd-devel.x86_64 mod_ssl.x86_64 php.x86_64 php-common.x86_64 php-devel.x86_64 php-gd.x86_64 php-mbstring.x86_64 php-mysql.x86_64 php-pdo.x86_64 php-soap.x86_64 php-xml.x86_64 php-xmlrpc.x86_64 php-pecl-apc-devel.x86_64 php-pecl-memcache.x86_64 -y</span></code></pre></td></tr></table></div></figure>


<p>Once you’ve successfully executed these commands, Apache and PHP will
install. Note that the commands we’ve used are specific to CentOS 6.3.
Choosing a different OS or a different CentOS may cause these commands
to fail to execute. T<strong>he php modules listed above are required for
WordPress setup and they may vary depending upon your requirements.</strong></p>

<p>Now update the PHP and Apache configurations for your Instance types and
for the expected load on application. We used t1.Micro for this article.
Keep in mind that a production system’s instance types need to be chosen
very carefully, and that the software must be tuned to those particular
types.</p>

<p><strong><em>Read more on selecting instance types in <a href="../autoscaling-for-lamp-on-aws-choosing-an-instance-type-part-4/index.html" title="Autoscaling for LAMP on AWS | Choosing an instance type: Part 4">Part 4 of this
series.</a></em></strong></p>

<p>Install Postfix to send email notification for the script used in the
next step.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ yum install postfix  
</span><span class='line'>$ chkconfig –level 2345 postfix on
</span><span class='line'>$ chkconfig –level 2345 httpd off</span></code></pre></td></tr></table></div></figure>


<p>​7. Now all of the required packages are installed and fine-tuned.</p>

<p>So, how does one deploy the code into AMI?</p>

<p>The method described here automates the deployment. One cron job will be
included in AMI, and it will download and deploy the latest available S3
code. That means that code updates will deploy to S3 every time, but not
to instances. Every time a new instance is up, the instance will
automatically download updates from the S3 bucket and deploy them.</p>

<p>What follows is a script scheduled for machine boot-up. It requires
updating based on specific user configurations and file paths.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/bash
</span><span class='line'>S3_URL= “http://s3-ap-southeast-1.amazonaws.com/lamp-deploy/wordpress.zip” #Change the following paths as per your requirement.
</span><span class='line'>DOCUMENT_ROOT=&lt;/var/www/html&gt;
</span><span class='line'>TEMP_LOC=&lt;/tmp&gt;
</span><span class='line'>APACHE_RESTART=`/etc/init.d/httpd restart`
</span><span class='line'>#Get the instance ID from AWS metadata.
</span><span class='line'>INSTANCE_ID=` curl http://169.254.169.254/latest/meta-data/instance-id`
</span><span class='line'>###download the latest code from s3 and deploy###
</span><span class='line'>cd /tmp
</span><span class='line'>wget $S3_URL
</span><span class='line'>if [ $? –ne 0 ]
</span><span class='line'>then
</span><span class='line'>pkill -9 httpd
</span><span class='line'>mail –s “Code Download Failed from S3##$INSTANCE_ID”  test@test.com #Provide valid email ID here”
</span><span class='line'>exit 1
</span><span class='line'>fi
</span><span class='line'>unzip wordpress.zip
</span><span class='line'>rm –rf  &lt;/var/www/html/*&gt;
</span><span class='line'>cp –r wordpress/* $DOCUMENT_ROOT
</span><span class='line'>chmod 775 –R $DOCUMENT_ROOT
</span><span class='line'>chown –R apache.root $DOCUMENT_ROOT
</span><span class='line'>$APACHE_RESTART
</span><span class='line'>if [ $? –ne 0 ]
</span><span class='line'>then
</span><span class='line'>pkill -9 httpd
</span><span class='line'>mail –s “Apache startup Failed- $INSTANCE_ID”  test@test.com #Provide valid email ID here”
</span><span class='line'>exit 1
</span><span class='line'>fi
</span><span class='line'>##script ends here####</span></code></pre></td></tr></table></div></figure>


<p>Put this bash script into a file in /opt/ directory:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$vim /opt/auto-deploy.sh
</span><span class='line'>$chmod +x /opt/auto-deploy.sh</span></code></pre></td></tr></table></div></figure>


<p>Open the Crontab and add this script to start at machine boot up:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ crontab –e</span></code></pre></td></tr></table></div></figure>


<p>Add the following script:</p>

<p><img src="https://lh4.googleusercontent.com/ZiqV0hI1ocrhmDqqLubt01OQdBcmJZqWhNciW6sbPfEz2Iwr99cxoNK4U5actZ6Vqdqat8asF-IwxjJQrVKiutlBZq6A74t9sp1wA0ezqCDbSR09YOjTQrJblX_8VShJfMA" alt="" /></p>

<p>Save your changes and deploy the code by running the script to test
whether or not the application is working as expected. This step is
mandatory before creating an AMI. If you find any issues with the code
or PHP/Apache settings, be sure to fix them before creating an AMI.</p>

<p>Bundle and Upload the Code to S3</p>

<ol>
<li><p>Update the code with RDS DB configuration parameters.</p></li>
<li><p>Now zip the working copy of the code and upload it to the S3
location. The S3 location should match the location in the script,
and also match the zip file name.</p></li>
<li><p>Upload the zip file to the S3 bucket that matches the script. You
can upload the zip file by using the console or any third party tool
like Cloudberry.</p></li>
<li><p>Grant read permissions to download the code from S3 to app servers.</p></li>
</ol>


<h2><strong>Create the ELB</strong> </h2>

<p>Go to Services-> EC2 &ndash;> Load Balancers and select “<strong>Create Load
Balancer</strong>”.</p>

<p><img src="https://lh5.googleusercontent.com/w-ZRahemXgWVTJcIwpwJYfdXgp7kcmUs23IYzjgdEpwUuYiCkoLJg2dnLs0-5fe8U0UFG1_3qyaNggPesGGHGUq1PwQH5MMo-QowBXfuhGSNthk65DDg5Oc1OA" alt="" /></p>

<p><img src="https://lh5.googleusercontent.com/JU0AmjRaIgFbWa8mdGz2qTlbYTU7wKWiOa93uUVuu-RDO5PPfEVhV1ggUBZ95rr5_z637SffLdwYgHb5srqZlj9GfcL-or3aG3xWsqb9Sd4kJ24LKZ8_84d_YA" alt="" /></p>

<p>Keep Healthy/Unhealthy threshold limits to 4 because that’s proven to be
an ideal value. Also make sure index.html is available in the Document
root. ELB looks for the index.html as a health check and removes the
instance from the load balancer if it doesn’t get the 200 response.
Create the required security groups and key pair for the autoscaling
instances.</p>

<h2><strong>Create RDS</strong> </h2>

<p>​1. Go to Services &ndash;> RDS and select the <strong>MySQL</strong> engine.</p>

<p><img src="https://lh5.googleusercontent.com/dgMcoX4gkePTWwEYT8soz9RdvW3iMPmmlVlnkgczHmwIDpfmh39BhRUmWRBEdbR0f6FMJi3LtaSW6z_sRhBmk_WYLfls0bv8VQ8r9PFPl52WOX0B12-vCLKBEQ" alt="" /></p>

<p>​2. Select instance types, provide the details for all required fields
and launch the instance. The RDS configuration details chosen for this
article can be seen below.</p>

<p>How should you select the DB instance type? There are many monitoring
tools available for monitoring MySQL performance. If AWS RDS is used for
MySQL, then it’s important to consider CloudWatch metrics. Since it’s
not possible to install agents in RDS, you can rely upon CloudWatch
metrics.</p>

<p>By default, CloudWatch contains all required RDS monitoring metrics. Be
sure to monitor the memory, CPU, DB connections and network I/O before
you scale up the server.</p>

<p>Ideally, m1.small RDS can handle up to 75 concurrent connections easily.
but that depends upon the DB queries and the code.</p>

<p>​3. After launching RDS, open the 3306 port to the app servers group.</p>

<p>​4. Import the DB and try connecting to the application. Then use
standard MySQL commands to import the dump.</p>

<p><strong>Example:</strong> mysql –h rds-endpoint –u user –ppasswwd databasename &lt;
dump.sql</p>

<h2><strong>Verify the Application</strong> </h2>

<p>At this point, it’s mandatory that you thoroughly verify the
application. Make sure to check Login/logout, upload/download and any
other relevant verifications. Create an AMI only after making sure that
the application is working as expected. Fix any issues before proceeding
to the next step.</p>

<h3><strong>Create an AMI</strong></h3>

<p>​1. Delete the command history:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ history -c</span></code></pre></td></tr></table></div></figure>


<p>Delete the deployed code and logs files and make things as clean as
possible.</p>

<p>​2. Create an AMI using the micro instance that you’ve configured.</p>

<p>Go to AWS Console->Services->EC2, right click on the instance, and
select “Create Image” (EBS AMI).</p>

<p><img src="https://lh4.googleusercontent.com/pR5iFmQYgO6FygwUB1SNh52dtHhrTr4TDk_G9fd6wlh1ds3V5gnbvhqYdn4yPOPRuQ7JZXwO54xuguOEoczq9yhVx1gsuHuZbVmCRgVFbida6ciJxv0EhNhwdQ" alt="" /></p>

<p>Name the AMI using a date as the naming convention. <strong>Do not</strong> select
“<strong>No reboot</strong>”. Instead, allow it to reboot while creating the AMI. The
reboot will give you a consistent snapshot of EBS.</p>

<p><img src="https://lh5.googleusercontent.com/5vWSfAF_enz-JCR8PlW-hDts2pvtUSI8yRFYkx8H27K0UdRCntJjTgcHGODGJpiGkXfFpe-XGva-EZUFF5gEDRcPcw5o8xc_8n1ueqYQaK8hpY9WolFLUdrHRw" alt="" /></p>

<p>In a few minutes an AMI will be created and made available.</p>

<p>Keep the root partition as minimal as possible because that helps to
prevent bugs. You don’t need large space for root partition in autoscale
because these instances keep rotating.</p>

<h2><strong>Terminate the Instance</strong> </h2>

<p>Finally, terminate the instance by right clicking on the instance and
selecting “Terminate”. Keep the ELB and RDS running.</p>

<p><strong><em>Watch out for Part 2, same time tomorrow, on how to setup autoscaling
groups!</em></strong></p>

<p><strong>Update</strong>: Check part 2 on how to setup autoscaling groups
<a href="../autoscaling-for-lamp-on-aws-setting-up-austoscaling-groups-part-2/index.html" title="Autoscaling for LAMP on AWS | Setting Up Austoscaling Groups : Part 2">here</a>.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/03/25/a-deep-dive-into-aws-reserved-instances/">A Deep Dive Into AWS Reserved Instances</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-03-25T09:35:27-04:00" pubdate data-updated="true">Mar 25<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>A Deep Dive into AWS Reserved Instances</h3>

<p>One of our primary goals at Flux7 Labs is to help our clients reduce
their AWS costs. In fact, our product VyScale is based entirely on cost
optimization using Spot instances. We inform our clients when it makes
economic sense for them to buy instance reservations because
reservations for periods of unexpected minimum usage can be beneficial.
Reserved instances function exactly like on-demand instances, except
that you pay an upfront fee to gain cheaper hourly rates.</p>

<h3>Reserved Instance Pricing</h3>

<p>There are several levels of reservations. Higher ones allow you to pay
more up front in order to achieve a lower hourly cost. The table below
shows the rates for various types of reserved and on-demand instances
for m1.large instances.</p>

<hr />

<pre><code>           Upfront   Hourly in cents
</code></pre>

<p>  On-demand    0         24
  Light Util   243       13.6
  Med Util     554       8.4
  Heavy Util   676       5.6</p>

<hr />

<p>At some levels of utilization it makes sense to purchase reservations,
rather than to rely solely upon on-demand instances. By factoring in
upfront costs, we can determine which levels warrant purchasing
reservations. What’s surprising is that those levels are fairly low,
especially in the case of light reservations. Even at 30% utilization,
light-reservation costs start to break even with those of on-demand
instances. The following graph shows equivalent hourly costs for
reservations at various utilization levels</p>

<p><a href="../../../wp-content/uploads/AWS-reserved-instances-1.png"><img src="../../../wp-content/uploads/AWS-reserved-instances-1.png" alt="Hourl rates of reserved
instances" /></a></p>

<p>To better understand these numbers in terms of total cost, as opposed to
incremental cost, the figure below shows total annual expenditures at
various levels. For a 100% utilization of an instance, one can reduce
costs to almost half of Amazon’s standard pricing for reserved
instances, and that’s without any of the bulk discounts made available
to high-volume AWS customers. At lower levels, reservations cost almost
twice as much as those at higher levels. However, keep in mind that, as
a fraction of overall cost, the amount is still not very high. So, in
cases where growth is expected, it can make sense to purchase
reservations early.</p>

<p><a href="../../../wp-content/uploads/AWS-reserved-instances-2.png"><img src="../../../wp-content/uploads/AWS-reserved-instances-2.png" alt="Annual rates of reserved
instances" /></a></p>

<h3>Do You Have A Reservation?</h3>

<p>Another thing to know about Amazon’s policy is that you receive a
guaranteed instance when you buy a reservation, whereas there is no such
guarantee for on-demand instances. When trying to acquire an on-demand
instance, AWS may return an error message stating that capacity is not
available for that instance type. On the other hand, once you purchase a
reservation Amazon guarantees that the instance will be made available
to you at the moment you request it. If you’re using an autoscaling
solution it can make sense to make light utilization reservations to
handle your excess capacity, even if it costs more to guarantee uptime
to your customers. With Amazon’s guarantee it’s no wonder that Netflix
runs almost exclusively on reservations. As a matter of policy,
Netflix’s use of on-demand instances indicates that more reservations
need to be purchased.</p>

<p>One thing to note is that Amazon always uses reservations first, so you
can’t keep unused light reservations as backups while using on-demand
instances for capacity.</p>

<h3>How Amazon Handles Unused Reservations</h3>

<p>Spot instances are one of the best uses of unutilized reserved capacity
with AWS. They come with no guarantee of availability, and can be taken
away from you at Amazon’s discretion at any time in order to fulfill
other customers’ needs. Spot prices hover around 15% of the on-demand
price, which allows Amazon to make a decent return on unused
reservations and while offering reservations at a relatively low price.</p>

<h3>Conclusion</h3>

<p>The most common reason to use reservations is price, but there are also
other considerations. The final decision should be based on capacity
analysis from a business perspective.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/03/25/6-reasons-why-large-enterprises-should-move-to-amazon-web-services/">6 Reasons Why Large Enterprises Should Move to Amazon Web Services</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-03-25T09:13:48-04:00" pubdate data-updated="true">Mar 25<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>6 reasons why large enterprises should move to Amazon Web Services</h2>

<h3>6 reasons why large enterprises should move to Amazon Web Services</h3>

<p>Amazon has changed the face of the world of startups with its cloud services. Now it’s possible for two men in a garage to set up large
computer clusters for zero capital cost.</p>

<p><a href="https://twitter.com/share?text=Amazon+has+its+sights+set+on+enterprise,+which+they+will+conquer+slowly+but+surely.+&amp;via=OurLabs&amp;related=Flux7Labs&amp;url=http://wp.me/p4sEOD-o6">Amazon has its sights set on enterprise, which they will conquer slowly
but
surely.</a></p>

<p>At Our Labs, one of our specialties is cloud migration for enterprise
clients, and we’ve received considerable feedback about both their
concerns and delights. In this post I’ll explain why I strongly believe
large enterprises should consider moving to the cloud.</p>

<h2><strong>Agility and Responsiveness</strong> </h2>

<p>Let’s face it, if a startup is a speedboat, then a large enterprise is
the Titanic. While startups can pivot any time there’s danger and large
enterprises turn more slowly, there are icebergs that can sink either.
Disasters happen to everyone, but a large enterprise has significantly
more skin in the game, and the old model of waiting a month before
buying a new rack just doesn’t cut it anymore. By shifting to the cloud,
large enterprises can bring up new servers in minutes, which means
shorter downtimes, rapid experimentation, more innovation, increased
global reach, improved demand-surge handling and more successful
short-lived-but-resource-intensive projects.</p>

<h2><strong>Innovation</strong> </h2>

<p>Business is a cutthroat world in which we must always strive to
out-innovate our competitors. This article is about the cloud in
general, but I want to explain why it’s also Amazon focused. Amazon has
been blazing the cloud-computing trail longer than anyone, which is
reflected by its 80% market share, and is clearly ahead of its
competitors. For example, check out Gartner’s analysis of different
cloud providers in the chart below and you’ll see Amazon in a corner of
their own. In fact, Amazon so dominated the competition that Gartner had
to artificially lower the scale. Amazon is innovating at such a rapid
pace in providing new services and broadening its customer base that
competitors are struggling to keep pace.</p>

<p>The effect of this innovation is obvious to developers. Do you want to
provide Active Directory integration with Amazon? You can do that. Do
you want to save on data costs by using the Bittorrent protocol for
content distribution? You can do that. Do you want to put heavy compute
behind your mobile games? You can do that. Do you want to ramp up your
clients quickly with remote workstations? You can do that, too. With
Amazon Web Services you can do all of that and more. I’m a
certified<a href="file:///C:/Users/Vishnu/Documents/Our/SMM/Blog/WhyEnterprisemustAmazon_(REVISED_CLEAN">[1]</a>.docx#_msocom_1) 
AWS instructor, yet I learn something new about AWS every day as Amazon
constantly releases new features. The advantage of moving your business
to the cloud is that you can let your developers focus on your company’s
area of expertise while leaving the boilerplate to Amazon.</p>

<p>\</p>

<p> <img src="https://lh3.googleusercontent.com/GmdBmLBmcGW9TIBqUCVkf403uIIf9arZ6-brIjBOt8tus6n_7YaKdOW3kcQqSIUbKnQd9vthqX1nyHHGouT8xbmDF3-xkpXUbJV0UJLpguSzg7EKB6QuaqR92uw_og" alt="" /></p>

<h2><strong>A Data Center That Hosts A Top Website</strong> </h2>

<p>One thing we mustn’t forget is what the AWS cloud really is—a product of
years of innovation by one of the largest e-commerce websites in the
world. Its data centers are distributed globally in numerous parts of
the world, with multiple, independently-operating data centers in each
region that provide different failure domains while being close enough
together to provide cheap communication. And that’s just the compute
side of the equation. Amazon also shares its global content distribution
network, its DNS servers. it compiles traffic data from across the globe
to find the best route for your network traffic. There are only a
handful of companies today with that kind of global reach, and they,
too, are developing cloud capabilities. But unless you’re in that elite
club, you won’t be able to create a data center that can compete with
AWS. So by using a cloud solution, rather than an in-house solution,
you’ll clearly benefit in terms of better performance, better fault
tolerance, and better disaster recovery.</p>

<h2><strong>Security</strong> </h2>

<p>Security is the biggest concern about cloud computing for most
enterprises, but Amazon holds many of the most important certifications,
including PCI, HIPAA, Sarbanes-Oxley, and ISO. Since it has so much at
stake, it maintains separation of logical and physical access to data in
order to limit the impact of disgruntled employees. While I certainly
understand someone hesitating to entrust one’s most valuable information
with a third party, it’s certainly debatable whether on-premise storage
is more secure than cloud storage.</p>

<h2><strong>In-house Expertise Not Required</strong> </h2>

<p>Anyone that’s had to hire people knows that good employees are worth
their weight in platinum. Moving to the cloud allows you to offload much
of your data-center maintenance onto Amazon. Let it do what it’s best at
while you focus on what you’re best at.</p>

<p>As we know, you often have to pay dearly to hire someone outside of your
area of expertise. For example, say you’re a director of IT at a company
like Schlumberger. Your company is great at what it does and has strong
brand value in its area of expertise. Do you think you can possibly
poach someone like <a href="http://mvdirona.com/jrh/work/">James Hamilton</a> for
your company? No, you can’t. With core expertise in data centers, Amazon
will likely offer a more intriguing challenge and a deeper sense of
mission to the kind of people you’ll want to hire for your team. And we
all know that employee engagement is not about the money, but rather
about being involved in a greater mission.</p>

<h2><strong>Lower Costs</strong> </h2>

<p>While I’m convinced that moving to the cloud will lower your costs, I
acknowledge that getting there requires a lot of expertise and hard
work. Additionally you may very easily find yourself comparing apples
and oranges. Yet cloud solutions are cheaper than on-premise solutions
when played right. First, when you pay for a machine on AWS you’re also
paying for Amazon’s years of expertise in setting up resilient data
centers, something that’s not true for in-house departments lacking core
IT expertise. Second, there’s great potential for saving money by
scaling to your variable demand needs instead of designing for max
capacity. That’s why Netflix, even though it comprises one-third of the
Internet in terms of data volume, has gone “all in” on AWS, or they’d
have to provision enough machines to handle 9PM Friday traffic.
Alternatively you can have an expected area of needing high demand, as
say for a chip company close to tapeout, a movie studio needing to
render for a year
(<a href="http://gigaom.com/2014/03/02/the-oscars-how-american-hustles-fx-team-made-2013-boston-look-like-1980s-new-york/">http://gigaom.com/2014/03/02/the-oscars-how-american-hustles-fx-team-made-2013-boston-look-like-1980s-new-york/</a>),
or a game company handling launch day demand
(<a href="http://www.respawn.com/news/lets-talk-about-the-xbox-live-cloud/">http://www.respawn.com/news/lets-talk-about-the-xbox-live-cloud/</a>).</p>

<h2><strong>Conclusion</strong> </h2>

<p>If you’re a large enterprise and on the fence about whether or not to
move to the cloud, we highly recommend trying it out. Try it on a
contained project, rather than one that’s on a critical path, something
that fits well within the cloud’s capabilities. After implementation, do
a post-mortem and analyze the results in terms of cost and
time-to-market. What is the expectation as you build in-house
expertise<strong>.</strong> The results may well surprise you.</p>

<h2><strong>Let’s Talk</strong> </h2>

<p>Our Research Labs has helped Fortune 100 and Fortune 500 companies move
successfully to AWS. We’d love to hear any questions, comments or
concerns you may have, so feel free to contact us anytime during weekly
<a href="http://ohours.org/aatersuleman">office hours</a> to discuss your specific
needs or situation. We offer this service free of charge with no
obligation or strings attached.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/03/23/creating-a-secure-deployment-pipeline-in-amazon-web-services/">Creating a Secure Deployment Pipeline in Amazon Web Services</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-03-23T11:24:27-04:00" pubdate data-updated="true">Mar 23<span>rd</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Many organizations require a secure infrastructure. I’ve yet to meet a customer that says that security isn’t a concern. But, the decision on “how secure?” should be closely associated with a risk analysis for your organization.</p>

<p>Since Amazon Web Services (AWS) is often referred to as a “public cloud”, people sometimes infer that “public” must mean it’s “out in the public” for all to see. I’ve always seen “public/private clouds” as an
unfortunate use of terms. In this context, public means more like
“Public Utility”. People often interpret “private clouds” to be
inherently more secure. Assuming that “public cloud” = less secure and
“private cloud” = more secure couldn’t be further from the truth. Like
most things, it’s all about how you architect your infrastructure. While
you can define your infrastructure to have open access, AWS provides
many tools to create a truly secure infrastructure while eliminating
access to all but only authorized users.</p>

<p>I’ve created an initial list of many of the practices we use. We don’t
employ all these practices in all situations, as it often depends on our
customers’ particular security requirements. But, if someone asked me
“How do I create a secure AWS infrastructure using a Deployment
Pipeline?”, I’d offer some of these practices in the solution. I’ll be
expanding these over the next few weeks, but I want to start with some
of our practices.</p>

<p><strong>AWS Security</strong></p>

<p>* After initial AWS account creation and login, configure
<a href="https://aws.amazon.com/iam/" title="AWS IAM">IAM</a> so that there’s no need to
use the AWS root account\
 * Apply least privilege to all IAM accounts. Be very careful about who
gets Administrator access.\
 * Enable all IAM password rules\
 * Enable MFA for all users\
 * Secure all data at rest\
 * Secure all data in transit\
 * Put all AWS resources in a <a href="https://aws.amazon.com/vpc/" title="Virtual Private Cloud">Virtual Private
Cloud</a> (VPC).\
 * No EC2 Key Pairs should be shared with others. Same goes for Access
Keys.\
 * Only open required ports to the Internet. For example, with the
exception of, say, port 80, no security groups should have a CIDR Source
of 0.0.0.0/0). The bastion host might have access to port 22 (SSH), but
you should enable
<a href="https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing" title="CIDR">CIDR</a>
to limit access to specific subnets. Using a VPC is a part of a solution
to eliminate Internet access. No canonical environments should have
SSH/RDP access.\
 * Use IAM to limit access to specific AWS resources and/or
remove/limit AWS console access\
 * Apply a bastion host configuration to reduce your attack profile\
 * Use <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html" title="IAM Roles">IAM
Roles</a>
so that there’s no need to configure Access Keys on the instances\
 * Use resource-level permissions in EC2 and RDS\
 * Use SSE to secure objects in S3 buckets\
 * Share initial IAM credentials with others through a secure mechanism
(e.g. <a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard" title="AES">AES-256
encryption</a>)\
 * Use and monitor AWS
<a href="https://aws.amazon.com/cloudtrail/" title="AWS CloudTrail">CloudTrail</a> logs</p>

<p><strong>Deployment Pipeline</strong></p>

<p>A deployment pipeline is a staged process in which the complete software
system is built and tested with every change. Team members receive
feedback as it completes each stage. With most customers, we usually
construct between 4-7 deployment pipeline stages and the pipeline only
goes to the next stage if the previous stages were successful. If a
stage fails, the whole pipeline instance fails. The first stage (often
referred to as the “Commit Stage”) will usually take no more than 10
minutes to complete. Other stages may take longer than this. Most stages
require no human intervention as the software system goes through more
extensive testing on its way to production. With a deployment pipeline,
software systems can be released at any time the business chooses to do
so. Here are some of the security-based practices we employ in
constructing a deployment pipeline.</p>

<p>* Automate everything: Networking (VPC, Route 53) Compute (EC2),
Storage, etc. All <em>AWS</em> automation should be defined in
<a href="https://aws.amazon.com/cloudformation/" title="CloudFormation">CloudFormation</a>.
All environment configuration should be defined using infrastructure
automation scripts – such as Chef, Puppet, etc.\
 * Version Everything: Application Code, Configuration, Infrastructure
and Data\
 * Manage your binary dependencies. Be specific about binary version
numbers. Ensure you have control over these binaries.\
 * Lockdown pipeline environments. Do not allow SSH/RDP access to any
environment in the deployment pipeline\
 * For project that require it, use permissions on the CI server or
Deployment application to limit who can run deployments in certain
environments – such as QA, Pre-Production and Production. When you have
a policy in which all changes are applied through automation and
environments are locked down, this usually becomes less of a concern.
But, it can still be a requirements on some teams.\
 * Use the Disposable Environments pattern – instances are terminated
once every few days. This approach reduces the attack profile\
 * Log everything outside of the EC2 instances (so that they can be
access later). Ensure these log files are encrypted e.g. securely
through S3)\
 * All canonical changes are only applied through automation that are
part of the deployment pipeline. This includes application,
configuration, infrastructure and data change. Infrastructure patch
management would be a part of the pipeline just like any outer software
system change.\
 * No one has access to nor can make direct changes to pipeline
environments\
 * Create high-availability systems Multi-AZ, <a href="https://aws.amazon.com/autoscaling/" title="Auto Scaling">Auto
Scaling</a>, <a href="https://aws.amazon.com/elasticloadbalancing/" title="ELB">Elastic
Load Balancing</a> and
Route 53\
 * For non-Admin AWS users, only provide access to AWS through a secure
<a href="https://en.wikipedia.org/wiki/Continuous_integration" title="Continuous Integration">Continuous
Integration</a>
(CI) server or a self-service application\
 * Use Self-Service Deployments and give developers full SSH/RDP access
to their self-service deployment. Only their particular EC2 Key Pair can
access the instance(s) associated with the deployment. Self-Service
Deployments can be defined in the CI server or a lightweight
self-service application.\
 * Provide capability for any authorized user to perform a self-service
deployment with full SSH/RDP access to the environment they created
(while eliminating outside access)\
 * Run two active environments – We’ve yet to do this for customers,
but if you want to eliminate all access to the canonical production
environment, you might choose to run two active environments at once so
that engineers can access the non-production environment to troubleshoot
a problem in which the environment has the exact same configuration and
data so you’re troubleshooting accurately.\
 * Run automated infrastructure tests to test for security
vulnerabilities (e.g. cross-site scripting, SQL injections, etc.) with
every change committed to the version-control repository as part of the
deployment pipeline.</p>

<p><strong>FAQ</strong></p>

<p>* <strong>What is a canonical environment?</strong> It’s your system of record. You
want your canonical environment to be solely defined in source code and
versioned. If someone makes a change to the canonical system and it
affects everyone it should only be done through automation. While you
can use a self-service deployment to get a copy of the canonical system,
any direct change you make to the environment is isolated and never made
part of the canonical system unless code is committed to the
version-control repository.\
 * <strong>How can I troubleshoot if I cannot directly access canonical
environments?</strong> Using a self-service deployment, you can usually
determine the cause of the problem. If it’s a data-specific problem, you
might import a copy of the production database. If this isn’t possible
for time or security reasons, you might run multiple versions of the
application at once.\
 * <strong>Why should we dispose of environments regularly?</strong> Two primary
reasons. The first is to reduce your attack profile (i.e. if
environments always go up and down, it’s more difficult to hone in on
specific resources. The second reason is that it ensures that all team
members are used to applying all canonical changes through automation
and not relying on environments to always be up and running somewhere.\
 * <strong>Why should we lockdown environments?</strong> To prevent people from
making disruptive environment changes that don’t go through the
version-control repository.</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>About Me</h1>
  <p>A little something about me.</p>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/04/04/a-case-study-on-using-100-percent-cloud-based-resources-with-automated-software-delivery/">A case study on using 100% cloud based Resources with Automated Software Delivery</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/03/25/continuous-delivery-implementation-laying-the-foundations-of-a-continuous-delivery-pipeline/">Continuous Delivery Implementation : Laying the Foundations of a Continuous Delivery Pipeline</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/03/25/continuous-delivery-implementation-getting-started-with-aws/">Continuous Delivery Implementation : Getting started with AWS</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/03/25/autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-4/">Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 4</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/03/25/autoscaling-for-lamp-on-aws-creating-a-lamp-stack-ami-part-3/">Autoscaling for LAMP on AWS |Creating a LAMP Stack AMI : Part 3</a>
      </li>
    
  </ul>
</section>



<section class="googleplus">
  <h1>
    <a href="https://plus.google.com/bvajjala@gmail.com?rel=author">
      <img src="http://www.google.com/images/icons/ui/gprofile_button-32.png" width="32" height="32">
      Google+
    </a>
  </h1>
</section>



  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Balaji Vajjala -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> | Themed with <a href="https://github.com/lucaslew/whitespace">Whitespace</a></span>
</p>

</footer>
  






<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
